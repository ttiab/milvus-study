import time
from typing import List

import numpy as np
from pymilvus import MilvusClient, FieldSchema, DataType, CollectionSchema
from tutorial.common.vector_utils import VectorUtils

client = MilvusClient(
    uri="http://localhost:19530",
    token="root:Milvus"
)

vectorUtils = VectorUtils()


"""
ì»¬ë ‰ì…˜ ìƒì„±
"""
collection_name = "partition_demo"
if client.has_collection(collection_name=collection_name):
    client.drop_collection(collection_name=collection_name)

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2000),
    FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),
    FieldSchema(name="region", dtype=DataType.VARCHAR, max_length=50),
    FieldSchema(name="created_date", dtype=DataType.VARCHAR, max_length=20),
    FieldSchema(name="priority", dtype=DataType.INT64),
    FieldSchema(name="score", dtype=DataType.FLOAT)
]

schema = CollectionSchema(
    fields=fields,
    description="íŒŒí‹°ì…˜ ê´€ë¦¬ ë°ëª¨ ì»¬ë ‰ì…˜"
)


client.create_collection(collection_name=collection_name, schema=schema)

"""
íŒŒí‹°ì…˜ ìƒì„±
"""

# 1. ì¹´í…Œê³ ë¦¬ë³„ íŒŒí‹°ì…˜
category_partitions = ["tech", "business", "science", "health"]

# 2. ì§€ì—­ë³„ íŒŒí‹°ì…˜
region_partitions = ["asia", "europe", "america"]

# 3. ì‹œê°„ë³„ íŒŒí‹°ì…˜
time_partitions = ["2023", "2024", "2025"]

all_partitions = {}

# ì¹´í…Œê³ ë¦¬ íŒŒí‹°ì…˜ ìƒì„±
for category in category_partitions:
    partition_name = f"category_{category}"
    client.create_partition(collection_name=collection_name, partition_name=partition_name)
    all_partitions[partition_name] = f"ì¹´í…Œê³ ë¦¬: {category}"
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}' ìƒì„±ë¨")

# ì§€ì—­ íŒŒí‹°ì…˜ ìƒì„±
for region in region_partitions:
    partition_name = f"region_{region}"
    client.create_partition(collection_name=collection_name, partition_name=partition_name)
    all_partitions[partition_name] = f"ì§€ì—­: {region}"
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}' ìƒì„±ë¨")

# ì‹œê°„ íŒŒí‹°ì…˜ ìƒì„±
for year in time_partitions:
    partition_name = f"year_{year}"
    client.create_partition(collection_name=collection_name, partition_name=partition_name)
    all_partitions[partition_name] = f"ë…„ë„: {year}"
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}' ìƒì„±ë¨")

"""
íŒŒí‹°ì…˜ ì¡°íšŒ
"""

partitions = client.list_partitions(collection_name=collection_name)
print(f"íŒŒí‹°ì…˜ ì´ ìˆ˜ {len(partitions)}")
print(partitions)
for i, partition in enumerate(partitions):
    print(f"  {i + 1}. {partition}")
    if hasattr(partition, 'description') and partition.description:
        print(f"      ì„¤ëª…: {partition.description}")

"""
íŒŒí‹°ì…˜ë³„ ë°ì´í„° ì‚½ì…
"""


def generate_partitioned_data(partition_type: str, partition_value: str, count: int = 1000) -> List[dict]:
    """íŒŒí‹°ì…˜ë³„ ë°ì´í„° ìƒì„±"""
    print(f"  ğŸ“Š {partition_type}={partition_value} ë°ì´í„° {count}ê°œ ìƒì„± ì¤‘...")

    titles = []
    contents = []
    categories = []
    regions = []
    dates = []
    priorities = []
    scores = []

    # íŒŒí‹°ì…˜ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ìƒì„±
    for i in range(count):
        if partition_type == "category":
            category = partition_value
            region = np.random.choice(["asia", "europe", "america"])
            year = np.random.choice(["2023", "2024", "2025"])
        elif partition_type == "region":
            category = np.random.choice(["tech", "business", "science", "health"])
            region = partition_value
            year = np.random.choice(["2023", "2024", "2025"])
        elif partition_type == "year":
            category = np.random.choice(["tech", "business", "science", "health"])
            region = np.random.choice(["asia", "europe", "america"])
            year = partition_value
        else:
            category = partition_value
            region = "asia"
            year = "2024"

        title = f"{category.title()} Article {i} from {region} in {year}"
        content = f"This is a detailed article about {category} topics, " \
                  f"published in {region} region during {year}. " \
                  f"It contains valuable insights and comprehensive analysis."

        titles.append(title)
        contents.append(content)
        categories.append(category)
        regions.append(region)
        dates.append(f"{year}-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}")
        priorities.append(np.random.randint(1, 6))
        scores.append(np.random.uniform(1.0, 5.0))

    # ë²¡í„° ë³€í™˜
    combined_texts = [f"{title} {content}" for title, content in zip(titles, contents)]
    vectors = vectorUtils.texts_to_vectors(combined_texts)

    # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°ì´í„° êµ¬ì„±
    data = []
    for i in range(count):
        data_item = {
            "id": i,
            "title": titles[i],
            "content": contents[i],
            "vector": vectors[i].tolist(),
            "category": categories[i],
            "region": regions[i],
            "created_date": dates[i],
            "priority": priorities[i],
            "score": scores[i]
        }
        data.append(data_item)

    return data

# ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì‚½ì…
categories = ["tech", "business", "science", "health"]
for category in categories:
    partition_name = f"category_{category}"
    data = generate_partitioned_data("category", category, 1000)
    insert_result = client.upsert(collection_name=collection_name, partition_name=partition_name, data=data)
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}': {insert_result}ê°œ ì‚½ì…")

# ì§€ì—­ë³„ ë°ì´í„° ì‚½ì…
regions = ["asia", "europe", "america"]
for region in regions:
    partition_name = f"region_{region}"
    data = generate_partitioned_data("region", region, 1500)

    insert_result = client.upsert(collection_name=collection_name, partition_name=partition_name, data=data)
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}': {insert_result}ê°œ ì‚½ì…")

# ì‹œê°„ë³„ ë°ì´í„° ì‚½ì…
years = ["2023", "2024", "2025"]
for year in years:
    partition_name = f"year_{year}"
    data = generate_partitioned_data("year", year, 1000)

    insert_result = client.upsert(collection_name=collection_name, partition_name=partition_name, data=data)
    print(f"  âœ… íŒŒí‹°ì…˜ '{partition_name}': {insert_result}ê°œ ì‚½ì…")

# ë°ì´í„° í”ŒëŸ¬ì‹œ
client.flush(collection_name=collection_name)
print(f"  âœ… ëª¨ë“  íŒŒí‹°ì…˜ ë°ì´í„° ì‚½ì… ì™„ë£Œ")

"""
ì¸ë±ìŠ¤ ìƒì„±
"""
index_params = client.prepare_index_params(collection_name=collection_name)

index_params.add_index(
    field_name="vector", # Name of the vector field to be indexed
    index_type="HNSW", # Type of the index to create
    index_name="vector_index", # Name of the index to create
    metric_type="L2", # Metric type used to measure similarity
    params={
        "M": 16, # Maximum number of neighbors each node can connect to in the graph
        "efConstruction": 200 # Number of candidate neighbors considered for connection during index construction
    } # Index building params
)

client.create_index(collection_name=collection_name, index_params=index_params)

print(client.list_indexes(collection_name=collection_name))

"""
íŒŒí‹°ì…˜ ê²€ìƒ‰
"""

client.load_collection(collection_name=collection_name)
query_text = "advanced technology artificial intelligence innovation"
query_vectors = vectorUtils.text_to_vector(query_text)
query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors

print(f"ê²€ìƒ‰ ì¿¼ë¦¬: '{query_text}'")

print("\n1. ì „ì²´ ì»¬ë ‰ì…˜ ê²€ìƒ‰")
start_time = time.time()

results = client.search(
    collection_name=collection_name,  # Collection name
    anns_field="vector",
    data=[query_vector],  # Query vector
    limit=10,  # TopK results to return
    search_params={"metric_type": "L2", "params": {"ef": 100}},
    output_fields=["title", "category", "region", "created_date"]
)

search_time = time.time() - start_time
print(f"ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
print(f"ê²°ê³¼ ìˆ˜: {len(results[0])}")


for j, hit in enumerate(results[0]):
    print(f"    {j + 1}. {hit['entity']['title']}")
    print(f"        ì¹´í…Œê³ ë¦¬: {hit['entity']['category']}")
    print(f"        ì§€ì—­: {hit['entity']['region']}")
    print(f"        ë‚ ì§œ: {hit['entity']['created_date']}")

print("\n2. íŠ¹ì • íŒŒí‹°ì…˜ ê²€ìƒ‰ (category_business)")
start_time = time.time()

results = client.search(
    collection_name=collection_name,  # Collection name
    partition_names=["category_business"],
    anns_field="vector",
    data=[query_vector],  # Query vector
    limit=10,  # TopK results to return
    search_params={"metric_type": "L2", "params": {"ef": 100}},
    output_fields=["title", "category", "region", "created_date"]
)

search_time = time.time() - start_time
print(f"ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
print(f"ê²°ê³¼ ìˆ˜: {len(results[0])}")

for j, hit in enumerate(results[0]):
    print(f"    {j + 1}. {hit['entity']['title']}")
    print(f"        ì¹´í…Œê³ ë¦¬: {hit['entity']['category']}")
    print(f"        ì§€ì—­: {hit['entity']['region']}")
    print(f"        ë‚ ì§œ: {hit['entity']['created_date']}")

print("\n3. ë‹¤ì¤‘ íŒŒí‹°ì…˜ ê²€ìƒ‰ (region_asia, region_europe)")
start_time = time.time()

results = client.search(
    collection_name=collection_name,  # Collection name
    partition_names=["region_asia", "region_europe"],
    anns_field="vector",
    data=[query_vector],  # Query vector
    limit=10,  # TopK results to return
    search_params={"metric_type": "L2", "params": {"ef": 100}},
    output_fields=["title", "category", "region", "created_date"]
)

search_time = time.time() - start_time
print(f"ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
print(f"ê²°ê³¼ ìˆ˜: {len(results[0])}")

for j, hit in enumerate(results[0]):
    print(f"    {j + 1}. {hit['entity']['title']}")
    print(f"        ì¹´í…Œê³ ë¦¬: {hit['entity']['category']}")
    print(f"        ì§€ì—­: {hit['entity']['region']}")
    print(f"        ë‚ ì§œ: {hit['entity']['created_date']}")






"""
íŒŒí‹°ì…˜ ì„±ëŠ¥ ë¹„êµ
"""

query_text = "business strategy management"
query_vectors = vectorUtils.text_to_vector(query_text)
query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors

test_cases = [
    {
        "name": "ì „ì²´ ì»¬ë ‰ì…˜",
        "partitions": None,
        "description": "ëª¨ë“  íŒŒí‹°ì…˜ì—ì„œ ê²€ìƒ‰"
    },
    {
        "name": "ë‹¨ì¼ íŒŒí‹°ì…˜",
        "partitions": ["category_business"],
        "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ì¹´í…Œê³ ë¦¬ë§Œ"
    },
    {
        "name": "ì§€ì—­ íŒŒí‹°ì…˜",
        "partitions": ["region_asia"],
        "description": "ì•„ì‹œì•„ ì§€ì—­ë§Œ"
    },
    {
        "name": "ì‹œê°„ íŒŒí‹°ì…˜",
        "partitions": ["year_2024"],
        "description": "2024ë…„ë§Œ"
    },
    {
        "name": "ë‹¤ì¤‘ íŒŒí‹°ì…˜",
        "partitions": ["category_business", "category_tech"],
        "description": "ë¹„ì¦ˆë‹ˆìŠ¤ + ê¸°ìˆ "
    }
]

print(f"{'í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤':<15} {'í‰ê· ì‹œê°„(ì´ˆ)':<12} {'QPS':<8} {'ê²°ê³¼ìˆ˜':<6} {'ì„¤ëª…':<20}")
print("-" * 80)

for case in test_cases:
    times = []
    result_count = 0

    # ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ í‰ê·  ê³„ì‚°
    for _ in range(3):
        start_time = time.time()

        if case["partitions"]:
            results = client.search(
                collection_name=collection_name,  # Collection name
                partition_names=case["partitions"],
                anns_field="vector",
                data=[query_vector],  # Query vector
                limit=10,  # TopK results to return
                search_params={"metric_type": "L2", "params": {"ef": 100}},
                output_fields=["title"]
            )
        else:
            results = client.search(
                collection_name=collection_name,  # Collection name
                anns_field="vector",
                data=[query_vector],  # Query vector
                limit=10,  # TopK results to return
                search_params={"metric_type": "L2", "params": {"ef": 100}},
                output_fields=["title"]
            )

        search_time = time.time() - start_time
        times.append(search_time)
        result_count = len(results[0])

    avg_time = np.mean(times)
    qps = 1 / avg_time

    print(f"{case['name']:<15} {avg_time:<12.4f} {qps:<8.2f} {result_count:<6} {case['description']:<20}")



