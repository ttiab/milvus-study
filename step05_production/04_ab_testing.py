#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Milvus A/B í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvus ì„œë¹„ìŠ¤ì˜ A/B í…ŒìŠ¤íŒ…ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì„±ëŠ¥ ë¹„êµ, ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸, í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ ë“±ì„ í†µí•´ 
í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import random
import statistics
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import threading

# ê³µí†µ ëª¨ë“ˆ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

class TestVariant(Enum):
    """í…ŒìŠ¤íŠ¸ ë³€í˜•"""
    CONTROL = "A"  # ê¸°ì¡´ ë²„ì „
    TREATMENT = "B"  # ìƒˆ ë²„ì „

@dataclass
class TestMetrics:
    """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­"""
    variant: TestVariant
    response_time_ms: List[float]
    throughput_qps: List[float]
    error_rate: List[float]
    memory_usage_mb: List[float]
    cpu_usage_percent: List[float]
    search_accuracy: List[float]
    user_satisfaction: List[float]
    timestamp: datetime

@dataclass
class ABTestConfig:
    """A/B í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    name: str
    description: str
    traffic_split: Dict[TestVariant, int]  # íŠ¸ë˜í”½ ë¶„í•  ë¹„ìœ¨
    duration_minutes: int
    sample_size: int
    confidence_level: float
    success_criteria: Dict[str, float]

class ABTestingManager:
    """A/B í…ŒìŠ¤íŒ… ê´€ë¦¬ì"""
    
    def __init__(self):
        self.tests: Dict[str, ABTestConfig] = {}
        self.metrics_data: Dict[str, List[TestMetrics]] = {}
        self.active_tests: Dict[str, bool] = {}
        self.results_dir = Path("ab-test-results")
        self.results_dir.mkdir(exist_ok=True)
    
    def create_ab_test_config(self, test_name: str) -> ABTestConfig:
        """A/B í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„±"""
        configs = {
            "search_algorithm_comparison": ABTestConfig(
                name="ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ",
                description="ê¸°ì¡´ IVF_FLAT vs ìƒˆë¡œìš´ HNSW ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ",
                traffic_split={TestVariant.CONTROL: 50, TestVariant.TREATMENT: 50},
                duration_minutes=30,
                sample_size=1000,
                confidence_level=0.95,
                success_criteria={
                    'response_time_improvement': 0.2,  # 20% ê°œì„ 
                    'accuracy_threshold': 0.95,
                    'error_rate_max': 0.01
                }
            ),
            "memory_optimization": ABTestConfig(
                name="ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸",
                description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë²„ì „ íš¨ê³¼ ê²€ì¦",
                traffic_split={TestVariant.CONTROL: 70, TestVariant.TREATMENT: 30},
                duration_minutes=45,
                sample_size=800,
                confidence_level=0.95,
                success_criteria={
                    'memory_reduction': 0.25,  # 25% ë©”ëª¨ë¦¬ ì ˆì•½
                    'performance_degradation_max': 0.05  # 5% ì´í•˜ ì„±ëŠ¥ ì €í•˜
                }
            ),
            "new_feature_rollout": ABTestConfig(
                name="ì‹ ê¸°ëŠ¥ ì ì§„ì  ì¶œì‹œ",
                description="ìƒˆë¡œìš´ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì˜ ì‚¬ìš©ì ê²½í—˜ í…ŒìŠ¤íŠ¸",
                traffic_split={TestVariant.CONTROL: 90, TestVariant.TREATMENT: 10},
                duration_minutes=60,
                sample_size=500,
                confidence_level=0.90,
                success_criteria={
                    'user_satisfaction_min': 4.0,  # 5ì  ë§Œì  ì¤‘ 4ì  ì´ìƒ
                    'adoption_rate_min': 0.30  # 30% ì´ìƒ ì‚¬ìš©ë¥ 
                }
            )
        }
        
        return configs.get(test_name, configs["search_algorithm_comparison"])
    
    def setup_ab_test_environment(self):
        """A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        print("ğŸ§ª A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        manifests_dir = Path("ab-test-manifests")
        manifests_dir.mkdir(exist_ok=True)
        
        # Traffic Splittingì„ ìœ„í•œ Istio VirtualService
        virtual_service = {
            'apiVersion': 'networking.istio.io/v1beta1',
            'kind': 'VirtualService',
            'metadata': {
                'name': 'milvus-ab-test',
                'namespace': 'milvus-production'
            },
            'spec': {
                'hosts': ['milvus.example.com'],
                'http': [{
                    'match': [{
                        'headers': {
                            'ab-test-group': {'exact': 'treatment'}
                        }
                    }],
                    'route': [{
                        'destination': {
                            'host': 'milvus-treatment',
                            'port': {'number': 19530}
                        }
                    }]
                }, {
                    'route': [{
                        'destination': {
                            'host': 'milvus-control',
                            'port': {'number': 19530}
                        },
                        'weight': 50
                    }, {
                        'destination': {
                            'host': 'milvus-treatment',
                            'port': {'number': 19530}
                        },
                        'weight': 50
                    }]
                }]
            }
        }
        
        # ConfigMap for feature flags
        feature_flags_config = {
            'apiVersion': 'v1',
            'kind': 'ConfigMap',
            'metadata': {
                'name': 'ab-test-config',
                'namespace': 'milvus-production'
            },
            'data': {
                'config.json': json.dumps({
                    'ab_tests': {
                        'search_algorithm': {
                            'enabled': True,
                            'treatment_percentage': 50,
                            'feature_flags': {
                                'use_hnsw': True,
                                'optimize_memory': True
                            }
                        }
                    }
                }, indent=2)
            }
        }
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
        import yaml
        
        with open(manifests_dir / 'virtual-service.yaml', 'w') as f:
            yaml.dump(virtual_service, f, default_flow_style=False)
        
        with open(manifests_dir / 'feature-flags-config.yaml', 'w') as f:
            yaml.dump(feature_flags_config, f, default_flow_style=False)
        
        print("  âœ… A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±ë¨")
        
        # Monitoring Dashboard ì„¤ì •
        self.create_ab_test_dashboard()
    
    def create_ab_test_dashboard(self):
        """A/B í…ŒìŠ¤íŠ¸ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        print("ğŸ“Š A/B í…ŒìŠ¤íŠ¸ ëŒ€ì‹œë³´ë“œ ì„¤ì • ì¤‘...")
        
        dashboard_config = {
            'dashboard': {
                'title': 'Milvus A/B Testing Dashboard',
                'panels': [
                    {
                        'title': 'Response Time Comparison',
                        'type': 'graph',
                        'metrics': [
                            'avg(milvus_request_duration_seconds{variant="control"})',
                            'avg(milvus_request_duration_seconds{variant="treatment"})'
                        ]
                    },
                    {
                        'title': 'Throughput (QPS)',
                        'type': 'graph',
                        'metrics': [
                            'rate(milvus_requests_total{variant="control"}[5m])',
                            'rate(milvus_requests_total{variant="treatment"}[5m])'
                        ]
                    },
                    {
                        'title': 'Error Rate',
                        'type': 'graph',
                        'metrics': [
                            'rate(milvus_errors_total{variant="control"}[5m])',
                            'rate(milvus_errors_total{variant="treatment"}[5m])'
                        ]
                    },
                    {
                        'title': 'Memory Usage',
                        'type': 'graph',
                        'metrics': [
                            'avg(milvus_memory_usage_bytes{variant="control"})',
                            'avg(milvus_memory_usage_bytes{variant="treatment"})'
                        ]
                    }
                ]
            }
        }
        
        with open(self.results_dir / 'grafana-dashboard.json', 'w') as f:
            json.dump(dashboard_config, f, indent=2)
        
        print("  âœ… Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì • ìƒì„±ë¨")
    
    def simulate_test_traffic(self, config: ABTestConfig, variant: TestVariant) -> TestMetrics:
        """í…ŒìŠ¤íŠ¸ íŠ¸ë˜í”½ ì‹œë®¬ë ˆì´ì…˜"""
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ë©”íŠ¸ë¦­ ìƒì„±
        base_response_time = 50 if variant == TestVariant.CONTROL else 40  # Treatmentê°€ ë” ë¹ ë¦„
        base_throughput = 1000 if variant == TestVariant.CONTROL else 1200  # Treatmentê°€ ë” ë†’ìŒ
        base_error_rate = 0.005 if variant == TestVariant.CONTROL else 0.003  # Treatmentê°€ ë” ë‚®ìŒ
        base_memory = 2048 if variant == TestVariant.CONTROL else 1600  # Treatmentê°€ ë” ì ìŒ
        base_cpu = 65 if variant == TestVariant.CONTROL else 60  # Treatmentê°€ ë” ë‚®ìŒ
        base_accuracy = 0.94 if variant == TestVariant.CONTROL else 0.96  # Treatmentê°€ ë” ë†’ìŒ
        base_satisfaction = 3.8 if variant == TestVariant.CONTROL else 4.2  # Treatmentê°€ ë” ë†’ìŒ
        
        # ëœë¤ ë³€ë™ ì¶”ê°€
        sample_size = min(config.sample_size, 100)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ì œí•œ
        
        response_times = [
            max(10, base_response_time + random.gauss(0, 10)) 
            for _ in range(sample_size)
        ]
        
        throughputs = [
            max(500, base_throughput + random.gauss(0, 100)) 
            for _ in range(sample_size)
        ]
        
        error_rates = [
            max(0, base_error_rate + random.gauss(0, 0.002)) 
            for _ in range(sample_size)
        ]
        
        memory_usages = [
            max(1000, base_memory + random.gauss(0, 200)) 
            for _ in range(sample_size)
        ]
        
        cpu_usages = [
            max(20, min(100, base_cpu + random.gauss(0, 10))) 
            for _ in range(sample_size)
        ]
        
        accuracies = [
            max(0.8, min(1.0, base_accuracy + random.gauss(0, 0.02))) 
            for _ in range(sample_size)
        ]
        
        satisfactions = [
            max(1.0, min(5.0, base_satisfaction + random.gauss(0, 0.3))) 
            for _ in range(sample_size)
        ]
        
        return TestMetrics(
            variant=variant,
            response_time_ms=response_times,
            throughput_qps=throughputs,
            error_rate=error_rates,
            memory_usage_mb=memory_usages,
            cpu_usage_percent=cpu_usages,
            search_accuracy=accuracies,
            user_satisfaction=satisfactions,
            timestamp=datetime.now()
        )
    
    def run_ab_test(self, test_name: str) -> bool:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\nğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_name}")
        
        config = self.create_ab_test_config(test_name)
        self.tests[test_name] = config
        self.metrics_data[test_name] = []
        self.active_tests[test_name] = True
        
        print(f"  ğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¤ì •:")
        print(f"    ì´ë¦„: {config.name}")
        print(f"    ì„¤ëª…: {config.description}")
        print(f"    íŠ¸ë˜í”½ ë¶„í• : A({config.traffic_split[TestVariant.CONTROL]}%) / B({config.traffic_split[TestVariant.TREATMENT]}%)")
        print(f"    ì§€ì† ì‹œê°„: {config.duration_minutes}ë¶„")
        print(f"    ìƒ˜í”Œ í¬ê¸°: {config.sample_size}")
        print(f"    ì‹ ë¢°ë„: {config.confidence_level * 100}%")
        
        # ë³‘ë ¬ë¡œ ì–‘ìª½ variant í…ŒìŠ¤íŠ¸
        control_metrics = self.simulate_test_traffic(config, TestVariant.CONTROL)
        treatment_metrics = self.simulate_test_traffic(config, TestVariant.TREATMENT)
        
        self.metrics_data[test_name] = [control_metrics, treatment_metrics]
        
        print(f"  âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return True
    
    def calculate_statistical_significance(self, 
                                           control_data: List[float], 
                                           treatment_data: List[float]) -> Tuple[float, bool]:
        """í†µê³„ì  ìœ ì˜ì„± ê³„ì‚° (ê°„ì†Œí™”ëœ t-test)"""
        
        if len(control_data) == 0 or len(treatment_data) == 0:
            return 0.0, False
        
        mean_control = statistics.mean(control_data)
        mean_treatment = statistics.mean(treatment_data)
        
        if len(control_data) == 1 and len(treatment_data) == 1:
            return 0.0, False
        
        # ê°„ì†Œí™”ëœ t-test ê³„ì‚°
        try:
            var_control = statistics.variance(control_data) if len(control_data) > 1 else 0
            var_treatment = statistics.variance(treatment_data) if len(treatment_data) > 1 else 0
            
            pooled_std = ((var_control + var_treatment) / 2) ** 0.5
            
            if pooled_std == 0:
                return 0.0, False
            
            t_stat = abs(mean_treatment - mean_control) / (pooled_std * (1/len(control_data) + 1/len(treatment_data)) ** 0.5)
            
            # ê°„ì†Œí™”ëœ p-value ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ê³„ì‚° í•„ìš”)
            p_value = max(0.001, 1 / (1 + t_stat))
            
            is_significant = p_value < 0.05
            
            return p_value, is_significant
            
        except (statistics.StatisticsError, ZeroDivisionError):
            return 0.0, False
    
    def analyze_test_results(self, test_name: str) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        print(f"\nğŸ“Š A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„: {test_name}")
        
        if test_name not in self.metrics_data or len(self.metrics_data[test_name]) < 2:
            print("  âŒ ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        control_metrics = self.metrics_data[test_name][0]
        treatment_metrics = self.metrics_data[test_name][1]
        config = self.tests[test_name]
        
        results = {}
        
        # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
        metrics_to_analyze = [
            ('response_time_ms', 'ì‘ë‹µ ì‹œê°„ (ms)', 'lower_is_better'),
            ('throughput_qps', 'ì²˜ë¦¬ëŸ‰ (QPS)', 'higher_is_better'),
            ('error_rate', 'ì˜¤ë¥˜ìœ¨', 'lower_is_better'),
            ('memory_usage_mb', 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', 'lower_is_better'),
            ('cpu_usage_percent', 'CPU ì‚¬ìš©ë¥  (%)', 'lower_is_better'),
            ('search_accuracy', 'ê²€ìƒ‰ ì •í™•ë„', 'higher_is_better'),
            ('user_satisfaction', 'ì‚¬ìš©ì ë§Œì¡±ë„', 'higher_is_better')
        ]
        
        print("  ğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ë¹„êµ ê²°ê³¼:")
        
        for metric_name, display_name, direction in metrics_to_analyze:
            control_data = getattr(control_metrics, metric_name)
            treatment_data = getattr(treatment_metrics, metric_name)
            
            control_mean = statistics.mean(control_data)
            treatment_mean = statistics.mean(treatment_data)
            
            improvement = ((treatment_mean - control_mean) / control_mean) * 100
            if direction == 'lower_is_better':
                improvement = -improvement
            
            p_value, is_significant = self.calculate_statistical_significance(control_data, treatment_data)
            
            results[metric_name] = {
                'control_mean': control_mean,
                'treatment_mean': treatment_mean,
                'improvement_percent': improvement,
                'p_value': p_value,
                'is_significant': is_significant,
                'direction': direction
            }
            
            significance_indicator = "âœ…" if is_significant else "âš ï¸"
            improvement_indicator = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â–"
            
            print(f"    {significance_indicator} {display_name}:")
            print(f"      A (Control): {control_mean:.2f}")
            print(f"      B (Treatment): {treatment_mean:.2f}")
            print(f"      {improvement_indicator} ë³€í™”: {improvement:+.1f}%")
            print(f"      p-value: {p_value:.3f}")
        
        # ì„±ê³µ ê¸°ì¤€ ì²´í¬
        print(f"\n  ğŸ¯ ì„±ê³µ ê¸°ì¤€ ê²€ì¦:")
        success_criteria_met = True
        
        for criterion, threshold in config.success_criteria.items():
            if criterion == 'response_time_improvement':
                actual = results.get('response_time_ms', {}).get('improvement_percent', 0)
                met = actual >= threshold * 100
                print(f"    {'âœ…' if met else 'âŒ'} ì‘ë‹µì‹œê°„ ê°œì„ : {actual:.1f}% (ëª©í‘œ: {threshold*100:.1f}%)")
                success_criteria_met = success_criteria_met and met
                
            elif criterion == 'accuracy_threshold':
                actual = results.get('search_accuracy', {}).get('treatment_mean', 0)
                met = actual >= threshold
                print(f"    {'âœ…' if met else 'âŒ'} ê²€ìƒ‰ ì •í™•ë„: {actual:.3f} (ëª©í‘œ: {threshold:.3f})")
                success_criteria_met = success_criteria_met and met
                
            elif criterion == 'error_rate_max':
                actual = results.get('error_rate', {}).get('treatment_mean', 1)
                met = actual <= threshold
                print(f"    {'âœ…' if met else 'âŒ'} ì˜¤ë¥˜ìœ¨: {actual:.3f} (ìµœëŒ€: {threshold:.3f})")
                success_criteria_met = success_criteria_met and met
        
        results['success_criteria_met'] = success_criteria_met
        results['recommendation'] = self.generate_recommendation(results, success_criteria_met)
        
        return results
    
    def generate_recommendation(self, results: Dict[str, Any], criteria_met: bool) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        if criteria_met:
            significant_improvements = []
            for metric, data in results.items():
                if isinstance(data, dict) and data.get('is_significant') and data.get('improvement_percent', 0) > 5:
                    significant_improvements.append(metric)
            
            if significant_improvements:
                return "ğŸš€ PROCEED: Treatment ë²„ì „ì„ í”„ë¡œë•ì…˜ì— ë°°í¬í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì„±ëŠ¥ ê°œì„ ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                return "ğŸ¤” NEUTRAL: ì„±ëŠ¥ ê°œì„ ì´ ìˆì§€ë§Œ í° ì°¨ì´ëŠ” ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ìš”ì¸ì„ ê³ ë ¤í•˜ì—¬ ê²°ì •í•˜ì„¸ìš”."
        else:
            critical_failures = []
            for metric, data in results.items():
                if isinstance(data, dict) and data.get('improvement_percent', 0) < -10:
                    critical_failures.append(metric)
            
            if critical_failures:
                return "ğŸ›‘ STOP: Treatment ë²„ì „ì— ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ê°€ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ìµœì í™” í›„ ì¬í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            else:
                return "âš ï¸ CAUTION: ì¼ë¶€ ì„±ê³µ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê°œì„  í›„ ì¬í…ŒìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
    
    def save_test_results(self, test_name: str, results: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{test_name}_{timestamp}.json"
        
        report = {
            'test_name': test_name,
            'config': {
                'name': self.tests[test_name].name,
                'description': self.tests[test_name].description,
                'traffic_split': {k.value: v for k, v in self.tests[test_name].traffic_split.items()},
                'duration_minutes': self.tests[test_name].duration_minutes,
                'sample_size': self.tests[test_name].sample_size,
                'confidence_level': self.tests[test_name].confidence_level,
                'success_criteria': self.tests[test_name].success_criteria
            },
            'results': results,
            'timestamp': datetime.now().isoformat(),
            'recommendations': results.get('recommendation', '')
        }
        
        with open(self.results_dir / filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"  ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")
    
    def create_ab_test_scripts(self):
        """A/B í…ŒìŠ¤íŠ¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        print("ğŸ“œ A/B í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        
        scripts_dir = Path("ab-test-scripts")
        scripts_dir.mkdir(exist_ok=True)
        
        # A/B í…ŒìŠ¤íŠ¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
        start_script = '''#!/bin/bash
set -e

GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
RED='\\033[0;31m'
NC='\\033[0m'

TEST_NAME=""
TREATMENT_PERCENTAGE=50
DURATION_MINUTES=30
NAMESPACE="milvus-production"

usage() {
    echo "Usage: $0 -t TEST_NAME [-p PERCENTAGE] [-d DURATION] [-h]"
    echo "  -t TEST_NAME    Name of the A/B test"
    echo "  -p PERCENTAGE   Traffic percentage for treatment (default: 50)"
    echo "  -d DURATION     Test duration in minutes (default: 30)"
    echo "  -h              Show this help"
    exit 1
}

while getopts "t:p:d:h" opt; do
    case $opt in
        t) TEST_NAME="$OPTARG" ;;
        p) TREATMENT_PERCENTAGE="$OPTARG" ;;
        d) DURATION_MINUTES="$OPTARG" ;;
        h) usage ;;
        *) usage ;;
    esac
done

if [ -z "$TEST_NAME" ]; then
    echo -e "${RED}Error: Test name is required${NC}"
    usage
fi

echo -e "${GREEN}ğŸ§ª Starting A/B Test: $TEST_NAME${NC}"
echo -e "Treatment Traffic: ${YELLOW}$TREATMENT_PERCENTAGE%${NC}"
echo -e "Duration: ${YELLOW}$DURATION_MINUTES minutes${NC}"

# Update traffic splitting
CONTROL_PERCENTAGE=$((100 - TREATMENT_PERCENTAGE))

kubectl patch virtualservice milvus-ab-test -n $NAMESPACE --type='merge' -p='{
  "spec": {
    "http": [{
      "route": [{
        "destination": {"host": "milvus-control"},
        "weight": '$CONTROL_PERCENTAGE'
      }, {
        "destination": {"host": "milvus-treatment"},
        "weight": '$TREATMENT_PERCENTAGE'
      }]
    }]
  }
}'

echo -e "${GREEN}âœ… A/B test started successfully${NC}"
echo -e "Monitor progress: ./ab-test-monitor.sh -t $TEST_NAME"
'''
        
        # A/B í…ŒìŠ¤íŠ¸ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
        monitor_script = '''#!/bin/bash

GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
BLUE='\\033[0;34m'
NC='\\033[0m'

TEST_NAME=""
NAMESPACE="milvus-production"

usage() {
    echo "Usage: $0 -t TEST_NAME [-h]"
    echo "  -t TEST_NAME    Name of the A/B test to monitor"
    echo "  -h              Show this help"
    exit 1
}

while getopts "t:h" opt; do
    case $opt in
        t) TEST_NAME="$OPTARG" ;;
        h) usage ;;
        *) usage ;;
    esac
done

if [ -z "$TEST_NAME" ]; then
    echo -e "${RED}Error: Test name is required${NC}"
    usage
fi

echo -e "${GREEN}ğŸ“Š Monitoring A/B Test: $TEST_NAME${NC}"
echo -e "${BLUE}Ctrl+C to stop monitoring${NC}"
echo ""

while true; do
    echo -e "${YELLOW}$(date): Checking metrics...${NC}"
    
    # Get pod metrics
    echo "Control Group (A):"
    kubectl top pods -n $NAMESPACE -l variant=control | head -5
    
    echo ""
    echo "Treatment Group (B):"
    kubectl top pods -n $NAMESPACE -l variant=treatment | head -5
    
    echo ""
    echo "Traffic Distribution:"
    kubectl get virtualservice milvus-ab-test -n $NAMESPACE -o jsonpath='{.spec.http[0].route[*].weight}' | tr ' ' '\\n' | paste -d: <(echo -e "Control\\nTreatment") -
    
    echo ""
    echo "----------------------------------------"
    sleep 30
done
'''
        
        # A/B í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸
        stop_script = '''#!/bin/bash
set -e

GREEN='\\033[0;32m'
RED='\\033[0;31m'
YELLOW='\\033[1;33m'
NC='\\033[0m'

TEST_NAME=""
WINNER=""
NAMESPACE="milvus-production"

usage() {
    echo "Usage: $0 -t TEST_NAME -w WINNER [-h]"
    echo "  -t TEST_NAME    Name of the A/B test"
    echo "  -w WINNER       Winner variant (control|treatment)"
    echo "  -h              Show this help"
    exit 1
}

while getopts "t:w:h" opt; do
    case $opt in
        t) TEST_NAME="$OPTARG" ;;
        w) WINNER="$OPTARG" ;;
        h) usage ;;
        *) usage ;;
    esac
done

if [ -z "$TEST_NAME" ] || [ -z "$WINNER" ]; then
    echo -e "${RED}Error: Test name and winner are required${NC}"
    usage
fi

echo -e "${GREEN}ğŸ Stopping A/B Test: $TEST_NAME${NC}"
echo -e "Winner: ${YELLOW}$WINNER${NC}"

if [ "$WINNER" = "control" ]; then
    # Route all traffic to control
    kubectl patch virtualservice milvus-ab-test -n $NAMESPACE --type='merge' -p='{
      "spec": {
        "http": [{
          "route": [{
            "destination": {"host": "milvus-control"},
            "weight": 100
          }]
        }]
      }
    }'
    echo -e "${GREEN}âœ… All traffic routed to control group${NC}"
elif [ "$WINNER" = "treatment" ]; then
    # Route all traffic to treatment
    kubectl patch virtualservice milvus-ab-test -n $NAMESPACE --type='merge' -p='{
      "spec": {
        "http": [{
          "route": [{
            "destination": {"host": "milvus-treatment"},
            "weight": 100
          }]
        }]
      }
    }'
    echo -e "${GREEN}âœ… All traffic routed to treatment group${NC}"
else
    echo -e "${RED}Error: Winner must be 'control' or 'treatment'${NC}"
    exit 1
fi

echo -e "${GREEN}A/B test completed successfully${NC}"
'''
        
        # ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
        scripts = [
            ('ab-test-start.sh', start_script),
            ('ab-test-monitor.sh', monitor_script),
            ('ab-test-stop.sh', stop_script)
        ]
        
        for filename, content in scripts:
            with open(scripts_dir / filename, 'w') as f:
                f.write(content)
        
        print("  âœ… A/B í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def demonstrate_ab_testing(self):
        """A/B í…ŒìŠ¤íŒ… ì‹œì—°"""
        print("\nğŸ­ A/B í…ŒìŠ¤íŒ… ì‹œì—° ì‹œì‘...")
        
        # ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
        test_scenarios = [
            "search_algorithm_comparison",
            "memory_optimization", 
            "new_feature_rollout"
        ]
        
        all_results = {}
        
        for scenario in test_scenarios:
            print(f"\n{'='*60}")
            self.run_ab_test(scenario)
            results = self.analyze_test_results(scenario)
            self.save_test_results(scenario, results)
            all_results[scenario] = results
            
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­: {results.get('recommendation', 'ë°ì´í„° ë¶€ì¡±')}")
        
        # ì¢…í•© ìš”ì•½
        print(f"\n{'='*80}")
        print(" ğŸ¯ A/B í…ŒìŠ¤íŠ¸ ì¢…í•© ìš”ì•½")
        print("="*80)
        
        for scenario, results in all_results.items():
            config = self.tests[scenario]
            success = results.get('success_criteria_met', False)
            
            print(f"\nğŸ“Š {config.name}:")
            print(f"  ê²°ê³¼: {'âœ… ì„±ê³µ' if success else 'âŒ ì‹¤íŒ¨'}")
            
            # ì£¼ìš” ê°œì„ ì‚¬í•­ í‘œì‹œ
            significant_improvements = []
            for metric, data in results.items():
                if isinstance(data, dict) and data.get('is_significant') and data.get('improvement_percent', 0) > 5:
                    improvement = data.get('improvement_percent', 0)
                    significant_improvements.append(f"{metric}: {improvement:+.1f}%")
            
            if significant_improvements:
                print(f"  ì£¼ìš” ê°œì„ : {', '.join(significant_improvements[:3])}")
            else:
                print(f"  ì£¼ìš” ê°œì„ : ì—†ìŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š Milvus A/B í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    manager = ABTestingManager()
    
    try:
        # 1. A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        print("\n" + "=" * 80)
        print(" ğŸ§ª A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •")
        print("=" * 80)
        manager.setup_ab_test_environment()
        
        # 2. ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        print("\n" + "=" * 80)
        print(" ğŸ“œ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")
        print("=" * 80)
        manager.create_ab_test_scripts()
        
        # 3. A/B í…ŒìŠ¤íŒ… ì‹œì—°
        print("\n" + "=" * 80)
        print(" ğŸ­ A/B í…ŒìŠ¤íŒ… ì‹œì—°")
        print("=" * 80)
        manager.demonstrate_ab_testing()
        
        # 4. ìš”ì•½
        print("\n" + "=" * 80)
        print(" ğŸ“Š A/B í…ŒìŠ¤íŒ… ì™„ë£Œ")
        print("=" * 80)
        
        print("âœ… ìƒì„±ëœ ë¦¬ì†ŒìŠ¤:")
        resources = [
            "ab-test-manifests/virtual-service.yaml",
            "ab-test-manifests/feature-flags-config.yaml",
            "ab-test-results/grafana-dashboard.json",
            "ab-test-scripts/ab-test-start.sh",
            "ab-test-scripts/ab-test-monitor.sh", 
            "ab-test-scripts/ab-test-stop.sh"
        ]
        
        for resource in resources:
            print(f"  ğŸ“„ {resource}")
        
        # ê²°ê³¼ íŒŒì¼ë“¤
        result_files = list(manager.results_dir.glob("*.json"))
        if result_files:
            print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼:")
            for file in result_files:
                print(f"  ğŸ“„ {file.name}")
        
        print("\nğŸ’¡ A/B í…ŒìŠ¤íŒ… ëª¨ë²” ì‚¬ë¡€:")
        best_practices = [
            "âœ… ì¶©ë¶„í•œ ìƒ˜í”Œ í¬ê¸° í™•ë³´",
            "âœ… í†µê³„ì  ìœ ì˜ì„± í™•ì¸",
            "âœ… ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ì¢…í•© ë¶„ì„",
            "âœ… ì¥ê¸°ê°„ ëª¨ë‹ˆí„°ë§", 
            "âœ… ì ì§„ì  íŠ¸ë˜í”½ ì¦ê°€",
            "âœ… ì¦‰ì‹œ ë¡¤ë°± ê°€ëŠ¥í•œ ì²´ê³„"
        ]
        
        for practice in best_practices:
            print(f"  {practice}")
        
        print("\nğŸš€ A/B í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ ì˜ˆì‹œ:")
        commands = [
            "# A/B í…ŒìŠ¤íŠ¸ ì‹œì‘",
            "./ab-test-scripts/ab-test-start.sh -t search_optimization -p 30 -d 60",
            "",
            "# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§",
            "./ab-test-scripts/ab-test-monitor.sh -t search_optimization",
            "",
            "# í…ŒìŠ¤íŠ¸ ì¢…ë£Œ (ìŠ¹ì ì„ íƒ)",
            "./ab-test-scripts/ab-test-stop.sh -t search_optimization -w treatment"
        ]
        
        for cmd in commands:
            if cmd.startswith('#') or cmd == '':
                print(f"  {cmd}")
            else:
                print(f"  $ {cmd}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ A/B í…ŒìŠ¤íŒ… ì‹¤ìŠµ ì™„ë£Œ!")
    print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
    print("  â€¢ A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ì‹¤í–‰")
    print("  â€¢ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦")
    print("  â€¢ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ë¶„ì„")
    print("  â€¢ ì ì§„ì  íŠ¸ë˜í”½ ë¶„ì‚°")
    
    print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
    print("  python step05_production/05_security_auth.py")

if __name__ == "__main__":
    main() 