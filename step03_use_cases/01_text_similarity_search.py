#!/usr/bin/env python3
"""
Milvus í•™ìŠµ í”„ë¡œì íŠ¸ - 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ

ì‹¤ì œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- ë¬¸ì„œ ê²€ìƒ‰ ì—”ì§„
- Q&A ë§¤ì¹­ ì‹œìŠ¤í…œ  
- ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ
- ì‹œë§¨í‹± ê²€ìƒ‰
"""

import sys
import os
import time
import json
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader
from pymilvus import Collection, utility, FieldSchema, CollectionSchema, DataType

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextSimilaritySearchEngine:
    """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, connection: MilvusConnection):
        self.connection = connection
        self.vector_utils = VectorUtils()
        self.data_loader = DataLoader()
        self.collections = {}
        
    def create_document_collection(self, collection_name: str = "document_search") -> Collection:
        """ë¬¸ì„œ ê²€ìƒ‰ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"\nğŸ“ ë¬¸ì„œ ê²€ìƒ‰ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜ - í’ë¶€í•œ ë©”íƒ€ë°ì´í„° í¬í•¨
        fields = [
            FieldSchema(name="doc_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="summary", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="publish_date", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="tags", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="language", dtype=DataType.VARCHAR, max_length=10),
            FieldSchema(name="doc_type", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="word_count", dtype=DataType.INT64),
            FieldSchema(name="reading_time", dtype=DataType.INT64),  # ì˜ˆìƒ ì½ê¸° ì‹œê°„ (ë¶„)
            FieldSchema(name="difficulty_level", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="quality_score", dtype=DataType.FLOAT),
            FieldSchema(name="view_count", dtype=DataType.INT64),
            FieldSchema(name="like_count", dtype=DataType.INT64),
            FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=384),
            FieldSchema(name="summary_vector", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="ê³ ê¸‰ ë¬¸ì„œ ê²€ìƒ‰ ì»¬ë ‰ì…˜",
            enable_dynamic_field=True
        )
        
        collection = Collection(name=collection_name, schema=schema)
        print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        return collection
    
    def create_qa_collection(self, collection_name: str = "qa_pairs") -> Collection:
        """Q&A ë§¤ì¹­ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"\nğŸ“ Q&A ë§¤ì¹­ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="qa_id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="question", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="answer", dtype=DataType.VARCHAR, max_length=3000),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="domain", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="difficulty", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="confidence_score", dtype=DataType.FLOAT),
            FieldSchema(name="created_date", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="updated_date", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="is_verified", dtype=DataType.BOOL),
            FieldSchema(name="usage_count", dtype=DataType.INT64),
            FieldSchema(name="question_vector", dtype=DataType.FLOAT_VECTOR, dim=384),
            FieldSchema(name="answer_vector", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Q&A ë§¤ì¹­ ì»¬ë ‰ì…˜"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        return collection
    
    def generate_sample_documents(self, count: int = 1000) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ ë¬¸ì„œ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸ“Š ìƒ˜í”Œ ë¬¸ì„œ {count}ê°œ ìƒì„± ì¤‘...")
        
        # ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì™€ ë„ë©”ì¸
        categories = ["Technology", "Science", "Business", "Health", "Education", "Entertainment", "Sports", "Politics"]
        doc_types = ["article", "research_paper", "blog_post", "news", "tutorial", "review", "case_study", "whitepaper"]
        languages = ["ko", "en", "ja", "zh"]
        difficulties = ["beginner", "intermediate", "advanced", "expert"]
        authors = [f"Author_{i}" for i in range(1, 51)]
        
        # ìƒ˜í”Œ ì œëª©ê³¼ ë‚´ìš© í…œí”Œë¦¿
        title_templates = [
            "{category}ì—ì„œì˜ í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ë²•",
            "{category} ë¶„ì•¼ì˜ ìµœì‹  ë™í–¥ ë¶„ì„",
            "{category} ì „ë¬¸ê°€ê°€ ë§í•˜ëŠ” í•µì‹¬ í¬ì¸íŠ¸",
            "{category}ì˜ ë¯¸ë˜ ì „ë§ê³¼ ê¸°íšŒ",
            "{category} ì…ë¬¸ìë¥¼ ìœ„í•œ ì™„ë²½ ê°€ì´ë“œ",
            "ì‹¤ë¬´ì§„ì´ ì•Œì•„ì•¼ í•  {category} í•„ìˆ˜ ì§€ì‹",
            "{category} ì„±ê³µ ì‚¬ë¡€ ì‹¬ì¸µ ë¶„ì„",
            "{category} íŠ¸ë Œë“œì™€ ì‹œì¥ ì¸ì‚¬ì´íŠ¸"
        ]
        
        content_templates = [
            "ì´ ë¬¸ì„œëŠ” {category} ë¶„ì•¼ì˜ {topic}ì— ëŒ€í•œ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ìµœì‹  ì—°êµ¬ ê²°ê³¼ì™€ ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ {detail}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•˜ë©°, ì‹¤ì œ ì ìš© ì‚¬ë¡€ì™€ í•¨ê»˜ í–¥í›„ ì „ë§ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "{category} ì˜ì—­ì—ì„œ {topic}ì˜ ì¤‘ìš”ì„±ì´ ë‚ ë¡œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” {detail}ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³ , ì „ë¬¸ê°€ ì¸í„°ë·°ì™€ ë°ì´í„° ë¶„ì„ì„ í†µí•´ ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.",
            "í˜„ëŒ€ {category} í™˜ê²½ì—ì„œ {topic}ëŠ” í•µì‹¬ì ì¸ ì—­í• ì„ ë‹´ë‹¹í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” {detail}ì— ì´ˆì ì„ ë§ì¶° ì´ë¡ ì  ë°°ê²½ë¶€í„° ì‹¤ì œ êµ¬í˜„ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ë©°, ëª¨ë²” ì‚¬ë¡€ë¥¼ í†µí•´ í•™ìŠµ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤."
        ]
        
        documents = []
        
        for i in range(count):
            category = np.random.choice(categories)
            doc_type = np.random.choice(doc_types)
            difficulty = np.random.choice(difficulties)
            author = np.random.choice(authors)
            language = np.random.choice(languages, p=[0.7, 0.2, 0.05, 0.05])  # í•œêµ­ì–´ ë¹„ì¤‘ ë†’ìŒ
            
            # ì œëª© ìƒì„±
            title_template = np.random.choice(title_templates)
            title = title_template.format(category=category)
            
            # ë‚´ìš© ìƒì„±
            content_template = np.random.choice(content_templates)
            topic = f"{category.lower()} topic {i%20+1}"
            detail = f"detailed analysis {i%15+1}"
            content = content_template.format(category=category, topic=topic, detail=detail)
            
            # ì¶”ê°€ ë‚´ìš© í™•ì¥
            content += f" íŠ¹íˆ {category} ë¶„ì•¼ì—ì„œëŠ” {topic}ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ë²•ì´ ì—°êµ¬ë˜ê³  ìˆìœ¼ë©°, {detail}ëŠ” í•µì‹¬ì ì¸ ìš”ì†Œë¡œ ì¸ì‹ë˜ê³  ìˆìŠµë‹ˆë‹¤."
            content += f" ì´ëŸ¬í•œ ê´€ì ì—ì„œ ë³¼ ë•Œ, {difficulty} ìˆ˜ì¤€ì˜ ì´í•´ê°€ í•„ìš”í•˜ë©°, ì‹¤ë¬´ ì ìš©ì„ ìœ„í•´ì„œëŠ” ë‹¨ê³„ì  ì ‘ê·¼ì´ ê¶Œì¥ë©ë‹ˆë‹¤."
            
            # ìš”ì•½ ìƒì„±
            summary = f"{category} ë¶„ì•¼ì˜ {topic}ì— ëŒ€í•œ {difficulty} ìˆ˜ì¤€ì˜ ë¶„ì„. {detail}ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì‹¤ìš©ì  ê°€ì´ë“œ."
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            word_count = len(content.split())
            reading_time = max(1, word_count // 200)  # ë¶„ë‹¹ 200ë‹¨ì–´ ê¸°ì¤€
            quality_score = np.random.uniform(1.0, 5.0)
            view_count = np.random.randint(100, 10000)
            like_count = int(view_count * np.random.uniform(0.01, 0.15))
            
            # ë°œí–‰ì¼ ìƒì„±
            publish_date = f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}"
            
            # íƒœê·¸ ìƒì„±
            tags = [category.lower(), doc_type, difficulty, f"topic_{i%10}"]
            if np.random.random() > 0.5:
                tags.append(f"author_{author.split('_')[1]}")
            
            document = {
                "title": title,
                "content": content,
                "summary": summary,
                "category": category,
                "author": author,
                "publish_date": publish_date,
                "tags": ", ".join(tags),
                "language": language,
                "doc_type": doc_type,
                "word_count": word_count,
                "reading_time": reading_time,
                "difficulty_level": difficulty,
                "quality_score": quality_score,
                "view_count": view_count,
                "like_count": like_count
            }
            
            documents.append(document)
        
        print(f"  âœ… {count}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
        return documents
    
    def generate_sample_qa_pairs(self, count: int = 500) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ Q&A ìŒ ìƒì„±"""
        print(f"\nğŸ“Š ìƒ˜í”Œ Q&A {count}ê°œ ìƒì„± ì¤‘...")
        
        categories = ["Technology", "Science", "Business", "Health", "Education"]
        domains = ["Programming", "AI/ML", "Web Development", "Data Science", "Cybersecurity", 
                  "Biology", "Physics", "Chemistry", "Medicine", "Finance", "Marketing", 
                  "Management", "Nutrition", "Psychology", "Mathematics"]
        difficulties = ["beginner", "intermediate", "advanced"]
        
        # Q&A í…œí”Œë¦¿
        qa_templates = [
            {
                "question": "{domain}ì—ì„œ {topic}ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
                "answer": "{domain} ë¶„ì•¼ì—ì„œ {topic}ë¥¼ ì‹œì‘í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ê¸°ë³¸ ê°œë…ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. {detail}ë¶€í„° ì‹œì‘í•˜ì—¬ ë‹¨ê³„ì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            },
            {
                "question": "{topic}ì˜ ì£¼ìš” ì¥ì ê³¼ ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "{topic}ì˜ ì£¼ìš” ì¥ì ìœ¼ë¡œëŠ” {advantage}ì´ ìˆìœ¼ë©°, ë‹¨ì ìœ¼ë¡œëŠ” {disadvantage}ì´ ìˆìŠµë‹ˆë‹¤. {domain} ë¶„ì•¼ì—ì„œëŠ” ì´ëŸ¬í•œ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆíˆ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
            },
            {
                "question": "{domain} ì „ë¬¸ê°€ê°€ ë˜ê¸° ìœ„í•œ í•™ìŠµ ë¡œë“œë§µì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                "answer": "{domain} ì „ë¬¸ê°€ê°€ ë˜ê¸° ìœ„í•´ì„œëŠ” {step1}, {step2}, {step3} ìˆœì„œë¡œ í•™ìŠµí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ê° ë‹¨ê³„ë§ˆë‹¤ ì¶©ë¶„í•œ ì‹¤ìŠµê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ìŒ“ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
            },
            {
                "question": "{topic} êµ¬í˜„ ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "{topic} êµ¬í˜„ ì‹œì—ëŠ” {caution1}ê³¼ {caution2}ë¥¼ íŠ¹íˆ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. {domain} ë¶„ì•¼ì˜ ëª¨ë²” ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ êµ¬í˜„ì„ ìœ„í•´ ë…¸ë ¥í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            }
        ]
        
        qa_pairs = []
        
        for i in range(count):
            category = np.random.choice(categories)
            domain = np.random.choice(domains)
            difficulty = np.random.choice(difficulties)
            template = np.random.choice(qa_templates)
            
            # ë³€ìˆ˜ ìƒì„±
            topic = f"{domain.lower()} concept {i%20+1}"
            detail = f"fundamental principle {i%10+1}"
            advantage = f"efficiency and scalability {i%5+1}"
            disadvantage = f"complexity and cost {i%3+1}"
            step1 = f"basic {domain.lower()} theory"
            step2 = f"intermediate {domain.lower()} practice"
            step3 = f"advanced {domain.lower()} application"
            caution1 = f"performance optimization {i%7+1}"
            caution2 = f"security consideration {i%6+1}"
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ ìƒì„±
            question = template["question"].format(
                domain=domain, topic=topic, detail=detail,
                advantage=advantage, disadvantage=disadvantage,
                step1=step1, step2=step2, step3=step3,
                caution1=caution1, caution2=caution2
            )
            
            answer = template["answer"].format(
                domain=domain, topic=topic, detail=detail,
                advantage=advantage, disadvantage=disadvantage,
                step1=step1, step2=step2, step3=step3,
                caution1=caution1, caution2=caution2
            )
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            confidence_score = np.random.uniform(0.6, 1.0)
            usage_count = np.random.randint(1, 100)
            is_verified = np.random.random() > 0.3  # 70% ê²€ì¦ë¨
            
            # ë‚ ì§œ ìƒì„±
            created_date = f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}"
            updated_date = created_date if np.random.random() > 0.3 else f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}"
            
            qa_pair = {
                "question": question,
                "answer": answer,
                "category": category,
                "domain": domain,
                "difficulty": difficulty,
                "confidence_score": confidence_score,
                "created_date": created_date,
                "updated_date": updated_date,
                "is_verified": is_verified,
                "usage_count": usage_count
            }
            
            qa_pairs.append(qa_pair)
        
        print(f"  âœ… {count}ê°œ Q&A ìŒ ìƒì„± ì™„ë£Œ")
        return qa_pairs
    
    def insert_documents(self, collection: Collection, documents: List[Dict[str, Any]]) -> None:
        """ë¬¸ì„œ ë°ì´í„° ì‚½ì…"""
        print(f"\nğŸ’¾ ë¬¸ì„œ ë°ì´í„° ì‚½ì… ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë²¡í„°í™”
        print("  í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
        titles = [doc["title"] for doc in documents]
        contents = [doc["content"] for doc in documents]
        summaries = [doc["summary"] for doc in documents]
        
        # ì œëª©+ë‚´ìš© ê²°í•© í…ìŠ¤íŠ¸ ë²¡í„°í™”
        combined_texts = [f"{title} {content}" for title, content in zip(titles, contents)]
        text_vectors = self.vector_utils.texts_to_vectors(combined_texts)
        
        # ìš”ì•½ ë²¡í„°í™”
        summary_vectors = self.vector_utils.texts_to_vectors(summaries)
        
        # ë°ì´í„°ë¥¼ 2ë‹¨ê³„ íŒ¨í„´ìœ¼ë¡œ êµ¬ì„± (List[List])
        data = [
            [doc["title"] for doc in documents],
            [doc["content"] for doc in documents], 
            [doc["summary"] for doc in documents],
            [doc["category"] for doc in documents],
            [doc["author"] for doc in documents],
            [doc["publish_date"] for doc in documents],
            [doc["tags"] for doc in documents],
            [doc["language"] for doc in documents],
            [doc["doc_type"] for doc in documents],
            [doc["word_count"] for doc in documents],
            [doc["reading_time"] for doc in documents],
            [doc["difficulty_level"] for doc in documents],
            [doc["quality_score"] for doc in documents],
            [doc["view_count"] for doc in documents],
            [doc["like_count"] for doc in documents],
            text_vectors.tolist(),
            summary_vectors.tolist()
        ]
        
        # ì‚½ì… (2ë‹¨ê³„ íŒ¨í„´)
        result = collection.insert(data)
        total_inserted = len(documents)
        print(f"  âœ… {total_inserted}ê°œ ë¬¸ì„œ ì‚½ì… ì™„ë£Œ")
        
        # ë°ì´í„° í”ŒëŸ¬ì‹œ
        print("  ë°ì´í„° í”ŒëŸ¬ì‹œ ì¤‘...")
        collection.flush()
        print(f"  âœ… ì´ {total_inserted}ê°œ ë¬¸ì„œ ì‚½ì… ì™„ë£Œ")
    
    def insert_qa_pairs(self, collection: Collection, qa_pairs: List[Dict[str, Any]]) -> None:
        """Q&A ë°ì´í„° ì‚½ì…"""
        print(f"\nğŸ’¾ Q&A ë°ì´í„° ì‚½ì… ì¤‘...")
        
        # ë²¡í„°í™”
        print("  í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
        questions = [qa["question"] for qa in qa_pairs]
        answers = [qa["answer"] for qa in qa_pairs]
        
        question_vectors = self.vector_utils.texts_to_vectors(questions)
        answer_vectors = self.vector_utils.texts_to_vectors(answers)
        
        # ë°ì´í„°ë¥¼ 2ë‹¨ê³„ íŒ¨í„´ìœ¼ë¡œ êµ¬ì„± (List[List])
        data = [
            [qa["question"] for qa in qa_pairs],
            [qa["answer"] for qa in qa_pairs],
            [qa["category"] for qa in qa_pairs],
            [qa["domain"] for qa in qa_pairs],
            [qa["difficulty"] for qa in qa_pairs],
            [qa["confidence_score"] for qa in qa_pairs],
            [qa["created_date"] for qa in qa_pairs],
            [qa["updated_date"] for qa in qa_pairs],  # ì¶”ê°€ëœ í•„ë“œ
            [qa["is_verified"] for qa in qa_pairs],
            [qa["usage_count"] for qa in qa_pairs],
            question_vectors.tolist(),
            answer_vectors.tolist()
        ]
        
        # ì‚½ì… (2ë‹¨ê³„ íŒ¨í„´)
        result = collection.insert(data)
        total_inserted = len(qa_pairs)
        print(f"  âœ… {total_inserted}ê°œ Q&A ì‚½ì… ì™„ë£Œ")
        
        # ë°ì´í„° í”ŒëŸ¬ì‹œ
        print("  ë°ì´í„° í”ŒëŸ¬ì‹œ ì¤‘...")
        collection.flush()
        print(f"  âœ… ì´ {total_inserted}ê°œ Q&A ì‚½ì… ì™„ë£Œ")
    
    def create_indexes(self, collection: Collection, vector_fields: List[str]) -> None:
        """ì¸ë±ìŠ¤ ìƒì„±"""
        print(f"\nğŸ” ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        for field_name in vector_fields:
            index_params = {
                "index_type": "HNSW",
                "metric_type": "COSINE",  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
                "params": {
                    "M": 16,
                    "efConstruction": 200
                }
            }
            
            print(f"  {field_name} ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            collection.create_index(field_name, index_params)
            print(f"  âœ… {field_name} ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        print(f"  âœ… ëª¨ë“  ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    
    def document_search_demo(self, collection: Collection) -> None:
        """ë¬¸ì„œ ê²€ìƒ‰ ë°ëª¨"""
        print("\n" + "="*80)
        print(" ğŸ“– ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë°ëª¨")
        print("="*80)
        
        collection.load()
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤
        search_queries = [
            {
                "query": "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ìµœì‹  ë™í–¥",
                "description": "AI/ML ê´€ë ¨ ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰"
            },
            {
                "query": "ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµê³¼ ê²½ì˜ í˜ì‹  ë°©ë²•",
                "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì•¼ ì „ë¬¸ ìë£Œ ê²€ìƒ‰"
            },
            {
                "query": "ê±´ê°•ê³¼ ì˜í•™ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ë°œê²¬",
                "description": "í—¬ìŠ¤ì¼€ì–´ ê´€ë ¨ ì—°êµ¬ ìë£Œ ê²€ìƒ‰"
            },
            {
                "query": "êµìœ¡ ê¸°ìˆ ê³¼ í•™ìŠµ íš¨ê³¼ í–¥ìƒ",
                "description": "êµìœ¡ ë¶„ì•¼ í˜ì‹  ì‚¬ë¡€ ê²€ìƒ‰"
            }
        ]
        
        for i, search_case in enumerate(search_queries, 1):
            print(f"\n{i}. {search_case['description']}")
            print(f"   ê²€ìƒ‰ì–´: '{search_case['query']}'")
            
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vectors = self.vector_utils.text_to_vector(search_case['query'])
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            # ê¸°ë³¸ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ë²¡í„° ê¸°ë°˜)
            start_time = time.time()
            results = collection.search(
                data=[query_vector],
                anns_field="text_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=5,
                output_fields=["title", "category", "author", "quality_score", "reading_time", "difficulty_level"]
            )
            search_time = time.time() - start_time
            
            print(f"   ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
            print(f"   ê²°ê³¼ ìˆ˜: {len(results[0])}")
            
            for j, hit in enumerate(results[0]):
                similarity = 1 - hit.distance  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                entity = hit.entity
                print(f"     {j+1}. {entity.get('title')[:60]}...")
                print(f"        ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ì €ì: {entity.get('author')}")
                print(f"        í’ˆì§ˆì ìˆ˜: {entity.get('quality_score'):.2f}, ì½ê¸°ì‹œê°„: {entity.get('reading_time')}ë¶„")
                print(f"        ë‚œì´ë„: {entity.get('difficulty_level')}, ìœ ì‚¬ë„: {similarity:.3f}")
            
            # í•„í„°ë§ ê²€ìƒ‰ (ê³ í’ˆì§ˆ ë¬¸ì„œë§Œ)
            print(f"\n   ğŸ“Š ê³ í’ˆì§ˆ ë¬¸ì„œ í•„í„°ë§ ê²€ìƒ‰ (í’ˆì§ˆì ìˆ˜ >= 4.0)")
            
            filtered_results = collection.search(
                data=[query_vector],
                anns_field="text_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=3,
                expr="quality_score >= 4.0",
                output_fields=["title", "category", "quality_score", "view_count", "like_count"]
            )
            
            print(f"   ê³ í’ˆì§ˆ ë¬¸ì„œ ê²°ê³¼ ìˆ˜: {len(filtered_results[0])}")
            for j, hit in enumerate(filtered_results[0]):
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"     {j+1}. {entity.get('title')[:60]}...")
                print(f"        í’ˆì§ˆì ìˆ˜: {entity.get('quality_score'):.2f}, ì¡°íšŒìˆ˜: {entity.get('view_count')}, ì¢‹ì•„ìš”: {entity.get('like_count')}")
                print(f"        ìœ ì‚¬ë„: {similarity:.3f}")
    
    def qa_matching_demo(self, collection: Collection) -> None:
        """Q&A ë§¤ì¹­ ë°ëª¨"""
        print("\n" + "="*80)
        print(" ğŸ¤” Q&A ë§¤ì¹­ ì‹œìŠ¤í…œ ë°ëª¨")
        print("="*80)
        
        collection.load()
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ì‹œë‚˜ë¦¬ì˜¤
        user_questions = [
            {
                "question": "í”„ë¡œê·¸ë˜ë°ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ”ë° ì–´ë–¤ ì–¸ì–´ë¶€í„° ë°°ì›Œì•¼ í• ê¹Œìš”?",
                "description": "í”„ë¡œê·¸ë˜ë° ì…ë¬¸ì ì§ˆë¬¸"
            },
            {
                "question": "ë°ì´í„° ê³¼í•™ìê°€ ë˜ê¸° ìœ„í•œ í•™ìŠµ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "description": "ì»¤ë¦¬ì–´ ì „í™˜ ê´€ë ¨ ì§ˆë¬¸"
            },
            {
                "question": "ì¸ê³µì§€ëŠ¥ í”„ë¡œì íŠ¸ì—ì„œ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì€?",
                "description": "AI í”„ë¡œì íŠ¸ ìµœì í™” ì§ˆë¬¸"
            },
            {
                "question": "ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ìœ„í•œ í•„ìˆ˜ ë„êµ¬ë“¤ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤",
                "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ë„êµ¬ ì¶”ì²œ ì§ˆë¬¸"
            }
        ]
        
        for i, user_q in enumerate(user_questions, 1):
            print(f"\n{i}. {user_q['description']}")
            print(f"   ì‚¬ìš©ì ì§ˆë¬¸: '{user_q['question']}'")
            
            # ì§ˆë¬¸ ë²¡í„°í™”
            query_vectors = self.vector_utils.text_to_vector(user_q['question'])
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            # ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰
            start_time = time.time()
            question_results = collection.search(
                data=[query_vector],
                anns_field="question_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=3,
                output_fields=["question", "answer", "domain", "difficulty", "confidence_score", "is_verified"]
            )
            search_time = time.time() - start_time
            
            print(f"   ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
            print(f"   ë§¤ì¹­ëœ Q&A ìˆ˜: {len(question_results[0])}")
            
            for j, hit in enumerate(question_results[0]):
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"\n     ğŸ“Œ ë§¤ì¹­ Q&A {j+1} (ìœ ì‚¬ë„: {similarity:.3f})")
                print(f"     ì§ˆë¬¸: {entity.get('question')[:80]}...")
                print(f"     ë‹µë³€: {entity.get('answer')[:100]}...")
                print(f"     ë„ë©”ì¸: {entity.get('domain')}, ë‚œì´ë„: {entity.get('difficulty')}")
                print(f"     ì‹ ë¢°ë„: {entity.get('confidence_score'):.2f}, ê²€ì¦ë¨: {entity.get('is_verified')}")
            
            # ê²€ì¦ëœ ë‹µë³€ë§Œ í•„í„°ë§
            print(f"\n   âœ… ê²€ì¦ëœ ë‹µë³€ë§Œ ê²€ìƒ‰")
            verified_results = collection.search(
                data=[query_vector],
                anns_field="question_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=2,
                expr="is_verified == True and confidence_score >= 0.8",
                output_fields=["question", "answer", "domain", "confidence_score"]
            )
            
            print(f"   ê²€ì¦ëœ ë‹µë³€ ìˆ˜: {len(verified_results[0])}")
            for j, hit in enumerate(verified_results[0]):
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"     {j+1}. {entity.get('question')[:60]}...")
                print(f"        ì‹ ë¢°ë„: {entity.get('confidence_score'):.2f}, ìœ ì‚¬ë„: {similarity:.3f}")
    
    def semantic_search_demo(self, collection: Collection) -> None:
        """ì‹œë§¨í‹± ê²€ìƒ‰ ë°ëª¨"""
        print("\n" + "="*80)
        print(" ğŸ§  ì‹œë§¨í‹± ê²€ìƒ‰ ë°ëª¨")
        print("="*80)
        
        collection.load()
        
        # ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ - í‚¤ì›Œë“œê°€ ë‹¤ë¥´ì§€ë§Œ ì˜ë¯¸ê°€ ìœ ì‚¬í•œ ê²€ìƒ‰
        semantic_cases = [
            {
                "query": "ê¸°ê³„ê°€ ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ëŠ” ë°©ë²•",
                "expected": "ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ë¬¸ì„œ",
                "description": "AIë¥¼ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ê²€ìƒ‰"
            },
            {
                "query": "íšŒì‚¬ ìˆ˜ìµì„ ëŠ˜ë¦¬ëŠ” ì „ëµ",
                "expected": "ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥, ë§ˆì¼€íŒ… ê´€ë ¨ ë¬¸ì„œ",
                "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥ì„ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ê²€ìƒ‰"
            },
            {
                "query": "ëª¸ì˜ ë©´ì—­ë ¥ì„ ê°•í™”í•˜ëŠ” ë°©ë²•",
                "expected": "ê±´ê°•, ì˜í•™ ê´€ë ¨ ë¬¸ì„œ",
                "description": "ê±´ê°• ê´€ë¦¬ë¥¼ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ê²€ìƒ‰"
            }
        ]
        
        for i, case in enumerate(semantic_cases, 1):
            print(f"\n{i}. {case['description']}")
            print(f"   ê²€ìƒ‰ì–´: '{case['query']}'")
            print(f"   ê¸°ëŒ€ ê²°ê³¼: {case['expected']}")
            
            # í…ìŠ¤íŠ¸ ë²¡í„°ë¡œ ê²€ìƒ‰
            query_vectors = self.vector_utils.text_to_vector(case['query'])
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            results = collection.search(
                data=[query_vector],
                anns_field="text_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=5,
                output_fields=["title", "category", "summary", "tags"]
            )
            
            print(f"   ê²€ìƒ‰ ê²°ê³¼:")
            for j, hit in enumerate(results[0]):
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"     {j+1}. {entity.get('title')[:70]}...")
                print(f"        ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ìœ ì‚¬ë„: {similarity:.3f}")
                print(f"        ìš”ì•½: {entity.get('summary')[:80]}...")
                print(f"        íƒœê·¸: {entity.get('tags')}")
            
            # ìš”ì•½ ë²¡í„°ë¡œë„ ê²€ìƒ‰í•´ì„œ ë¹„êµ
            print(f"\n   ğŸ“‹ ìš”ì•½ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:")
            summary_results = collection.search(
                data=[query_vector],
                anns_field="summary_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=3,
                output_fields=["title", "summary"]
            )
            
            for j, hit in enumerate(summary_results[0]):
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"     {j+1}. {entity.get('title')[:70]}...")
                print(f"        ìœ ì‚¬ë„: {similarity:.3f}")
                print(f"        ìš”ì•½: {entity.get('summary')}")
    
    def recommendation_demo(self, collection: Collection) -> None:
        """ë¬¸ì„œ ì¶”ì²œ ë°ëª¨"""
        print("\n" + "="*80)
        print(" ğŸ¯ ë¬¸ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ ë°ëª¨")
        print("="*80)
        
        collection.load()
        
        # ì‚¬ìš©ìê°€ ì½ì€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ì¶”ì²œ
        print("ì‚¬ìš©ìê°€ ìµœê·¼ì— ì½ì€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ì„ì˜ì˜ ë¬¸ì„œë¥¼ "ì‚¬ìš©ìê°€ ì½ì€ ë¬¸ì„œ"ë¡œ ê°€ì •
        sample_results = collection.search(
            data=[[0.1] * 384],  # ì„ì˜ì˜ ë²¡í„°
            anns_field="text_vector",
            param={"metric_type": "COSINE", "params": {"ef": 200}},
            limit=1,
            output_fields=["title", "category", "text_vector", "tags", "author"]
        )
        
        if sample_results and len(sample_results[0]) > 0:
            read_doc = sample_results[0][0].entity
            read_vector = read_doc.get('text_vector')
            
            print(f"\nğŸ“– ì‚¬ìš©ìê°€ ì½ì€ ë¬¸ì„œ:")
            print(f"   ì œëª©: {read_doc.get('title')}")
            print(f"   ì¹´í…Œê³ ë¦¬: {read_doc.get('category')}")
            print(f"   ì €ì: {read_doc.get('author')}")
            print(f"   íƒœê·¸: {read_doc.get('tags')}")
            
            # ìœ ì‚¬í•œ ë¬¸ì„œ ì¶”ì²œ
            print(f"\nğŸ¯ ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì¶”ì²œ:")
            
            recommendations = collection.search(
                data=[read_vector],
                anns_field="text_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=6,  # ì²« ë²ˆì§¸ëŠ” ìê¸° ìì‹ ì´ë¯€ë¡œ 6ê°œ ê²€ìƒ‰
                output_fields=["title", "category", "author", "quality_score", "reading_time", "view_count"]
            )
            
            # ìê¸° ìì‹  ì œì™¸
            for j, hit in enumerate(recommendations[0][1:], 1):  # ì²« ë²ˆì§¸ ê²°ê³¼ ì œì™¸
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"   {j}. {entity.get('title')[:70]}...")
                print(f"      ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ì €ì: {entity.get('author')}")
                print(f"      í’ˆì§ˆì ìˆ˜: {entity.get('quality_score'):.2f}, ì½ê¸°ì‹œê°„: {entity.get('reading_time')}ë¶„")
                print(f"      ìœ ì‚¬ë„: {similarity:.3f}, ì¡°íšŒìˆ˜: {entity.get('view_count')}")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì¶”ì²œ
            category = read_doc.get('category')
            print(f"\nğŸ“š ê°™ì€ ì¹´í…Œê³ ë¦¬({category}) ë‚´ ì¶”ì²œ:")
            
            category_recommendations = collection.search(
                data=[read_vector],
                anns_field="text_vector",
                param={"metric_type": "COSINE", "params": {"ef": 200}},
                limit=4,
                expr=f"category == '{category}'",
                output_fields=["title", "author", "quality_score"]
            )
            
            for j, hit in enumerate(category_recommendations[0][1:], 1):  # ìê¸° ìì‹  ì œì™¸
                similarity = 1 - hit.distance
                entity = hit.entity
                print(f"   {j}. {entity.get('title')[:70]}...")
                print(f"      ì €ì: {entity.get('author')}, í’ˆì§ˆì ìˆ˜: {entity.get('quality_score'):.2f}")
                print(f"      ìœ ì‚¬ë„: {similarity:.3f}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹¤ìŠµ")
    print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Milvus ì—°ê²°
        with MilvusConnection() as conn:
            print("âœ… Milvus ì—°ê²° ì„±ê³µ")
            
            # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
            search_engine = TextSimilaritySearchEngine(conn)
            
            # 1. ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
            print("\n" + "="*80)
            print(" ğŸ“– ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•")
            print("="*80)
            
            # ë¬¸ì„œ ì»¬ë ‰ì…˜ ìƒì„±
            doc_collection = search_engine.create_document_collection()
            
            # ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ë° ì‚½ì…
            documents = search_engine.generate_sample_documents(1000)
            search_engine.insert_documents(doc_collection, documents)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            search_engine.create_indexes(doc_collection, ["text_vector", "summary_vector"])
            
            # ë¬¸ì„œ ê²€ìƒ‰ ë°ëª¨
            search_engine.document_search_demo(doc_collection)
            
            # ì‹œë§¨í‹± ê²€ìƒ‰ ë°ëª¨
            search_engine.semantic_search_demo(doc_collection)
            
            # ì¶”ì²œ ì‹œìŠ¤í…œ ë°ëª¨
            search_engine.recommendation_demo(doc_collection)
            
            # 2. Q&A ë§¤ì¹­ ì‹œìŠ¤í…œ
            print("\n" + "="*80)
            print(" ğŸ¤” Q&A ë§¤ì¹­ ì‹œìŠ¤í…œ êµ¬ì¶•")
            print("="*80)
            
            # Q&A ì»¬ë ‰ì…˜ ìƒì„±
            qa_collection = search_engine.create_qa_collection()
            
            # ìƒ˜í”Œ Q&A ìƒì„± ë° ì‚½ì…
            qa_pairs = search_engine.generate_sample_qa_pairs(500)
            search_engine.insert_qa_pairs(qa_collection, qa_pairs)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            search_engine.create_indexes(qa_collection, ["question_vector", "answer_vector"])
            
            # Q&A ë§¤ì¹­ ë°ëª¨
            search_engine.qa_matching_demo(qa_collection)
            
            # ì»¬ë ‰ì…˜ ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘...")
            if utility.has_collection("document_search"):
                utility.drop_collection("document_search")
            if utility.has_collection("qa_pairs"):
                utility.drop_collection("qa_pairs")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        print("\nğŸ‰ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹¤ìŠµ ì™„ë£Œ!")
        
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ë‹¤ì–‘í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ ê¸‰ ë¬¸ì„œ ê²€ìƒ‰")
        print("  â€¢ ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ í‚¤ì›Œë“œ í•œê³„ ê·¹ë³µ")
        print("  â€¢ Q&A ë§¤ì¹­ìœ¼ë¡œ ì§€ëŠ¥í˜• ê³ ê°ì§€ì› êµ¬í˜„")
        print("  â€¢ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê°œì¸í™” ì¶”ì²œ")
        print("  â€¢ í•„í„°ë§ê³¼ ë²¡í„° ê²€ìƒ‰ì˜ íš¨ê³¼ì  ì¡°í•©")
        
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  python step03_use_cases/02_image_similarity_search.py")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 