#!/usr/bin/env python3
"""
Milvus í•™ìŠµ í”„ë¡œì íŠ¸ - 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ (ê°„ë‹¨ ë²„ì „)

2ë‹¨ê³„ì—ì„œ ê²€ì¦ëœ ì•ˆì •ì ì¸ íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
import numpy as np
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from pymilvus import Collection, utility, FieldSchema, CollectionSchema, DataType


def create_text_collection(collection_name: str = "text_search_demo") -> Collection:
    """í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
    print(f"\nğŸ“ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
    if utility.has_collection(collection_name):
        utility.drop_collection(collection_name)
        print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
    
    # ê°„ë‹¨í•œ ìŠ¤í‚¤ë§ˆ (2ë‹¨ê³„ íŒ¨í„´)
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2000),
        FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="score", dtype=DataType.FLOAT),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384)
    ]
    
    schema = CollectionSchema(
        fields=fields,
        description="í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë°ëª¨ ì»¬ë ‰ì…˜"
    )
    
    collection = Collection(name=collection_name, schema=schema)
    print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
    return collection


def generate_sample_documents(count: int = 100) -> List[Dict[str, Any]]:
    """ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±"""
    print(f"\nğŸ“Š ìƒ˜í”Œ ë¬¸ì„œ {count}ê°œ ìƒì„± ì¤‘...")
    
    categories = ["Technology", "Science", "Business", "Health", "Education"]
    authors = [f"Author_{i}" for i in range(1, 21)]
    
    documents = []
    
    for i in range(count):
        category = np.random.choice(categories)
        author = np.random.choice(authors)
        
        title = f"{category} Article {i+1}: Advanced concepts and applications"
        content = f"This {category.lower()} article discusses advanced concepts in {category.lower()}. "
        content += f"Written by {author}, this comprehensive guide covers key principles and practical applications. "
        content += f"The content is designed for professionals and students interested in {category.lower()} research."
        
        doc = {
            "title": title,
            "content": content,
            "category": category,
            "author": author,
            "score": round(np.random.uniform(1.0, 5.0), 2)
        }
        
        documents.append(doc)
    
    print(f"  âœ… {count}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
    return documents


def insert_documents(collection: Collection, documents: List[Dict[str, Any]], vector_utils: VectorUtils) -> None:
    """ë¬¸ì„œ ë°ì´í„° ì‚½ì…"""
    print(f"\nğŸ’¾ ë¬¸ì„œ ë°ì´í„° ì‚½ì… ì¤‘...")
    
    # í…ìŠ¤íŠ¸ ë²¡í„°í™”
    combined_texts = [f"{doc['title']} {doc['content']}" for doc in documents]
    vectors = vector_utils.texts_to_vectors(combined_texts)
    
    # ë°ì´í„°ë¥¼ 2ë‹¨ê³„ íŒ¨í„´ìœ¼ë¡œ êµ¬ì„± (List[List])
    data = [
        [doc["title"] for doc in documents],        # ì œëª©ë“¤
        [doc["content"] for doc in documents],      # ë‚´ìš©ë“¤
        [doc["category"] for doc in documents],     # ì¹´í…Œê³ ë¦¬ë“¤
        [doc["author"] for doc in documents],       # ì €ìë“¤
        [doc["score"] for doc in documents],        # ì ìˆ˜ë“¤
        vectors.tolist()                            # ë²¡í„°ë“¤
    ]
    
    print(f"  ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ:")
    print(f"    ì œëª©: {data[0][0][:50]}...")
    print(f"    ë²¡í„° ê¸¸ì´: {len(data[5][0])}")
    
    # ì‚½ì… (2ë‹¨ê³„ íŒ¨í„´)
    result = collection.insert(data)
    print(f"  ì‚½ì…ëœ ì—”í‹°í‹° ìˆ˜: {len(result.primary_keys)}")
    
    # í”ŒëŸ¬ì‹œ
    collection.flush()
    print(f"  âœ… ë°ì´í„° ì‚½ì… ì™„ë£Œ")


def create_index(collection: Collection) -> None:
    """ì¸ë±ìŠ¤ ìƒì„±"""
    print(f"\nğŸ” ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    
    index_params = {
        "index_type": "HNSW",
        "metric_type": "COSINE",
        "params": {
            "M": 16,
            "efConstruction": 200
        }
    }
    
    collection.create_index("vector", index_params)
    print(f"  âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")


def document_search_demo(collection: Collection, vector_utils: VectorUtils) -> None:
    """ë¬¸ì„œ ê²€ìƒ‰ ë°ëª¨"""
    print("\n" + "="*70)
    print(" ğŸ“– ë¬¸ì„œ ê²€ìƒ‰ ë°ëª¨")
    print("="*70)
    
    collection.load()
    
    search_queries = [
        "artificial intelligence machine learning technology",
        "scientific research methodology analysis",
        "business strategy management innovation",
        "health medical treatment care"
    ]
    
    for i, query in enumerate(search_queries, 1):
        print(f"\n{i}. ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        
        # ì¿¼ë¦¬ ë²¡í„°í™”
        query_vectors = vector_utils.text_to_vector(query)
        query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
        
        # ê²€ìƒ‰
        start_time = time.time()
        results = collection.search(
            data=[query_vector.tolist()],
            anns_field="vector",
            param={"metric_type": "COSINE", "params": {"ef": 200}},
            limit=3,
            output_fields=["title", "category", "author", "score"]
        )
        search_time = time.time() - start_time
        
        print(f"   ê²€ìƒ‰ ì‹œê°„: {search_time:.4f}ì´ˆ")
        print(f"   ê²°ê³¼ ìˆ˜: {len(results[0])}")
        
        for j, hit in enumerate(results[0]):
            similarity = 1 - hit.distance
            entity = hit.entity
            print(f"     {j+1}. {entity.get('title')[:60]}...")
            print(f"        ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ì €ì: {entity.get('author')}")
            print(f"        ì ìˆ˜: {entity.get('score')}, ìœ ì‚¬ë„: {similarity:.3f}")


def text_to_text_search_demo(collection: Collection, vector_utils: VectorUtils) -> None:
    """í…ìŠ¤íŠ¸-í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë°ëª¨"""
    print("\n" + "="*70)
    print(" ğŸ’¬ ì‹œë§¨í‹± ê²€ìƒ‰ ë°ëª¨")
    print("="*70)
    
    semantic_queries = [
        {
            "query": "ê¸°ê³„ê°€ ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ëŠ” ë°©ë²•",
            "description": "AI ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"
        },
        {
            "query": "íšŒì‚¬ ìˆ˜ìµì„ ëŠ˜ë¦¬ëŠ” ì „ëµ",
            "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"
        },
        {
            "query": "ë³‘ì„ ì¹˜ë£Œí•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•",
            "description": "ì˜ë£Œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"
        }
    ]
    
    for i, case in enumerate(semantic_queries, 1):
        print(f"\n{i}. {case['description']}")
        print(f"   ê²€ìƒ‰ì–´: '{case['query']}'")
        
        # ì¿¼ë¦¬ ë²¡í„°í™”
        query_vectors = vector_utils.text_to_vector(case['query'])
        query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
        
        # ê²€ìƒ‰
        results = collection.search(
            data=[query_vector.tolist()],
            anns_field="vector",
            param={"metric_type": "COSINE", "params": {"ef": 200}},
            limit=3,
            output_fields=["title", "category", "score"]
        )
        
        print(f"   ê²€ìƒ‰ ê²°ê³¼:")
        for j, hit in enumerate(results[0]):
            similarity = 1 - hit.distance
            entity = hit.entity
            print(f"     {j+1}. {entity.get('title')[:50]}...")
            print(f"        ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ìœ ì‚¬ë„: {similarity:.3f}")


def filtered_search_demo(collection: Collection, vector_utils: VectorUtils) -> None:
    """í•„í„°ë§ ê²€ìƒ‰ ë°ëª¨"""
    print("\n" + "="*70)
    print(" ğŸ¯ í•„í„°ë§ ê²€ìƒ‰ ë°ëª¨")
    print("="*70)
    
    query_text = "advanced research technology innovation"
    query_vectors = vector_utils.text_to_vector(query_text)
    query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
    
    # 1. ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    print(f"\n1. ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (Technology)")
    results = collection.search(
        data=[query_vector.tolist()],
        anns_field="vector",
        param={"metric_type": "COSINE", "params": {"ef": 200}},
        limit=3,
        expr="category == 'Technology'",
        output_fields=["title", "author", "score"]
    )
    
    print(f"   Technology ì¹´í…Œê³ ë¦¬ ê²°ê³¼:")
    for j, hit in enumerate(results[0]):
        similarity = 1 - hit.distance
        entity = hit.entity
        print(f"     {j+1}. {entity.get('title')[:50]}...")
        print(f"        ì €ì: {entity.get('author')}, ì ìˆ˜: {entity.get('score')}")
        print(f"        ìœ ì‚¬ë„: {similarity:.3f}")
    
    # 2. ì ìˆ˜ í•„í„°ë§
    print(f"\n2. ê³ í’ˆì§ˆ ë¬¸ì„œ í•„í„°ë§ (score >= 4.0)")
    results = collection.search(
        data=[query_vector.tolist()],
        anns_field="vector", 
        param={"metric_type": "COSINE", "params": {"ef": 200}},
        limit=3,
        expr="score >= 4.0",
        output_fields=["title", "category", "score"]
    )
    
    print(f"   ê³ í’ˆì§ˆ ë¬¸ì„œ ê²°ê³¼:")
    for j, hit in enumerate(results[0]):
        similarity = 1 - hit.distance
        entity = hit.entity
        print(f"     {j+1}. {entity.get('title')[:50]}...")
        print(f"        ì¹´í…Œê³ ë¦¬: {entity.get('category')}, ì ìˆ˜: {entity.get('score')}")
        print(f"        ìœ ì‚¬ë„: {similarity:.3f}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹¤ìŠµ (ê°„ë‹¨ ë²„ì „)")
    print(f"ì‹¤í–‰ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Milvus ì—°ê²°
        with MilvusConnection() as conn:
            print("âœ… Milvus ì—°ê²° ì„±ê³µ")
            
            # ë²¡í„° ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
            vector_utils = VectorUtils()
            
            # ì»¬ë ‰ì…˜ ìƒì„±
            collection = create_text_collection()
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì‚½ì…
            documents = generate_sample_documents(100)
            insert_documents(collection, documents, vector_utils)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            create_index(collection)
            
            # ê²€ìƒ‰ ë°ëª¨
            document_search_demo(collection, vector_utils)
            text_to_text_search_demo(collection, vector_utils)
            filtered_search_demo(collection, vector_utils)
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("text_search_demo")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        print("\nğŸ‰ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‹¤ìŠµ ì™„ë£Œ!")
        
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ì°¾ê¸°")
        print("  â€¢ í•„í„°ë§ê³¼ ë²¡í„° ê²€ìƒ‰ì˜ íš¨ê³¼ì  ì¡°í•©")
        print("  â€¢ ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œì˜ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í™œìš©")
        print("  â€¢ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 