#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“ˆ Milvus ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ì‹¤ìŠµ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvusì˜ ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ê¸°ë²•ë“¤ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
- í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ë° ë…¸ë“œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ë°ì´í„° íŒŒí‹°ì…”ë‹ ë° ìƒ¤ë”© ì „ëµ
- ë¡œë“œ ë°¸ëŸ°ì‹± ë° íŠ¸ë˜í”½ ë¶„ì‚°
- ë³µì œë³¸ ê´€ë¦¬ ë° ê³ ê°€ìš©ì„±
- ìŠ¤ì¼€ì¼ ì•„ì›ƒ ì‹œë®¬ë ˆì´ì…˜
- ì„±ëŠ¥ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹
"""

import os
import sys
import time
import logging
import threading
import numpy as np
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Tuple
import json
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DistributedScalingManager:
    """ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.milvus_conn = MilvusConnection()
        self.vector_utils = VectorUtils()
        self.scaling_stats = defaultdict(list)
        self.partition_info = {}
        
    def check_cluster_status(self):
        """í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
        print("ğŸŒ í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸...")
        
        try:
            # Milvus ì„œë²„ ì •ë³´ í™•ì¸
            print("  ğŸ“Š ì„œë²„ ì •ë³´:")
            print(f"    ì—°ê²° ìƒíƒœ: {'ì—°ê²°ë¨' if connections.has_connection('default') else 'ì—°ê²° ëŠê¹€'}")
            
            # ì»¬ë ‰ì…˜ ëª©ë¡ í™•ì¸
            collections = utility.list_collections()
            print(f"    í™œì„± ì»¬ë ‰ì…˜ ìˆ˜: {len(collections)}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì‹œë®¬ë ˆì´ì…˜)
            memory_usage = {
                "total_memory": "8.0GB",
                "used_memory": "2.3GB",
                "available_memory": "5.7GB",
                "memory_usage_percent": 28.8
            }
            
            print(f"  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            for key, value in memory_usage.items():
                print(f"    {key}: {value}")
            
            # CPU ì‚¬ìš©ëŸ‰ (ì‹œë®¬ë ˆì´ì…˜)
            cpu_usage = {
                "cpu_cores": 8,
                "avg_cpu_usage": 45.2,
                "peak_cpu_usage": 78.9,
                "idle_cpu": 54.8
            }
            
            print(f"  ğŸ–¥ï¸  CPU ì‚¬ìš©ëŸ‰:")
            for key, value in cpu_usage.items():
                print(f"    {key}: {value}")
                
            return True
            
        except Exception as e:
            print(f"  âŒ í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def create_partitioned_collection(self, collection_name: str, num_partitions: int = 8) -> Collection:
        """íŒŒí‹°ì…˜ ê¸°ë°˜ ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"ğŸ“‚ íŒŒí‹°ì…˜ ê¸°ë°˜ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        print(f"  ëª©í‘œ íŒŒí‹°ì…˜ ìˆ˜: {num_partitions}ê°œ")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="region", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="priority", dtype=DataType.INT32),
            FieldSchema(name="score", dtype=DataType.FLOAT),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Distributed collection with partitioning strategy"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        
        # íŒŒí‹°ì…˜ ìƒì„± (ì§€ì—­ë³„, ì¹´í…Œê³ ë¦¬ë³„, ì‹œê°„ë³„)
        partition_strategies = [
            # ì§€ì—­ë³„ íŒŒí‹°ì…˜
            "region_us", "region_eu", "region_asia",
            # ì¹´í…Œê³ ë¦¬ë³„ íŒŒí‹°ì…˜
            "category_tech", "category_business", "category_health",
            # ìš°ì„ ìˆœìœ„ë³„ íŒŒí‹°ì…˜
            "priority_high", "priority_normal"
        ]
        
        for partition_name in partition_strategies[:num_partitions]:
            collection.create_partition(partition_name)
            print(f"    âœ… íŒŒí‹°ì…˜ '{partition_name}' ìƒì„±ë¨")
            
        self.partition_info[collection_name] = partition_strategies[:num_partitions]
        print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ ({num_partitions}ê°œ íŒŒí‹°ì…˜)")
        
        return collection
    
    def generate_distributed_data(self, total_size: int = 10000) -> Dict[str, List[List]]:
        """ë¶„ì‚° ì²˜ë¦¬ìš© ë°ì´í„° ìƒì„±"""
        print(f"ğŸ“Š ë¶„ì‚° ì²˜ë¦¬ìš© ë°ì´í„° {total_size:,}ê°œ ìƒì„± ì¤‘...")
        
        # ë°ì´í„° ë¶„í¬ ì •ì˜
        regions = ["us", "eu", "asia"]
        categories = ["tech", "business", "health"] 
        priorities = [1, 2, 3]  # 1: high, 2: normal, 3: low
        
        # ê° íŒŒí‹°ì…˜ë³„ ë°ì´í„° ìƒì„±
        partition_data = {}
        
        for i, partition_name in enumerate(self.partition_info["distributed_collection"]):
            # íŒŒí‹°ì…˜ë³„ ë°ì´í„° í¬ê¸° ê³„ì‚°
            partition_size = total_size // len(self.partition_info["distributed_collection"])
            if i == 0:  # ì²« ë²ˆì§¸ íŒŒí‹°ì…˜ì— ë‚˜ë¨¸ì§€ ë°ì´í„° í• ë‹¹
                partition_size += total_size % len(self.partition_info["distributed_collection"])
            
            print(f"  ğŸ“‚ '{partition_name}' íŒŒí‹°ì…˜: {partition_size:,}ê°œ ë°ì´í„°")
            
            # íŒŒí‹°ì…˜ íŠ¹ì„±ì— ë§ëŠ” ë°ì´í„° ìƒì„±
            if "region_" in partition_name:
                region = partition_name.split("_")[1]
                region_filter = region
                category_filter = None
                priority_filter = None
            elif "category_" in partition_name:
                category_filter = partition_name.split("_")[1]
                region_filter = None
                priority_filter = None
            elif "priority_" in partition_name:
                priority_filter = 1 if "high" in partition_name else 2
                region_filter = None
                category_filter = None
            else:
                region_filter = None
                category_filter = None
                priority_filter = None
            
            # ë°ì´í„° ìƒì„±
            titles = []
            contents = []
            categories_list = []
            regions_list = []
            timestamps = []
            priorities_list = []
            scores = []
            
            for j in range(partition_size):
                # íŒŒí‹°ì…˜ íŠ¹ì„± ë°˜ì˜
                if region_filter:
                    region = region_filter
                else:
                    region = np.random.choice(regions)
                    
                if category_filter:
                    category = category_filter
                else:
                    category = np.random.choice(categories)
                    
                if priority_filter:
                    priority = priority_filter
                else:
                    priority = np.random.choice(priorities)
                
                # ë¬¸ì„œ ìƒì„±
                titles.append(f"{category.title()} Document {j} in {region.upper()}")
                contents.append(f"This is a {category} document from {region} region with priority {priority}. "
                              f"Content includes relevant information for distributed processing.")
                categories_list.append(category)
                regions_list.append(region)
                timestamps.append(int(time.time()) + j)
                priorities_list.append(priority)
                scores.append(np.random.uniform(1.0, 10.0))
            
            # ë²¡í„° ìƒì„±
            vectors = self.vector_utils.texts_to_vectors(titles)
            
            # íŒŒí‹°ì…˜ ë°ì´í„° êµ¬ì¡°í™”
            partition_data[partition_name] = [
                titles,
                contents,
                categories_list,
                regions_list,
                timestamps,
                priorities_list,
                scores,
                vectors.tolist()
            ]
        
        print(f"  âœ… ë¶„ì‚° ë°ì´í„° ìƒì„± ì™„ë£Œ")
        return partition_data
    
    def distributed_data_insertion(self, collection: Collection, partition_data: Dict[str, List[List]]) -> Dict[str, float]:
        """ë¶„ì‚° ë°ì´í„° ì‚½ì…"""
        print("ğŸ’¾ ë¶„ì‚° ë°ì´í„° ì‚½ì… ì¤‘...")
        
        insertion_stats = {}
        
        # ê° íŒŒí‹°ì…˜ì— ë³‘ë ¬ë¡œ ë°ì´í„° ì‚½ì…
        def insert_to_partition(partition_name: str, data: List[List]) -> float:
            start_time = time.time()
            collection.insert(data, partition_name=partition_name)
            collection.flush()
            insertion_time = time.time() - start_time
            
            data_count = len(data[0])  # ì²« ë²ˆì§¸ í•„ë“œì˜ ê¸¸ì´ê°€ ë°ì´í„° ê°œìˆ˜
            print(f"    âœ… '{partition_name}': {data_count:,}ê°œ ì‚½ì… ì™„ë£Œ ({insertion_time:.2f}ì´ˆ)")
            return insertion_time
        
        # ë³‘ë ¬ ì‚½ì… ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(insert_to_partition, partition_name, data): partition_name
                for partition_name, data in partition_data.items()
            }
            
            for future in as_completed(futures):
                partition_name = futures[future]
                try:
                    insertion_time = future.result()
                    insertion_stats[partition_name] = insertion_time
                except Exception as e:
                    print(f"    âŒ '{partition_name}' ì‚½ì… ì‹¤íŒ¨: {e}")
                    insertion_stats[partition_name] = -1
        
        total_time = sum(t for t in insertion_stats.values() if t > 0)
        print(f"  âœ… ì „ì²´ ì‚½ì… ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        return insertion_stats
    
    def create_distributed_indexes(self, collection: Collection) -> Dict[str, float]:
        """ë¶„ì‚° ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ” ë¶„ì‚° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        # ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„° (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•œ ì„¤ì •)
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 256}  # íŒŒí‹°ì…˜ ìˆ˜ì— ë§ì¶° ì¡°ì •
        }
        
        start_time = time.time()
        collection.create_index(
            field_name="vector",
            index_params=index_params
        )
        build_time = time.time() - start_time
        
        print(f"  âœ… ë¶„ì‚° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {build_time:.2f}ì´ˆ")
        
        return {"vector_index": build_time}
    
    def partition_specific_search(self, collection: Collection) -> Dict[str, Any]:
        """íŒŒí‹°ì…˜ë³„ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ” íŒŒí‹°ì…˜ë³„ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        collection.load()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            {"query": "technology innovation artificial intelligence", "partitions": ["region_us", "category_tech"]},
            {"query": "business strategy market analysis", "partitions": ["region_eu", "category_business"]},
            {"query": "healthcare medical research", "partitions": ["region_asia", "category_health"]},
            {"query": "high priority urgent task", "partitions": ["priority_high"]}
        ]
        
        search_results = {}
        
        for i, test in enumerate(test_queries, 1):
            print(f"\n  ğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: '{test['query']}'")
            
            # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
            query_vectors = self.vector_utils.text_to_vector(test['query'])
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            # ì „ì²´ ì»¬ë ‰ì…˜ ê²€ìƒ‰
            start_time = time.time()
            all_results = collection.search(
                data=[query_vector.tolist()],
                anns_field="vector",
                param={"metric_type": "COSINE", "params": {"nprobe": 32}},
                limit=10,
                output_fields=["title", "category", "region", "priority"]
            )
            all_search_time = time.time() - start_time
            
            # íŠ¹ì • íŒŒí‹°ì…˜ ê²€ìƒ‰
            start_time = time.time()
            partition_results = collection.search(
                data=[query_vector.tolist()],
                anns_field="vector",
                param={"metric_type": "COSINE", "params": {"nprobe": 32}},
                limit=10,
                partition_names=test['partitions'],
                output_fields=["title", "category", "region", "priority"]
            )
            partition_search_time = time.time() - start_time
            
            # ê²°ê³¼ ë¶„ì„
            speedup = all_search_time / partition_search_time if partition_search_time > 0 else 0
            
            print(f"    ì „ì²´ ê²€ìƒ‰: {all_search_time*1000:.2f}ms, ê²°ê³¼: {len(all_results[0])}ê°œ")
            print(f"    íŒŒí‹°ì…˜ ê²€ìƒ‰: {partition_search_time*1000:.2f}ms, ê²°ê³¼: {len(partition_results[0])}ê°œ")
            print(f"    ì„±ëŠ¥ í–¥ìƒ: {speedup:.1f}x")
            
            search_results[f"test_{i}"] = {
                "query": test['query'],
                "partitions": test['partitions'],
                "all_search_time": all_search_time,
                "partition_search_time": partition_search_time,
                "speedup": speedup,
                "all_results_count": len(all_results[0]),
                "partition_results_count": len(partition_results[0])
            }
        
        collection.release()
        return search_results
    
    def load_balancing_simulation(self, collection: Collection) -> Dict[str, Any]:
        """ë¡œë“œ ë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜"""
        print("\nâš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜...")
        
        collection.load()
        
        # ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜
        query_patterns = [
            {"type": "regional", "weight": 0.4, "queries": ["news from america", "european markets", "asian technology"]},
            {"type": "categorical", "weight": 0.35, "queries": ["tech innovation", "business analysis", "health research"]},
            {"type": "priority", "weight": 0.25, "queries": ["urgent task", "high priority", "critical update"]}
        ]
        
        # ë™ì‹œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
        def worker_simulation(worker_id: int, num_requests: int) -> Dict[str, Any]:
            worker_stats = {
                "worker_id": worker_id,
                "total_requests": num_requests,
                "total_time": 0,
                "avg_response_time": 0,
                "requests_per_pattern": defaultdict(int)
            }
            
            start_time = time.time()
            
            for i in range(num_requests):
                # ì¿¼ë¦¬ íŒ¨í„´ ì„ íƒ (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
                pattern = np.random.choice(
                    [p["type"] for p in query_patterns],
                    p=[p["weight"] for p in query_patterns]
                )
                
                # íŒ¨í„´ì— ë§ëŠ” ì¿¼ë¦¬ ì„ íƒ
                pattern_info = next(p for p in query_patterns if p["type"] == pattern)
                query = np.random.choice(pattern_info["queries"])
                
                # ë²¡í„° ë³€í™˜ ë° ê²€ìƒ‰
                query_vectors = self.vector_utils.text_to_vector(query)
                query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
                
                # íŒ¨í„´ì— ë”°ë¥¸ íŒŒí‹°ì…˜ ì„ íƒ
                if pattern == "regional":
                    partition_names = ["region_us", "region_eu", "region_asia"]
                elif pattern == "categorical":
                    partition_names = ["category_tech", "category_business", "category_health"]
                else:  # priority
                    partition_names = ["priority_high", "priority_normal"]
                
                # ê²€ìƒ‰ ì‹¤í–‰
                selected_partitions = [np.random.choice(partition_names)]
                collection.search(
                    data=[query_vector.tolist()],
                    anns_field="vector",
                    param={"metric_type": "COSINE", "params": {"nprobe": 16}},
                    limit=5,
                    partition_names=selected_partitions,
                    output_fields=["title"]
                )
                
                worker_stats["requests_per_pattern"][pattern] += 1
            
            worker_stats["total_time"] = time.time() - start_time
            worker_stats["avg_response_time"] = worker_stats["total_time"] / num_requests
            
            return worker_stats
        
        # ë‹¤ì¤‘ ì›Œì»¤ë¡œ ë¶€í•˜ ë¶„ì‚° í…ŒìŠ¤íŠ¸
        print("  ğŸ”„ ë‹¤ì¤‘ ì›Œì»¤ ë¶€í•˜ ë¶„ì‚° í…ŒìŠ¤íŠ¸...")
        
        num_workers = 5
        requests_per_worker = 20
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [
                executor.submit(worker_simulation, i, requests_per_worker)
                for i in range(num_workers)
            ]
            
            worker_results = [future.result() for future in as_completed(futures)]
        
        # ê²°ê³¼ ë¶„ì„
        total_requests = sum(w["total_requests"] for w in worker_results)
        total_time = max(w["total_time"] for w in worker_results)
        avg_response_time = np.mean([w["avg_response_time"] for w in worker_results])
        throughput = total_requests / total_time
        
        print(f"    ì´ ìš”ì²­ ìˆ˜: {total_requests}")
        print(f"    ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"    í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time*1000:.2f}ms")
        print(f"    ì²˜ë¦¬ëŸ‰: {throughput:.1f} requests/sec")
        
        # íŒ¨í„´ë³„ ë¶„í¬ ë¶„ì„
        pattern_distribution = defaultdict(int)
        for worker in worker_results:
            for pattern, count in worker["requests_per_pattern"].items():
                pattern_distribution[pattern] += count
        
        print(f"    íŒ¨í„´ ë¶„í¬:")
        for pattern, count in pattern_distribution.items():
            percentage = (count / total_requests) * 100
            print(f"      {pattern}: {count}íšŒ ({percentage:.1f}%)")
        
        collection.release()
        
        return {
            "total_requests": total_requests,
            "total_time": total_time,
            "avg_response_time": avg_response_time,
            "throughput": throughput,
            "pattern_distribution": dict(pattern_distribution),
            "worker_results": worker_results
        }
    
    def scalability_benchmarking(self, collection: Collection) -> Dict[str, Any]:
        """í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹"""
        print("\nğŸ“ˆ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹...")
        
        collection.load()
        
        # ë‹¤ì–‘í•œ ë¶€í•˜ ë ˆë²¨ í…ŒìŠ¤íŠ¸
        load_levels = [
            {"name": "ì €ë¶€í•˜", "concurrent_users": 2, "requests_per_user": 10},
            {"name": "ì¤‘ë¶€í•˜", "concurrent_users": 5, "requests_per_user": 15},
            {"name": "ê³ ë¶€í•˜", "concurrent_users": 10, "requests_per_user": 20}
        ]
        
        benchmark_results = {}
        
        for load_test in load_levels:
            print(f"\n  ğŸ“Š {load_test['name']} í…ŒìŠ¤íŠ¸:")
            print(f"    ë™ì‹œ ì‚¬ìš©ì: {load_test['concurrent_users']}ëª…")
            print(f"    ì‚¬ìš©ìë‹¹ ìš”ì²­: {load_test['requests_per_user']}íšŒ")
            
            def benchmark_user(user_id: int, requests: int) -> Dict[str, Any]:
                user_times = []
                
                for i in range(requests):
                    query = f"benchmark query {i} from user {user_id}"
                    query_vectors = self.vector_utils.text_to_vector(query)
                    query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
                    
                    start_time = time.time()
                    collection.search(
                        data=[query_vector.tolist()],
                        anns_field="vector",
                        param={"metric_type": "COSINE", "params": {"nprobe": 16}},
                        limit=10,
                        output_fields=["title"]
                    )
                    response_time = time.time() - start_time
                    user_times.append(response_time)
                
                return {
                    "user_id": user_id,
                    "requests": requests,
                    "times": user_times,
                    "avg_time": np.mean(user_times),
                    "p95_time": np.percentile(user_times, 95)
                }
            
            # ë™ì‹œ ì‹¤í–‰
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=load_test['concurrent_users']) as executor:
                futures = [
                    executor.submit(benchmark_user, i, load_test['requests_per_user'])
                    for i in range(load_test['concurrent_users'])
                ]
                user_results = [future.result() for future in as_completed(futures)]
            
            total_test_time = time.time() - start_time
            
            # ê²°ê³¼ ë¶„ì„
            total_requests = sum(r["requests"] for r in user_results)
            all_times = []
            for r in user_results:
                all_times.extend(r["times"])
            
            avg_response_time = np.mean(all_times)
            p95_response_time = np.percentile(all_times, 95)
            p99_response_time = np.percentile(all_times, 99)
            throughput = total_requests / total_test_time
            
            print(f"    ì´ ìš”ì²­ ìˆ˜: {total_requests}")
            print(f"    í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time*1000:.2f}ms")
            print(f"    P95 ì‘ë‹µ ì‹œê°„: {p95_response_time*1000:.2f}ms")
            print(f"    P99 ì‘ë‹µ ì‹œê°„: {p99_response_time*1000:.2f}ms")
            print(f"    ì²˜ë¦¬ëŸ‰: {throughput:.1f} req/sec")
            
            benchmark_results[load_test['name']] = {
                "concurrent_users": load_test['concurrent_users'],
                "requests_per_user": load_test['requests_per_user'],
                "total_requests": total_requests,
                "total_time": total_test_time,
                "avg_response_time": avg_response_time,
                "p95_response_time": p95_response_time,
                "p99_response_time": p99_response_time,
                "throughput": throughput
            }
        
        collection.release()
        return benchmark_results
    
    def resource_utilization_analysis(self) -> Dict[str, Any]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¶„ì„"""
        print("\nğŸ’» ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¶„ì„...")
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ìŠ¤
        resource_metrics = {
            "cpu_utilization": {
                "idle": np.random.uniform(15, 25),
                "user": np.random.uniform(40, 60),
                "system": np.random.uniform(10, 20),
                "iowait": np.random.uniform(2, 8)
            },
            "memory_usage": {
                "total_gb": 16.0,
                "used_gb": np.random.uniform(8, 12),
                "cached_gb": np.random.uniform(2, 4),
                "buffer_gb": np.random.uniform(0.5, 1.5)
            },
            "disk_io": {
                "read_mbps": np.random.uniform(50, 150),
                "write_mbps": np.random.uniform(30, 100),
                "io_utilization": np.random.uniform(20, 60)
            },
            "network": {
                "rx_mbps": np.random.uniform(10, 50),
                "tx_mbps": np.random.uniform(5, 30),
                "connections": np.random.randint(100, 500)
            }
        }
        
        print("  ğŸ“Š í˜„ì¬ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ :")
        
        cpu = resource_metrics["cpu_utilization"]
        cpu_total_used = cpu["user"] + cpu["system"] + cpu["iowait"]
        print(f"    CPU: {cpu_total_used:.1f}% (ì‚¬ìš©ì: {cpu['user']:.1f}%, ì‹œìŠ¤í…œ: {cpu['system']:.1f}%)")
        
        mem = resource_metrics["memory_usage"]
        mem_usage_percent = (mem["used_gb"] / mem["total_gb"]) * 100
        print(f"    ë©”ëª¨ë¦¬: {mem_usage_percent:.1f}% ({mem['used_gb']:.1f}GB / {mem['total_gb']}GB)")
        
        disk = resource_metrics["disk_io"]
        print(f"    ë””ìŠ¤í¬ I/O: ì½ê¸° {disk['read_mbps']:.1f}MB/s, ì“°ê¸° {disk['write_mbps']:.1f}MB/s")
        
        net = resource_metrics["network"]
        print(f"    ë„¤íŠ¸ì›Œí¬: ìˆ˜ì‹  {net['rx_mbps']:.1f}MB/s, ì†¡ì‹  {net['tx_mbps']:.1f}MB/s")
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        if cpu_total_used > 80:
            recommendations.append("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ì›Œì»¤ ë…¸ë“œ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if mem_usage_percent > 85:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì¦ì„¤ ë˜ëŠ” ìºì‹œ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        if disk["io_utilization"] > 70:
            recommendations.append("ë””ìŠ¤í¬ I/Oê°€ í¬í™” ìƒíƒœì…ë‹ˆë‹¤. SSD ì‚¬ìš© ë˜ëŠ” I/O ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”.")
        
        if net["connections"] > 400:
            recommendations.append("ë™ì‹œ ì—°ê²° ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ì—°ê²° í’€ í¬ê¸° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ì´ ì ì • ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        
        print("\n  ğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
        for rec in recommendations:
            print(f"    â€¢ {rec}")
        
        return {
            "metrics": resource_metrics,
            "recommendations": recommendations
        }
    
    def run_distributed_scaling_demo(self):
        """ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ì¢…í•© ë°ëª¨"""
        print("ğŸ“ˆ Milvus ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ì‹¤ìŠµ")
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Milvus ì—°ê²°
            self.milvus_conn.connect()
            print("âœ… Milvus ì—°ê²° ì„±ê³µ\n")
            
            print("=" * 80)
            print(" ğŸŒ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë° í™˜ê²½ í™•ì¸")
            print("=" * 80)
            
            # í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
            cluster_ok = self.check_cluster_status()
            
            if not cluster_ok:
                print("âš ï¸  í´ëŸ¬ìŠ¤í„° ìƒíƒœê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ë‹¨ì¼ ë…¸ë“œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            print("\n" + "=" * 80)
            print(" ğŸ“‚ ë¶„ì‚° ë°ì´í„° ì•„í‚¤í…ì²˜ êµ¬ì¶•")
            print("=" * 80)
            
            # íŒŒí‹°ì…˜ ê¸°ë°˜ ì»¬ë ‰ì…˜ ìƒì„±
            collection = self.create_partitioned_collection("distributed_collection", num_partitions=8)
            
            # ë¶„ì‚° ë°ì´í„° ìƒì„±
            partition_data = self.generate_distributed_data(total_size=8000)
            
            # ë¶„ì‚° ë°ì´í„° ì‚½ì…
            insertion_stats = self.distributed_data_insertion(collection, partition_data)
            
            # ë¶„ì‚° ì¸ë±ìŠ¤ ìƒì„±
            index_stats = self.create_distributed_indexes(collection)
            
            print("\n" + "=" * 80)
            print(" ğŸ” íŒŒí‹°ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ìµœì í™”")
            print("=" * 80)
            
            # íŒŒí‹°ì…˜ë³„ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            search_results = self.partition_specific_search(collection)
            
            print(f"\nğŸ“Š íŒŒí‹°ì…˜ ê²€ìƒ‰ ì„±ëŠ¥ ìš”ì•½:")
            total_speedup = 0
            valid_tests = 0
            
            for test_name, result in search_results.items():
                if result['speedup'] > 0:
                    print(f"  {test_name}: {result['speedup']:.1f}x ì„±ëŠ¥ í–¥ìƒ")
                    total_speedup += result['speedup']
                    valid_tests += 1
            
            if valid_tests > 0:
                avg_speedup = total_speedup / valid_tests
                print(f"  í‰ê·  ì„±ëŠ¥ í–¥ìƒ: {avg_speedup:.1f}x")
            
            print("\n" + "=" * 80)
            print(" âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹± ë° ë™ì‹œì„±")
            print("=" * 80)
            
            # ë¡œë“œ ë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜
            load_balancing_results = self.load_balancing_simulation(collection)
            
            print("\n" + "=" * 80)
            print(" ğŸ“ˆ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹")
            print("=" * 80)
            
            # í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹
            scalability_results = self.scalability_benchmarking(collection)
            
            print(f"\nğŸ“Š í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹ ìš”ì•½:")
            for load_name, result in scalability_results.items():
                print(f"  {load_name}: {result['throughput']:.1f} req/sec, "
                      f"í‰ê·  {result['avg_response_time']*1000:.2f}ms")
            
            print("\n" + "=" * 80)
            print(" ğŸ’» ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë° ìµœì í™”")
            print("=" * 80)
            
            # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ë¶„ì„
            resource_analysis = self.resource_utilization_analysis()
            
            print("\n" + "=" * 80)
            print(" ğŸ¯ ë¶„ì‚° ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­")
            print("=" * 80)
            
            print("\nğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ ê°€ì´ë“œ:")
            print("  ğŸ“Š ë°ì´í„° ë¶„ì‚° ì „ëµ:")
            print("    â€¢ ì§€ì—­ë³„ íŒŒí‹°ì…”ë‹: ì§€ë¦¬ì  ë¶„ì‚° ë° ì§€ì—° ì‹œê°„ ìµœì†Œí™”")
            print("    â€¢ ì¹´í…Œê³ ë¦¬ë³„ íŒŒí‹°ì…”ë‹: ë„ë©”ì¸ íŠ¹í™” ê²€ìƒ‰ ìµœì í™”")
            print("    â€¢ ì‹œê°„ë³„ íŒŒí‹°ì…”ë‹: ìµœì‹  ë°ì´í„° ìš°ì„  ê²€ìƒ‰")
            print("    â€¢ ìš°ì„ ìˆœìœ„ë³„ íŒŒí‹°ì…”ë‹: ì¤‘ìš”ë„ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ í• ë‹¹")
            
            print("\n  âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹±:")
            print("    â€¢ ë¼ìš´ë“œ ë¡œë¹ˆ: ê· ë“± ë¶„ì‚°")
            print("    â€¢ ê°€ì¤‘ì¹˜ ê¸°ë°˜: ë…¸ë“œ ì„±ëŠ¥ì— ë”°ë¥¸ ë¶„ì‚°")
            print("    â€¢ ì§€ì—­ ê¸°ë°˜: ì§€ì—° ì‹œê°„ ìµœì†Œí™”")
            print("    â€¢ ë™ì  ë¶„ì‚°: ì‹¤ì‹œê°„ ë¶€í•˜ ëª¨ë‹ˆí„°ë§")
            
            print("\n  ğŸ“ˆ í™•ì¥ì„± ìµœì í™”:")
            print("    â€¢ ìˆ˜í‰ í™•ì¥: ë…¸ë“œ ì¶”ê°€ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ëŒ€")
            print("    â€¢ ìˆ˜ì§ í™•ì¥: í•˜ë“œì›¨ì–´ ì„±ëŠ¥ í–¥ìƒ")
            print("    â€¢ ì½ê¸° ë³µì œë³¸: ì½ê¸° ì„±ëŠ¥ ë¶„ì‚°")
            print("    â€¢ ìºì‹± ê³„ì¸µ: ë°˜ë³µ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ")
            
            print("\n  ğŸ”§ ìš´ì˜ ëª¨ë²” ì‚¬ë¡€:")
            print("    â€¢ ëª¨ë‹ˆí„°ë§: ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ì¶”ì ")
            print("    â€¢ ì˜¤í†  ìŠ¤ì¼€ì¼ë§: ë¶€í•˜ì— ë”°ë¥¸ ìë™ í™•ì¥")
            print("    â€¢ ì¥ì•  ë³µêµ¬: ê³ ê°€ìš©ì„± ì„¤ê³„")
            print("    â€¢ ë°±ì—… ì „ëµ: ë°ì´í„° ë³´í˜¸ ë° ë³µêµ¬")
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("distributed_collection")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            self.milvus_conn.disconnect()
            
        print(f"\nğŸ‰ ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„± ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ íŒŒí‹°ì…”ë‹ì„ í†µí•œ ë°ì´í„° ë¶„ì‚° ë° ê²€ìƒ‰ ìµœì í™”")
        print("  â€¢ ë¡œë“œ ë°¸ëŸ°ì‹±ìœ¼ë¡œ ì‹œìŠ¤í…œ ë¶€í•˜ ë¶„ì‚°")
        print("  â€¢ í™•ì¥ì„± ë²¤ì¹˜ë§ˆí‚¹ì„ í†µí•œ ì„±ëŠ¥ í•œê³„ íŒŒì•…")
        print("  â€¢ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ì „ëµ ìˆ˜ë¦½")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  python step04_advanced/04_realtime_streaming.py")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scaling_manager = DistributedScalingManager()
    scaling_manager.run_distributed_scaling_demo()

if __name__ == "__main__":
    main() 