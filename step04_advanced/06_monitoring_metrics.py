#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ“Š Milvus ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ìŠ¤ ì‹¤ìŠµ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvusì˜ ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
- ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ ë° ì¶”ì 
- ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
- ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
- ì„±ëŠ¥ ì´ìƒ íƒì§€
"""

import os
import sys
import time
import logging
import threading
import psutil
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import List, Dict, Any, Tuple, Optional
import json
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, collection_window: int = 300):
        self.collection_window = collection_window  # 5ë¶„ ìœˆë„ìš°
        self.metrics_history = defaultdict(lambda: deque(maxlen=1000))
        self.alert_rules = []
        self.is_collecting = False
        self.collection_thread = None
        
    def start_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
        self.is_collecting = True
        self.collection_thread = threading.Thread(target=self._collect_metrics_loop)
        self.collection_thread.start()
        print("ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘ë¨")
    
    def stop_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€"""
        self.is_collecting = False
        if self.collection_thread:
            self.collection_thread.join()
        print("ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€ë¨")
    
    def _collect_metrics_loop(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë£¨í”„"""
        while self.is_collecting:
            try:
                timestamp = time.time()
                
                # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                system_metrics = self._collect_system_metrics()
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                for metric_name, value in system_metrics.items():
                    self.metrics_history[metric_name].append({
                        "timestamp": timestamp,
                        "value": value
                    })
                
                # ì•Œë¦¼ í™•ì¸
                self._check_alerts(system_metrics, timestamp)
                
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
                
            except Exception as e:
                logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    def _collect_system_metrics(self) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_used_gb = memory.used / (1024**3)
            memory_available_gb = memory.available / (1024**3)
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100
            disk_used_gb = disk.used / (1024**3)
            disk_free_gb = disk.free / (1024**3)
            
            # ë„¤íŠ¸ì›Œí¬ I/O
            network = psutil.net_io_counters()
            network_sent_mb = network.bytes_sent / (1024**2)
            network_recv_mb = network.bytes_recv / (1024**2)
            
            # í”„ë¡œì„¸ìŠ¤ ìˆ˜
            process_count = len(psutil.pids())
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "memory_used_gb": memory_used_gb,
                "memory_available_gb": memory_available_gb,
                "disk_percent": disk_percent,
                "disk_used_gb": disk_used_gb,
                "disk_free_gb": disk_free_gb,
                "network_sent_mb": network_sent_mb,
                "network_recv_mb": network_recv_mb,
                "process_count": process_count
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def add_alert_rule(self, metric_name: str, threshold: float, 
                      operator: str = "greater", duration: int = 60):
        """ì•Œë¦¼ ê·œì¹™ ì¶”ê°€"""
        rule = {
            "metric_name": metric_name,
            "threshold": threshold,
            "operator": operator,  # greater, less, equal
            "duration": duration,  # ì´ˆ
            "last_triggered": 0,
            "consecutive_violations": 0
        }
        self.alert_rules.append(rule)
    
    def _check_alerts(self, current_metrics: Dict[str, float], timestamp: float):
        """ì•Œë¦¼ í™•ì¸"""
        for rule in self.alert_rules:
            metric_name = rule["metric_name"]
            
            if metric_name not in current_metrics:
                continue
            
            current_value = current_metrics[metric_name]
            threshold = rule["threshold"]
            
            # ì¡°ê±´ í™•ì¸
            violation = False
            if rule["operator"] == "greater" and current_value > threshold:
                violation = True
            elif rule["operator"] == "less" and current_value < threshold:
                violation = True
            elif rule["operator"] == "equal" and abs(current_value - threshold) < 0.01:
                violation = True
            
            if violation:
                rule["consecutive_violations"] += 1
                
                # ì§€ì† ì‹œê°„ í™•ì¸
                if (rule["consecutive_violations"] * 10 >= rule["duration"] and 
                    timestamp - rule["last_triggered"] > 300):  # 5ë¶„ ê°„ê²©
                    
                    self._trigger_alert(rule, current_value, timestamp)
                    rule["last_triggered"] = timestamp
                    rule["consecutive_violations"] = 0
            else:
                rule["consecutive_violations"] = 0
    
    def _trigger_alert(self, rule: Dict, current_value: float, timestamp: float):
        """ì•Œë¦¼ ë°œìƒ"""
        alert_time = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
        print(f"ğŸš¨ ì•Œë¦¼: {rule['metric_name']} = {current_value:.2f} "
              f"({rule['operator']} {rule['threshold']}) at {alert_time}")
    
    def get_metrics_summary(self, metric_name: str, 
                           time_range: int = 300) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìš”ì•½ í†µê³„"""
        if metric_name not in self.metrics_history:
            return {"error": f"Metric {metric_name} not found"}
        
        current_time = time.time()
        recent_data = [
            point for point in self.metrics_history[metric_name]
            if current_time - point["timestamp"] <= time_range
        ]
        
        if not recent_data:
            return {"error": "No recent data available"}
        
        values = [point["value"] for point in recent_data]
        
        return {
            "metric_name": metric_name,
            "time_range_seconds": time_range,
            "data_points": len(values),
            "current": values[-1] if values else None,
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "std_dev": statistics.stdev(values) if len(values) > 1 else 0
        }

class QueryPerformanceTracker:
    """ì¿¼ë¦¬ ì„±ëŠ¥ ì¶”ì ê¸°"""
    
    def __init__(self, max_history: int = 10000):
        self.query_history = deque(maxlen=max_history)
        self.performance_stats = defaultdict(list)
        self.slow_query_threshold = 1.0  # 1ì´ˆ ì´ìƒì€ ëŠë¦° ì¿¼ë¦¬
        
    def track_query(self, query_type: str, execution_time: float, 
                   result_count: int, parameters: Dict[str, Any] = None):
        """ì¿¼ë¦¬ ì¶”ì """
        query_record = {
            "timestamp": time.time(),
            "query_type": query_type,
            "execution_time": execution_time,
            "result_count": result_count,
            "parameters": parameters or {},
            "is_slow": execution_time > self.slow_query_threshold
        }
        
        self.query_history.append(query_record)
        self.performance_stats[query_type].append(execution_time)
    
    def get_performance_metrics(self, time_range: int = 300) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        current_time = time.time()
        recent_queries = [
            q for q in self.query_history
            if current_time - q["timestamp"] <= time_range
        ]
        
        if not recent_queries:
            return {"message": "No recent queries"}
        
        # ì „ì²´ í†µê³„
        execution_times = [q["execution_time"] for q in recent_queries]
        result_counts = [q["result_count"] for q in recent_queries]
        slow_queries = [q for q in recent_queries if q["is_slow"]]
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ í†µê³„
        query_type_stats = defaultdict(list)
        for query in recent_queries:
            query_type_stats[query["query_type"]].append(query["execution_time"])
        
        type_metrics = {}
        for query_type, times in query_type_stats.items():
            type_metrics[query_type] = {
                "count": len(times),
                "avg_time": statistics.mean(times),
                "max_time": max(times),
                "min_time": min(times)
            }
        
        return {
            "time_range_seconds": time_range,
            "total_queries": len(recent_queries),
            "avg_execution_time": statistics.mean(execution_times),
            "p95_execution_time": np.percentile(execution_times, 95),
            "p99_execution_time": np.percentile(execution_times, 99),
            "avg_result_count": statistics.mean(result_counts),
            "slow_query_count": len(slow_queries),
            "slow_query_percentage": (len(slow_queries) / len(recent_queries)) * 100,
            "qps": len(recent_queries) / time_range,
            "query_type_metrics": type_metrics
        }
    
    def get_slow_queries(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ëŠë¦° ì¿¼ë¦¬ ì¡°íšŒ"""
        slow_queries = [q for q in self.query_history if q["is_slow"]]
        slow_queries.sort(key=lambda x: x["execution_time"], reverse=True)
        return slow_queries[:limit]

class MilvusMonitor:
    """Milvus ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, collection: Collection):
        self.collection = collection
        self.metrics_collector = MetricsCollector()
        self.query_tracker = QueryPerformanceTracker()
        self.vector_utils = VectorUtils()
        self.monitoring_active = False
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        print("ğŸ” Milvus ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
        self.metrics_collector.start_collection()
        
        # ì•Œë¦¼ ê·œì¹™ ì„¤ì •
        self.setup_alert_rules()
        
        self.monitoring_active = True
        print("âœ… ëª¨ë‹ˆí„°ë§ í™œì„±í™”ë¨")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        self.metrics_collector.stop_collection()
        print("âœ… ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
    
    def setup_alert_rules(self):
        """ì•Œë¦¼ ê·œì¹™ ì„¤ì •"""
        # CPU ì‚¬ìš©ë¥  ì•Œë¦¼
        self.metrics_collector.add_alert_rule("cpu_percent", 80.0, "greater", 60)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì•Œë¦¼
        self.metrics_collector.add_alert_rule("memory_percent", 85.0, "greater", 60)
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ì•Œë¦¼
        self.metrics_collector.add_alert_rule("disk_percent", 90.0, "greater", 120)
        
        print("ğŸ“‹ ì•Œë¦¼ ê·œì¹™ ì„¤ì • ì™„ë£Œ")
    
    def execute_monitored_search(self, query_text: str, limit: int = 10, 
                                filters: str = None) -> Tuple[List, float]:
        """ëª¨ë‹ˆí„°ë§ë˜ëŠ” ê²€ìƒ‰ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            # ë²¡í„°í™”
            query_vectors = self.vector_utils.text_to_vector(query_text)
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_params = {"metric_type": "COSINE", "params": {"nprobe": 16}}
            
            results = self.collection.search(
                data=[query_vector.tolist()],
                anns_field="vector",
                param=search_params,
                limit=limit,
                expr=filters,
                output_fields=["content", "source", "priority"]
            )
            
            execution_time = time.time() - start_time
            result_count = len(results[0]) if results and len(results) > 0 else 0
            
            # ì„±ëŠ¥ ì¶”ì 
            self.query_tracker.track_query(
                query_type="vector_search",
                execution_time=execution_time,
                result_count=result_count,
                parameters={"limit": limit, "filters": filters}
            )
            
            return results, execution_time
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ê²€ìƒ‰ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            
            # ì˜¤ë¥˜ë„ ì¶”ì 
            self.query_tracker.track_query(
                query_type="vector_search_error",
                execution_time=execution_time,
                result_count=0,
                parameters={"error": str(e)}
            )
            
            return [], execution_time
    
    def execute_monitored_query(self, expr: str, limit: int = 100) -> Tuple[List, float]:
        """ëª¨ë‹ˆí„°ë§ë˜ëŠ” ì¿¼ë¦¬ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            results = self.collection.query(
                expr=expr,
                limit=limit,
                output_fields=["content", "source", "priority"]
            )
            
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ì¶”ì 
            self.query_tracker.track_query(
                query_type="scalar_query",
                execution_time=execution_time,
                result_count=len(results),
                parameters={"expr": expr, "limit": limit}
            )
            
            return results, execution_time
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            
            # ì˜¤ë¥˜ë„ ì¶”ì 
            self.query_tracker.track_query(
                query_type="scalar_query_error",
                execution_time=execution_time,
                result_count=0,
                parameters={"error": str(e)}
            )
            
            return [], execution_time
    
    def run_performance_stress_test(self, duration: int = 60):
        """ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        print(f"âš¡ ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({duration}ì´ˆ)...")
        
        self.collection.load()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "artificial intelligence machine learning",
            "data science analytics",
            "cloud computing technology",
            "mobile app development",
            "cybersecurity network security",
            "blockchain cryptocurrency",
            "software engineering",
            "database management"
        ]
        
        test_filters = [
            None,
            "priority >= 3",
            "source == 'web'",
            "priority <= 2 and source != 'api'"
        ]
        
        def worker_function(worker_id: int):
            """ì›Œì»¤ í•¨ìˆ˜"""
            end_time = time.time() + duration
            worker_queries = 0
            
            while time.time() < end_time:
                try:
                    # ëœë¤ ì¿¼ë¦¬ ì„ íƒ
                    query = np.random.choice(test_queries)
                    filter_expr = np.random.choice(test_filters)
                    limit = np.random.randint(5, 20)
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    _, exec_time = self.execute_monitored_search(query, limit, filter_expr)
                    worker_queries += 1
                    
                    # ìŠ¤ì¹¼ë¼ ì¿¼ë¦¬ë„ ì¼ë¶€ ì‹¤í–‰
                    if np.random.random() < 0.3:  # 30% í™•ë¥ 
                        expr = np.random.choice(["priority >= 1", "source == 'web'", "priority <= 3"])
                        _, exec_time = self.execute_monitored_query(expr, 50)
                        worker_queries += 1
                    
                    # ê°„ê²© ì¡°ì •
                    time.sleep(np.random.uniform(0.1, 0.5))
                    
                except Exception as e:
                    logger.error(f"ì›Œì»¤ {worker_id} ì˜¤ë¥˜: {e}")
            
            return worker_queries
        
        # ë™ì‹œ ì›Œì»¤ ì‹¤í–‰
        num_workers = 5
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(worker_function, i) for i in range(num_workers)]
            worker_results = [future.result() for future in as_completed(futures)]
        
        total_queries = sum(worker_results)
        
        self.collection.release()
        
        print(f"  âœ… ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"  ğŸ“Š ì´ ì¿¼ë¦¬ ìˆ˜: {total_queries}")
        print(f"  âš¡ í‰ê·  QPS: {total_queries/duration:.1f}")
        
        return {
            "duration": duration,
            "total_queries": total_queries,
            "average_qps": total_queries / duration,
            "worker_count": num_workers
        }
    
    def generate_monitoring_report(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        current_time = datetime.now()
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìš”ì•½
        system_metrics = {}
        key_metrics = ["cpu_percent", "memory_percent", "disk_percent"]
        
        for metric in key_metrics:
            summary = self.metrics_collector.get_metrics_summary(metric, 300)
            system_metrics[metric] = summary
        
        # ì¿¼ë¦¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        query_metrics = self.query_tracker.get_performance_metrics(300)
        
        # ëŠë¦° ì¿¼ë¦¬ ëª©ë¡
        slow_queries = self.query_tracker.get_slow_queries(5)
        
        # ì»¬ë ‰ì…˜ ìƒíƒœ
        collection_stats = {
            "name": self.collection.name,
            "num_entities": self.collection.num_entities,
            "description": self.collection.description
        }
        
        # ë³´ê³ ì„œ êµ¬ì„±
        report = {
            "generated_at": current_time.isoformat(),
            "monitoring_period": "Last 5 minutes",
            "system_metrics": system_metrics,
            "query_performance": query_metrics,
            "slow_queries": slow_queries,
            "collection_status": collection_stats,
            "alert_rules_count": len(self.metrics_collector.alert_rules),
            "recommendations": self._generate_recommendations(system_metrics, query_metrics)
        }
        
        return report
    
    def _generate_recommendations(self, system_metrics: Dict, query_metrics: Dict) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # CPU ì‚¬ìš©ë¥  í™•ì¸
        if system_metrics.get("cpu_percent", {}).get("mean", 0) > 70:
            recommendations.append("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ ìµœì í™” ë˜ëŠ” ìŠ¤ì¼€ì¼ ì•„ì›ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
        if system_metrics.get("memory_percent", {}).get("mean", 0) > 80:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ìµœì í™” ë˜ëŠ” ë©”ëª¨ë¦¬ ì¦ì„¤ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ëŠë¦° ì¿¼ë¦¬ í™•ì¸
        if query_metrics.get("slow_query_percentage", 0) > 10:
            recommendations.append("ëŠë¦° ì¿¼ë¦¬ê°€ ë§ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬ íŒ¨í„´ì„ ê²€í† í•˜ì„¸ìš”.")
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ í™•ì¸
        if query_metrics.get("avg_execution_time", 0) > 0.5:
            recommendations.append("í‰ê·  ì‘ë‹µ ì‹œê°„ì´ ë†’ìŠµë‹ˆë‹¤. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        return recommendations

class MonitoringDashboard:
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, monitor: MilvusMonitor):
        self.monitor = monitor
        
    def display_realtime_dashboard(self, refresh_interval: int = 10, duration: int = 60):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ í‘œì‹œ"""
        print("ğŸ–¥ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
        print("=" * 80)
        
        end_time = time.time() + duration
        iteration = 0
        
        while time.time() < end_time:
            iteration += 1
            
            # í—¤ë” ì •ë³´
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(f"\nğŸ“Š ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸ #{iteration} - {current_time}")
            print("-" * 60)
            
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
            print("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
            cpu_metrics = self.monitor.metrics_collector.get_metrics_summary("cpu_percent", 60)
            mem_metrics = self.monitor.metrics_collector.get_metrics_summary("memory_percent", 60)
            
            if cpu_metrics.get("current") is not None:
                print(f"  CPU: {cpu_metrics['current']:.1f}% (í‰ê· : {cpu_metrics['mean']:.1f}%)")
            if mem_metrics.get("current") is not None:
                print(f"  ë©”ëª¨ë¦¬: {mem_metrics['current']:.1f}% (í‰ê· : {mem_metrics['mean']:.1f}%)")
            
            # ì¿¼ë¦¬ ì„±ëŠ¥
            print("\nâš¡ ì¿¼ë¦¬ ì„±ëŠ¥:")
            query_metrics = self.monitor.query_tracker.get_performance_metrics(60)
            
            if query_metrics.get("total_queries", 0) > 0:
                print(f"  ì´ ì¿¼ë¦¬: {query_metrics['total_queries']}")
                print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {query_metrics['avg_execution_time']*1000:.2f}ms")
                print(f"  QPS: {query_metrics['qps']:.1f}")
                print(f"  ëŠë¦° ì¿¼ë¦¬: {query_metrics['slow_query_count']}")
            else:
                print("  ì¿¼ë¦¬ ë°ì´í„° ì—†ìŒ")
            
            # ì»¬ë ‰ì…˜ ìƒíƒœ
            print(f"\nğŸ“ ì»¬ë ‰ì…˜ ìƒíƒœ:")
            print(f"  ì´ë¦„: {self.monitor.collection.name}")
            print(f"  ì—”í‹°í‹° ìˆ˜: {self.monitor.collection.num_entities:,}")
            
            # ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ ëŒ€ê¸°
            if time.time() + refresh_interval < end_time:
                print(f"\nâ³ {refresh_interval}ì´ˆ í›„ ì—…ë°ì´íŠ¸...")
                time.sleep(refresh_interval)
            else:
                break
        
        print("\nâœ… ëŒ€ì‹œë³´ë“œ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ")

class MonitoringManager:
    """ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.milvus_conn = MilvusConnection()
        self.vector_utils = VectorUtils()
        self.monitor = None
        self.dashboard = None
        
    def create_monitoring_collection(self, collection_name: str, data_size: int = 5000) -> Collection:
        """ëª¨ë‹ˆí„°ë§ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"ğŸ—ï¸ ëª¨ë‹ˆí„°ë§ìš© ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="priority", dtype=DataType.INT32),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="score", dtype=DataType.FLOAT),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Collection for monitoring and metrics testing"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° {data_size:,}ê°œ ìƒì„± ì¤‘...")
        
        sources = ["web", "mobile", "api", "batch", "stream"]
        priorities = [1, 2, 3, 4, 5]
        
        contents = []
        source_list = []
        priority_list = []
        timestamp_list = []
        score_list = []
        
        for i in range(data_size):
            contents.append(f"Monitoring test document {i} with various performance characteristics")
            source_list.append(np.random.choice(sources))
            priority_list.append(np.random.choice(priorities))
            timestamp_list.append(int(time.time()) + i)
            score_list.append(np.random.uniform(1.0, 10.0))
        
        # ë²¡í„° ìƒì„±
        vectors = self.vector_utils.texts_to_vectors(contents)
        
        # ë°ì´í„° ì‚½ì…
        data = [
            contents,
            source_list,
            priority_list,
            timestamp_list,
            score_list,
            vectors.tolist()
        ]
        
        collection.insert(data)
        collection.flush()
        
        # ì¸ë±ìŠ¤ ìƒì„±
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        collection.create_index("vector", index_params)
        
        print(f"  âœ… ëª¨ë‹ˆí„°ë§ìš© ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ ({data_size:,}ê°œ ì—”í‹°í‹°)")
        return collection
    
    def run_monitoring_demo(self):
        """ëª¨ë‹ˆí„°ë§ ì¢…í•© ë°ëª¨"""
        print("ğŸ“Š Milvus ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ìŠ¤ ì‹¤ìŠµ")
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Milvus ì—°ê²°
            self.milvus_conn.connect()
            print("âœ… Milvus ì—°ê²° ì„±ê³µ\n")
            
            print("=" * 80)
            print(" ğŸ—ï¸ ëª¨ë‹ˆí„°ë§ í™˜ê²½ êµ¬ì¶•")
            print("=" * 80)
            
            # ëª¨ë‹ˆí„°ë§ìš© ì»¬ë ‰ì…˜ ìƒì„±
            collection = self.create_monitoring_collection("monitoring_test_collection", 3000)
            
            # ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            self.monitor = MilvusMonitor(collection)
            self.dashboard = MonitoringDashboard(self.monitor)
            
            print("\n" + "=" * 80)
            print(" ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
            print("=" * 80)
            
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            self.monitor.start_monitoring()
            
            print("\n" + "=" * 80)
            print(" âš¡ ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸")
            print("=" * 80)
            
            # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
            stress_results = self.monitor.run_performance_stress_test(duration=30)
            
            print("\n" + "=" * 80)
            print(" ğŸ–¥ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
            print("=" * 80)
            
            # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ (30ì´ˆ)
            self.dashboard.display_realtime_dashboard(refresh_interval=10, duration=30)
            
            print("\n" + "=" * 80)
            print(" ğŸ“‹ ì¢…í•© ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ")
            print("=" * 80)
            
            # ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ ìƒì„±
            report = self.monitor.generate_monitoring_report()
            
            # ë³´ê³ ì„œ ì¶œë ¥
            print(f"\nğŸ“Š ëª¨ë‹ˆí„°ë§ ë³´ê³ ì„œ ({report['monitoring_period']}):")
            print(f"  ìƒì„± ì‹œê°„: {report['generated_at'][:19]}")
            
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
            print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­:")
            for metric_name, summary in report["system_metrics"].items():
                if "error" not in summary:
                    print(f"  {metric_name}: í‰ê·  {summary['mean']:.1f}%, "
                          f"ìµœëŒ€ {summary['max']:.1f}%, í˜„ì¬ {summary['current']:.1f}%")
            
            # ì¿¼ë¦¬ ì„±ëŠ¥
            print(f"\nâš¡ ì¿¼ë¦¬ ì„±ëŠ¥:")
            query_perf = report["query_performance"]
            if "total_queries" in query_perf:
                print(f"  ì´ ì¿¼ë¦¬: {query_perf['total_queries']}")
                print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {query_perf['avg_execution_time']*1000:.2f}ms")
                print(f"  P95 ì‘ë‹µì‹œê°„: {query_perf['p95_execution_time']*1000:.2f}ms")
                print(f"  QPS: {query_perf['qps']:.1f}")
                print(f"  ëŠë¦° ì¿¼ë¦¬ìœ¨: {query_perf['slow_query_percentage']:.1f}%")
            
            # ëŠë¦° ì¿¼ë¦¬
            slow_queries = report["slow_queries"]
            if slow_queries:
                print(f"\nğŸŒ ìƒìœ„ ëŠë¦° ì¿¼ë¦¬:")
                for i, query in enumerate(slow_queries[:3], 1):
                    print(f"  {i}. {query['query_type']}: {query['execution_time']*1000:.2f}ms "
                          f"(ê²°ê³¼: {query['result_count']}ê°œ)")
            
            # ê¶Œì¥ì‚¬í•­
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for rec in report["recommendations"]:
                print(f"  â€¢ {rec}")
            
            print("\n" + "=" * 80)
            print(" ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëª¨ë²” ì‚¬ë¡€")
            print("=" * 80)
            
            print("\nğŸ¯ í•µì‹¬ ë©”íŠ¸ë¦­:")
            print("  ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
            print("    â€¢ QPS (Queries Per Second): ì²˜ë¦¬ëŸ‰")
            print("    â€¢ ì‘ë‹µ ì‹œê°„: P50, P95, P99 ì§€ì—°ì‹œê°„")
            print("    â€¢ ì˜¤ë¥˜ìœ¨: ì‹¤íŒ¨í•œ ì¿¼ë¦¬ ë¹„ìœ¨")
            print("    â€¢ ì²˜ë¦¬ëŸ‰: ì´ˆë‹¹ ì²˜ë¦¬ëœ ë°ì´í„°ëŸ‰")
            
            print("\n  ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
            print("    â€¢ CPU ì‚¬ìš©ë¥ : í”„ë¡œì„¸ì„œ ë¶€í•˜")
            print("    â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : RAM ì‚¬ìš©ëŸ‰")
            print("    â€¢ ë””ìŠ¤í¬ I/O: ì €ì¥ì†Œ ì„±ëŠ¥")
            print("    â€¢ ë„¤íŠ¸ì›Œí¬: ëŒ€ì—­í­ ì‚¬ìš©ëŸ‰")
            
            print("\nğŸš¨ ì•Œë¦¼ ì „ëµ:")
            print("  âš¡ ì‹¤ì‹œê°„ ì•Œë¦¼:")
            print("    â€¢ ì„ê³„ê°’ ê¸°ë°˜: CPU > 80%, Memory > 85%")
            print("    â€¢ íŠ¸ë Œë“œ ê¸°ë°˜: ì„±ëŠ¥ ì €í•˜ íŒ¨í„´ ê°ì§€")
            print("    â€¢ ì´ìƒ íƒì§€: ë¹„ì •ìƒì ì¸ íŒ¨í„´ ê°ì§€")
            
            print("\n  ğŸ“§ ì•Œë¦¼ ì±„ë„:")
            print("    â€¢ ì´ë©”ì¼: ì¤‘ìš”í•œ ì¥ì•  ì•Œë¦¼")
            print("    â€¢ Slack/Teams: íŒ€ í˜‘ì—… ì±„ë„")
            print("    â€¢ SMS: ê¸´ê¸‰ ìƒí™© ì•Œë¦¼")
            print("    â€¢ ëŒ€ì‹œë³´ë“œ: ì‹¤ì‹œê°„ ì‹œê°í™”")
            
            print("\nğŸ“Š ëŒ€ì‹œë³´ë“œ êµ¬ì„±:")
            print("  ğŸ” ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:")
            print("    â€¢ ì‹œìŠ¤í…œ ê°œìš”: ì „ì²´ ìƒíƒœ í•œëˆˆì—")
            print("    â€¢ ì„±ëŠ¥ ê·¸ë˜í”„: ì‹œê³„ì—´ ë°ì´í„°")
            print("    â€¢ ë¡œê·¸ ë¶„ì„: ì˜¤ë¥˜ ë° ê²½ê³  ì¶”ì ")
            
            print("\n  ğŸ“ˆ ë¶„ì„ ë„êµ¬:")
            print("    â€¢ ì„±ëŠ¥ íŠ¸ë Œë“œ: ì¥ê¸° íŒ¨í„´ ë¶„ì„")
            print("    â€¢ ìš©ëŸ‰ ê³„íš: ë¦¬ì†ŒìŠ¤ ì˜ˆì¸¡")
            print("    â€¢ ë¹„êµ ë¶„ì„: ê¸°ê°„ë³„ ì„±ëŠ¥ ë¹„êµ")
            
            print("\nğŸ› ï¸ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
            print("  âš¡ ì„±ëŠ¥ ìµœì í™”:")
            print("    â€¢ ì¸ë±ìŠ¤ íŠœë‹: ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ")
            print("    â€¢ ì¿¼ë¦¬ ìµœì í™”: ë¹„íš¨ìœ¨ì  ì¿¼ë¦¬ ê°œì„ ")
            print("    â€¢ ìºì‹± ì „ëµ: ë°˜ë³µ ì¿¼ë¦¬ ìµœì í™”")
            
            print("\n  ğŸ“ ìš©ëŸ‰ ê´€ë¦¬:")
            print("    â€¢ ìŠ¤í† ë¦¬ì§€ ìµœì í™”: ì••ì¶• ë° ì •ë¦¬")
            print("    â€¢ ë©”ëª¨ë¦¬ ê´€ë¦¬: íš¨ìœ¨ì  í• ë‹¹")
            print("    â€¢ ìŠ¤ì¼€ì¼ë§: ìˆ˜í‰/ìˆ˜ì§ í™•ì¥")
            
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.monitor.stop_monitoring()
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("monitoring_test_collection")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            if self.monitor:
                self.monitor.stop_monitoring()
            self.milvus_conn.disconnect()
            
        print(f"\nğŸ‰ ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ìŠ¤ ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§")
        print("  â€¢ ì¿¼ë¦¬ ì„±ëŠ¥ ì¶”ì  ë° ë¶„ì„")
        print("  â€¢ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ì„ê³„ê°’ ê´€ë¦¬")
        print("  â€¢ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ë° ë³´ê³ ì„œ ìƒì„±")
        print("\nğŸŠ 4ë‹¨ê³„ ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™” ì™„ë£Œ!")
        print("  âœ… ì„±ëŠ¥ ìµœì í™”")
        print("  âœ… ê³ ê¸‰ ì¸ë±ì‹±") 
        print("  âœ… ë¶„ì‚° ì²˜ë¦¬ ë° í™•ì¥ì„±")
        print("  âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°")
        print("  âœ… ë°±ì—… ë° ë³µêµ¬")
        print("  âœ… ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ìŠ¤")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    monitoring_manager = MonitoringManager()
    monitoring_manager.run_monitoring_demo()

if __name__ == "__main__":
    main() 