#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ’¾ Milvus ë°±ì—… ë° ë³µêµ¬ ì‹¤ìŠµ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvusì˜ ë°±ì—… ë° ë³µêµ¬ ì „ëµì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
- ì „ì²´ ì»¬ë ‰ì…˜ ë°±ì—… ë° ë³µì›
- ì¦ë¶„ ë°±ì—… ë° ë³µêµ¬
- ë©”íƒ€ë°ì´í„° ë°±ì—… ê´€ë¦¬
- ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
- ë³µêµ¬ ê²€ì¦ ë° ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
- ìë™í™”ëœ ë°±ì—… ìŠ¤ì¼€ì¤„ë§
"""

import os
import sys
import time
import logging
import shutil
import pickle
import json
import gzip
import hashlib
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import threading
from concurrent.futures import ThreadPoolExecutor

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BackupManager:
    """ë°±ì—… ê´€ë¦¬ì"""
    
    def __init__(self, backup_root: str = "./backups"):
        self.backup_root = Path(backup_root)
        self.backup_root.mkdir(exist_ok=True)
        self.vector_utils = VectorUtils()
        self.backup_metadata = {}
        
    def create_full_backup(self, collection: Collection, backup_name: str) -> Dict[str, Any]:
        """ì „ì²´ ë°±ì—… ìƒì„±"""
        print(f"ğŸ’¾ ì „ì²´ ë°±ì—… ìƒì„±: '{backup_name}'...")
        
        backup_path = self.backup_root / backup_name
        backup_path.mkdir(exist_ok=True)
        
        start_time = time.time()
        
        try:
            # 1. ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„° ë°±ì—…
            metadata = self._backup_collection_metadata(collection, backup_path)
            
            # 2. ì¸ë±ìŠ¤ ì •ë³´ ë°±ì—…
            index_info = self._backup_index_metadata(collection, backup_path)
            
            # 3. ë°ì´í„° ë°±ì—…
            data_info = self._backup_collection_data(collection, backup_path)
            
            # 4. ë°±ì—… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
            manifest = {
                "backup_name": backup_name,
                "collection_name": collection.name,
                "backup_type": "full",
                "created_at": datetime.now().isoformat(),
                "backup_path": str(backup_path),
                "metadata": metadata,
                "index_info": index_info,
                "data_info": data_info,
                "backup_size_mb": self._calculate_backup_size(backup_path),
                "backup_duration": time.time() - start_time
            }
            
            # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
            manifest_path = backup_path / "manifest.json"
            with open(manifest_path, 'w') as f:
                json.dump(manifest, f, indent=2)
            
            self.backup_metadata[backup_name] = manifest
            
            print(f"  âœ… ë°±ì—… ì™„ë£Œ: {manifest['backup_duration']:.2f}ì´ˆ")
            print(f"  ğŸ“Š ë°±ì—… í¬ê¸°: {manifest['backup_size_mb']:.1f}MB")
            print(f"  ğŸ“ ë°±ì—… ê²½ë¡œ: {backup_path}")
            
            return manifest
            
        except Exception as e:
            logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•œ ë°±ì—… ë””ë ‰í† ë¦¬ ì •ë¦¬
            if backup_path.exists():
                shutil.rmtree(backup_path)
            raise
    
    def _backup_collection_metadata(self, collection: Collection, backup_path: Path) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„° ë°±ì—…"""
        print("  ğŸ“‹ ë©”íƒ€ë°ì´í„° ë°±ì—… ì¤‘...")
        
        # ìŠ¤í‚¤ë§ˆ ì •ë³´
        schema_info = {
            "description": collection.description,
            "fields": []
        }
        
        for field in collection.schema.fields:
            field_info = {
                "name": field.name,
                "dtype": str(field.dtype),
                "is_primary": field.is_primary,
                "auto_id": field.auto_id if hasattr(field, 'auto_id') else False,
                "description": field.description
            }
            
            # ë²¡í„° í•„ë“œì˜ ì°¨ì› ì •ë³´
            if field.dtype in [DataType.FLOAT_VECTOR, DataType.BINARY_VECTOR]:
                field_info["dim"] = field.params.get("dim", 0)
            
            # VARCHAR í•„ë“œì˜ ìµœëŒ€ ê¸¸ì´
            if field.dtype == DataType.VARCHAR:
                field_info["max_length"] = field.params.get("max_length", 0)
            
            schema_info["fields"].append(field_info)
        
        # íŒŒí‹°ì…˜ ì •ë³´
        partitions_info = []
        for partition in collection.partitions:
            partitions_info.append({
                "name": partition.name,
                "description": partition.description
            })
        
        metadata = {
            "schema": schema_info,
            "partitions": partitions_info,
            "num_entities": collection.num_entities
        }
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
        metadata_path = backup_path / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"    âœ… ë©”íƒ€ë°ì´í„° ë°±ì—… ì™„ë£Œ ({len(schema_info['fields'])}ê°œ í•„ë“œ)")
        return metadata
    
    def _backup_index_metadata(self, collection: Collection, backup_path: Path) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ë°±ì—…"""
        print("  ğŸ” ì¸ë±ìŠ¤ ì •ë³´ ë°±ì—… ì¤‘...")
        
        index_info = {}
        
        try:
            # ë²¡í„° í•„ë“œì˜ ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            for field in collection.schema.fields:
                if field.dtype in [DataType.FLOAT_VECTOR, DataType.BINARY_VECTOR]:
                    try:
                        index = collection.index(field.name)
                        if index:
                            index_info[field.name] = {
                                "index_type": index.params.get("index_type"),
                                "metric_type": index.params.get("metric_type"),
                                "params": index.params.get("params", {})
                            }
                    except Exception as e:
                        logger.warning(f"ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ {field.name}: {e}")
        
        except Exception as e:
            logger.warning(f"ì¸ë±ìŠ¤ ì •ë³´ ë°±ì—… ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì¸ë±ìŠ¤ ì •ë³´ íŒŒì¼ ì €ì¥
        index_path = backup_path / "indexes.json"
        with open(index_path, 'w') as f:
            json.dump(index_info, f, indent=2)
        
        print(f"    âœ… ì¸ë±ìŠ¤ ì •ë³´ ë°±ì—… ì™„ë£Œ ({len(index_info)}ê°œ ì¸ë±ìŠ¤)")
        return index_info
    
    def _backup_collection_data(self, collection: Collection, backup_path: Path) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ë°ì´í„° ë°±ì—…"""
        print("  ğŸ“Š ë°ì´í„° ë°±ì—… ì¤‘...")
        
        collection.load()
        
        try:
            # ëª¨ë“  ë°ì´í„° ê²€ìƒ‰ (í° ì»¬ë ‰ì…˜ì˜ ê²½ìš° ë°°ì¹˜ ì²˜ë¦¬ í•„ìš”)
            limit = 1000  # ë°°ì¹˜ í¬ê¸°
            offset = 0
            total_entities = 0
            batch_count = 0
            
            # ì¶œë ¥ í•„ë“œ ê²°ì •
            output_fields = [field.name for field in collection.schema.fields 
                           if not field.is_primary or not getattr(field, 'auto_id', False)]
            
            data_batches = []
            
            while True:
                # ë°ì´í„° ê²€ìƒ‰ (ë²¡í„° í•„ë“œëŠ” ì œì™¸í•˜ê³  ê²€ìƒ‰)
                vector_fields = [field.name for field in collection.schema.fields 
                               if field.dtype in [DataType.FLOAT_VECTOR, DataType.BINARY_VECTOR]]
                
                non_vector_fields = [field.name for field in collection.schema.fields 
                                   if field.name not in vector_fields and 
                                   (not field.is_primary or not getattr(field, 'auto_id', False))]
                
                if non_vector_fields:
                    # ìŠ¤ì¹¼ë¼ ë°ì´í„° ê²€ìƒ‰
                    query_results = collection.query(
                        expr="",
                        offset=offset,
                        limit=limit,
                        output_fields=non_vector_fields
                    )
                    
                    if not query_results:
                        break
                    
                    # ë²¡í„° ë°ì´í„° ë³„ë„ ì²˜ë¦¬ (ê²€ìƒ‰ì„ í†µí•´)
                    if vector_fields:
                        # ë”ë¯¸ ë²¡í„°ë¡œ ê²€ìƒ‰í•˜ì—¬ ë²¡í„° ë°ì´í„° íšë“
                        first_vector_field = vector_fields[0]
                        vector_dim = next(field.params.get("dim", 384) 
                                        for field in collection.schema.fields 
                                        if field.name == first_vector_field)
                        
                        dummy_vector = [0.0] * vector_dim
                        search_results = collection.search(
                            data=[dummy_vector],
                            anns_field=first_vector_field,
                            param={"metric_type": "L2", "params": {"nprobe": 1}},
                            limit=limit,
                            offset=offset,
                            output_fields=output_fields
                        )
                        
                        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë²¡í„° ì¶”ì¶œ
                        if search_results and len(search_results[0]) > 0:
                            for i, hit in enumerate(search_results[0]):
                                if i < len(query_results):
                                    for vector_field in vector_fields:
                                        query_results[i][vector_field] = hit.entity.get(vector_field)
                    
                    data_batches.append(query_results)
                    total_entities += len(query_results)
                    batch_count += 1
                    
                    if len(query_results) < limit:
                        break
                    
                    offset += limit
                else:
                    break
            
            # ë°ì´í„° ì••ì¶• ì €ì¥
            data_path = backup_path / "data.pkl.gz"
            with gzip.open(data_path, 'wb') as f:
                pickle.dump(data_batches, f)
            
            # ì²´í¬ì„¬ ê³„ì‚°
            checksum = self._calculate_checksum(data_path)
            
            data_info = {
                "total_entities": total_entities,
                "batch_count": batch_count,
                "batch_size": limit,
                "checksum": checksum,
                "compressed_size_mb": data_path.stat().st_size / (1024 * 1024)
            }
            
            print(f"    âœ… ë°ì´í„° ë°±ì—… ì™„ë£Œ ({total_entities:,}ê°œ ì—”í‹°í‹°, {batch_count}ê°œ ë°°ì¹˜)")
            
            return data_info
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë°±ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            # ê°„ë‹¨í•œ ëŒ€ì•ˆ: ë©”íƒ€ë°ì´í„°ë§Œ ë°±ì—…
            return {
                "total_entities": collection.num_entities,
                "backup_method": "metadata_only",
                "note": f"ë°ì´í„° ë°±ì—… ì‹¤íŒ¨: {str(e)}"
            }
        
        finally:
            collection.release()
    
    def _calculate_backup_size(self, backup_path: Path) -> float:
        """ë°±ì—… í¬ê¸° ê³„ì‚°"""
        total_size = 0
        for file_path in backup_path.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size / (1024 * 1024)  # MB ë‹¨ìœ„
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def restore_from_backup(self, backup_name: str, target_collection_name: Optional[str] = None) -> Collection:
        """ë°±ì—…ì—ì„œ ë³µì›"""
        print(f"â™»ï¸ ë°±ì—…ì—ì„œ ë³µì›: '{backup_name}'...")
        
        backup_path = self.backup_root / backup_name
        
        if not backup_path.exists():
            raise FileNotFoundError(f"ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_path}")
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ
        manifest_path = backup_path / "manifest.json"
        if not manifest_path.exists():
            raise FileNotFoundError(f"ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {manifest_path}")
        
        with open(manifest_path, 'r') as f:
            manifest = json.load(f)
        
        start_time = time.time()
        
        try:
            # 1. ì»¬ë ‰ì…˜ ì¬ìƒì„±
            collection_name = target_collection_name or f"{manifest['collection_name']}_restored"
            collection = self._restore_collection_schema(manifest, collection_name, backup_path)
            
            # 2. ì¸ë±ìŠ¤ ë³µì›
            self._restore_indexes(collection, backup_path)
            
            # 3. ë°ì´í„° ë³µì›
            self._restore_collection_data(collection, backup_path, manifest)
            
            restore_duration = time.time() - start_time
            
            print(f"  âœ… ë³µì› ì™„ë£Œ: {restore_duration:.2f}ì´ˆ")
            print(f"  ğŸ“Š ë³µì›ëœ ì»¬ë ‰ì…˜: {collection_name}")
            
            return collection
            
        except Exception as e:
            logger.error(f"ë³µì› ì‹¤íŒ¨: {e}")
            raise
    
    def _restore_collection_schema(self, manifest: Dict, collection_name: str, backup_path: Path) -> Collection:
        """ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆ ë³µì›"""
        print("  ğŸ—ï¸ ìŠ¤í‚¤ë§ˆ ë³µì› ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = backup_path / "metadata.json"
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        # ìŠ¤í‚¤ë§ˆ ì¬êµ¬ì„±
        fields = []
        for field_info in metadata["schema"]["fields"]:
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            dtype_mapping = {
                "DataType.INT64": DataType.INT64,
                "DataType.INT32": DataType.INT32,
                "DataType.FLOAT": DataType.FLOAT,
                "DataType.DOUBLE": DataType.DOUBLE,
                "DataType.VARCHAR": DataType.VARCHAR,
                "DataType.BOOL": DataType.BOOL,
                "DataType.FLOAT_VECTOR": DataType.FLOAT_VECTOR,
                "DataType.BINARY_VECTOR": DataType.BINARY_VECTOR
            }
            
            dtype = dtype_mapping.get(field_info["dtype"], DataType.VARCHAR)
            
            # í•„ë“œ íŒŒë¼ë¯¸í„°
            field_params = {}
            if "dim" in field_info:
                field_params["dim"] = field_info["dim"]
            if "max_length" in field_info:
                field_params["max_length"] = field_info["max_length"]
            
            field = FieldSchema(
                name=field_info["name"],
                dtype=dtype,
                is_primary=field_info.get("is_primary", False),
                auto_id=field_info.get("auto_id", False),
                description=field_info.get("description", ""),
                **field_params
            )
            fields.append(field)
        
        # ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆ ìƒì„±
        schema = CollectionSchema(
            fields=fields,
            description=metadata["schema"]["description"]
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        collection = Collection(name=collection_name, schema=schema)
        
        # íŒŒí‹°ì…˜ ë³µì›
        for partition_info in metadata.get("partitions", []):
            if partition_info["name"] != "_default":  # ê¸°ë³¸ íŒŒí‹°ì…˜ ì œì™¸
                collection.create_partition(partition_info["name"])
        
        print(f"    âœ… ìŠ¤í‚¤ë§ˆ ë³µì› ì™„ë£Œ ({len(fields)}ê°œ í•„ë“œ)")
        return collection
    
    def _restore_indexes(self, collection: Collection, backup_path: Path):
        """ì¸ë±ìŠ¤ ë³µì›"""
        print("  ğŸ” ì¸ë±ìŠ¤ ë³µì› ì¤‘...")
        
        index_path = backup_path / "indexes.json"
        if not index_path.exists():
            print("    âš ï¸ ì¸ë±ìŠ¤ ì •ë³´ ì—†ìŒ")
            return
        
        with open(index_path, 'r') as f:
            index_info = json.load(f)
        
        for field_name, index_config in index_info.items():
            try:
                collection.create_index(
                    field_name=field_name,
                    index_params=index_config
                )
                print(f"    âœ… ì¸ë±ìŠ¤ ë³µì›: {field_name} ({index_config['index_type']})")
            except Exception as e:
                logger.warning(f"ì¸ë±ìŠ¤ ë³µì› ì‹¤íŒ¨ {field_name}: {e}")
    
    def _restore_collection_data(self, collection: Collection, backup_path: Path, manifest: Dict):
        """ì»¬ë ‰ì…˜ ë°ì´í„° ë³µì›"""
        print("  ğŸ“Š ë°ì´í„° ë³µì› ì¤‘...")
        
        data_path = backup_path / "data.pkl.gz"
        
        if not data_path.exists():
            print("    âš ï¸ ë°ì´í„° íŒŒì¼ ì—†ìŒ")
            return
        
        # ì²´í¬ì„¬ ê²€ì¦
        if "data_info" in manifest and "checksum" in manifest["data_info"]:
            current_checksum = self._calculate_checksum(data_path)
            expected_checksum = manifest["data_info"]["checksum"]
            
            if current_checksum != expected_checksum:
                raise ValueError(f"ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {current_checksum} != {expected_checksum}")
        
        # ë°ì´í„° ë¡œë“œ
        try:
            with gzip.open(data_path, 'rb') as f:
                data_batches = pickle.load(f)
            
            total_inserted = 0
            
            for batch_idx, batch_data in enumerate(data_batches):
                if batch_data:
                    # ë°ì´í„° êµ¬ì¡° ë³€í™˜ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ â†’ í•„ë“œë³„ ë¦¬ìŠ¤íŠ¸)
                    field_data = {}
                    for record in batch_data:
                        for field_name, value in record.items():
                            if field_name not in field_data:
                                field_data[field_name] = []
                            field_data[field_name].append(value)
                    
                    # ìŠ¤í‚¤ë§ˆ ìˆœì„œì— ë§ê²Œ ë°ì´í„° ì •ë ¬
                    schema_fields = [field.name for field in collection.schema.fields 
                                   if not field.is_primary or not getattr(field, 'auto_id', False)]
                    
                    ordered_data = []
                    for field_name in schema_fields:
                        if field_name in field_data:
                            ordered_data.append(field_data[field_name])
                    
                    if ordered_data:
                        collection.insert(ordered_data)
                        total_inserted += len(batch_data)
                        
                        if (batch_idx + 1) % 5 == 0:
                            print(f"    ì§„í–‰ë¥ : {batch_idx + 1}/{len(data_batches)} ë°°ì¹˜ ì²˜ë¦¬ë¨")
            
            collection.flush()
            print(f"    âœ… ë°ì´í„° ë³µì› ì™„ë£Œ ({total_inserted:,}ê°œ ì—”í‹°í‹°)")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë³µì› ì¤‘ ì˜¤ë¥˜: {e}")
            print("    âš ï¸ ë°ì´í„° ë³µì› ì‹¤íŒ¨ - ìŠ¤í‚¤ë§ˆë§Œ ë³µì›ë¨")

class DisasterRecoverySimulator:
    """ì¬í•´ ë³µêµ¬ ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self, backup_manager: BackupManager):
        self.backup_manager = backup_manager
        self.vector_utils = VectorUtils()
        
    def simulate_data_corruption(self, collection: Collection) -> Dict[str, Any]:
        """ë°ì´í„° ì†ìƒ ì‹œë®¬ë ˆì´ì…˜"""
        print("ğŸ’¥ ë°ì´í„° ì†ìƒ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜...")
        
        original_count = collection.num_entities
        
        # ì‹œë®¬ë ˆì´ì…˜: ì¼ë¶€ ë°ì´í„° ì†ì‹¤
        simulated_corruption = {
            "scenario": "partial_data_loss",
            "original_entities": original_count,
            "corruption_percentage": 15.5,
            "affected_partitions": ["region_us", "category_tech"],
            "corruption_time": datetime.now().isoformat(),
            "symptoms": [
                "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ê°ì†Œ",
                "íŠ¹ì • íŒŒí‹°ì…˜ ì‘ë‹µ ì—†ìŒ",
                "ì¸ë±ìŠ¤ ë¬´ê²°ì„± ì˜¤ë¥˜"
            ]
        }
        
        print(f"  ğŸ’€ ì‹œë‚˜ë¦¬ì˜¤: {simulated_corruption['scenario']}")
        print(f"  ğŸ“Š ì˜í–¥ ë²”ìœ„: {simulated_corruption['corruption_percentage']}% ì†ì‹¤")
        print(f"  ğŸ¯ ì˜í–¥ íŒŒí‹°ì…˜: {simulated_corruption['affected_partitions']}")
        
        return simulated_corruption
    
    def simulate_system_failure(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜"""
        print("ğŸ”¥ ì‹œìŠ¤í…œ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜...")
        
        failure_scenario = {
            "scenario": "complete_system_failure",
            "failure_type": "hardware_failure",
            "failure_time": datetime.now().isoformat(),
            "affected_components": [
                "Milvus ì„œë²„",
                "ì¸ë±ìŠ¤ ì €ì¥ì†Œ",
                "ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ"
            ],
            "recovery_requirements": [
                "ì „ì²´ ì‹œìŠ¤í…œ ì¬êµ¬ì¶•",
                "ë°±ì—…ì—ì„œ ë°ì´í„° ë³µì›",
                "ì¸ë±ìŠ¤ ì¬ìƒì„±",
                "ì„œë¹„ìŠ¤ ì¬ì‹œì‘"
            ]
        }
        
        print(f"  ğŸ”¥ ì¥ì•  ìœ í˜•: {failure_scenario['failure_type']}")
        print(f"  ğŸ’» ì˜í–¥ ì»´í¬ë„ŒíŠ¸: {len(failure_scenario['affected_components'])}ê°œ")
        print(f"  ğŸ› ï¸ ë³µêµ¬ ë‹¨ê³„: {len(failure_scenario['recovery_requirements'])}ë‹¨ê³„")
        
        return failure_scenario
    
    def test_recovery_procedures(self, original_collection: Collection, 
                               backup_name: str) -> Dict[str, Any]:
        """ë³µêµ¬ ì ˆì°¨ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”§ ë³µêµ¬ ì ˆì°¨ í…ŒìŠ¤íŠ¸...")
        
        recovery_results = {
            "test_start_time": datetime.now().isoformat(),
            "steps": [],
            "success": False,
            "total_time": 0
        }
        
        start_time = time.time()
        
        try:
            # 1. ë°±ì—… ê²€ì¦
            step1_start = time.time()
            print("  1ï¸âƒ£ ë°±ì—… ë¬´ê²°ì„± ê²€ì¦...")
            
            backup_path = self.backup_manager.backup_root / backup_name
            if not backup_path.exists():
                raise FileNotFoundError(f"ë°±ì—… ì—†ìŒ: {backup_name}")
            
            recovery_results["steps"].append({
                "step": "backup_verification",
                "duration": time.time() - step1_start,
                "status": "success"
            })
            print("    âœ… ë°±ì—… ê²€ì¦ ì™„ë£Œ")
            
            # 2. ì‹œìŠ¤í…œ ì¤€ë¹„
            step2_start = time.time()
            print("  2ï¸âƒ£ ë³µêµ¬ í™˜ê²½ ì¤€ë¹„...")
            
            # ìƒˆ ì»¬ë ‰ì…˜ëª… ìƒì„±
            recovery_collection_name = f"{original_collection.name}_recovery_{int(time.time())}"
            
            recovery_results["steps"].append({
                "step": "system_preparation",
                "duration": time.time() - step2_start,
                "status": "success"
            })
            print("    âœ… í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ")
            
            # 3. ë°ì´í„° ë³µì›
            step3_start = time.time()
            print("  3ï¸âƒ£ ë°ì´í„° ë³µì› ì‹¤í–‰...")
            
            restored_collection = self.backup_manager.restore_from_backup(
                backup_name, recovery_collection_name
            )
            
            recovery_results["steps"].append({
                "step": "data_restoration",
                "duration": time.time() - step3_start,
                "status": "success"
            })
            print("    âœ… ë°ì´í„° ë³µì› ì™„ë£Œ")
            
            # 4. ë¬´ê²°ì„± ê²€ì¦
            step4_start = time.time()
            print("  4ï¸âƒ£ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦...")
            
            integrity_results = self._verify_data_integrity(
                original_collection, restored_collection
            )
            
            recovery_results["steps"].append({
                "step": "integrity_verification",
                "duration": time.time() - step4_start,
                "status": "success" if integrity_results["passed"] else "failed",
                "details": integrity_results
            })
            
            if integrity_results["passed"]:
                print("    âœ… ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
            else:
                print("    âš ï¸ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨")
            
            # 5. ì„œë¹„ìŠ¤ ê²€ì¦
            step5_start = time.time()
            print("  5ï¸âƒ£ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê²€ì¦...")
            
            service_results = self._verify_service_functionality(restored_collection)
            
            recovery_results["steps"].append({
                "step": "service_verification",
                "duration": time.time() - step5_start,
                "status": "success" if service_results["passed"] else "failed",
                "details": service_results
            })
            
            if service_results["passed"]:
                print("    âœ… ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê²€ì¦ í†µê³¼")
                recovery_results["success"] = True
            else:
                print("    âš ï¸ ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê²€ì¦ ì‹¤íŒ¨")
            
            # ì •ë¦¬
            utility.drop_collection(recovery_collection_name)
            
        except Exception as e:
            logger.error(f"ë³µêµ¬ ì ˆì°¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            recovery_results["error"] = str(e)
        
        recovery_results["total_time"] = time.time() - start_time
        recovery_results["test_end_time"] = datetime.now().isoformat()
        
        return recovery_results
    
    def _verify_data_integrity(self, original: Collection, restored: Collection) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        try:
            original_count = original.num_entities
            restored_count = restored.num_entities
            
            # ê¸°ë³¸ ê²€ì¦
            count_match = original_count == restored_count
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦
            schema_match = len(original.schema.fields) == len(restored.schema.fields)
            
            # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            search_test_passed = True
            try:
                restored.load()
                
                # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
                test_vector = [0.1] * 384  # ê¸°ë³¸ ì°¨ì›
                results = restored.search(
                    data=[test_vector],
                    anns_field="vector",
                    param={"metric_type": "L2", "params": {"nprobe": 1}},
                    limit=5
                )
                
                search_test_passed = len(results) > 0
                restored.release()
                
            except Exception as e:
                search_test_passed = False
                logger.warning(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            passed = count_match and schema_match and search_test_passed
            
            return {
                "passed": passed,
                "original_count": original_count,
                "restored_count": restored_count,
                "count_match": count_match,
                "schema_match": schema_match,
                "search_test_passed": search_test_passed
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e)
            }
    
    def _verify_service_functionality(self, collection: Collection) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ê¸°ëŠ¥ ê²€ì¦"""
        try:
            collection.load()
            
            tests = []
            
            # 1. ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            try:
                test_vector = [0.1] * 384
                results = collection.search(
                    data=[test_vector],
                    anns_field="vector",
                    param={"metric_type": "L2", "params": {"nprobe": 1}},
                    limit=10
                )
                tests.append({"name": "basic_search", "passed": len(results) > 0})
            except Exception as e:
                tests.append({"name": "basic_search", "passed": False, "error": str(e)})
            
            # 2. í•„í„°ë§ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            try:
                results = collection.search(
                    data=[test_vector],
                    anns_field="vector",
                    param={"metric_type": "L2", "params": {"nprobe": 1}},
                    limit=5,
                    expr="priority >= 1",
                    output_fields=["content", "source"]
                )
                tests.append({"name": "filtered_search", "passed": True})
            except Exception as e:
                tests.append({"name": "filtered_search", "passed": False, "error": str(e)})
            
            # 3. ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            try:
                query_results = collection.query(
                    expr="priority >= 1",
                    limit=10,
                    output_fields=["content", "source"]
                )
                tests.append({"name": "query", "passed": len(query_results) >= 0})
            except Exception as e:
                tests.append({"name": "query", "passed": False, "error": str(e)})
            
            collection.release()
            
            passed_count = sum(1 for test in tests if test["passed"])
            total_tests = len(tests)
            
            return {
                "passed": passed_count == total_tests,
                "passed_tests": passed_count,
                "total_tests": total_tests,
                "test_results": tests
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e)
            }

class BackupRecoveryManager:
    """ë°±ì—… ë° ë³µêµ¬ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.milvus_conn = MilvusConnection()
        self.vector_utils = VectorUtils()
        self.backup_manager = BackupManager()
        self.disaster_simulator = DisasterRecoverySimulator(self.backup_manager)
        
    def create_test_collection(self, collection_name: str, data_size: int = 1000) -> Collection:
        """í…ŒìŠ¤íŠ¸ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"ğŸ—ï¸ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="priority", dtype=DataType.INT32),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="score", dtype=DataType.FLOAT),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Test collection for backup and recovery"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        
        # íŒŒí‹°ì…˜ ìƒì„±
        partitions = ["region_us", "region_eu", "category_tech", "category_business"]
        for partition_name in partitions:
            collection.create_partition(partition_name)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° {data_size:,}ê°œ ìƒì„± ì¤‘...")
        
        sources = ["web", "mobile", "api", "batch"]
        priorities = [1, 2, 3, 4, 5]
        
        contents = []
        source_list = []
        priority_list = []
        timestamp_list = []
        score_list = []
        
        for i in range(data_size):
            contents.append(f"Test document {i} for backup and recovery testing with various content")
            source_list.append(np.random.choice(sources))
            priority_list.append(np.random.choice(priorities))
            timestamp_list.append(int(time.time()) + i)
            score_list.append(np.random.uniform(1.0, 10.0))
        
        # ë²¡í„° ìƒì„±
        vectors = self.vector_utils.texts_to_vectors(contents)
        
        # ë°ì´í„° ì‚½ì…
        data = [
            contents,
            source_list,
            priority_list,
            timestamp_list,
            score_list,
            vectors.tolist()
        ]
        
        collection.insert(data)
        collection.flush()
        
        # ì¸ë±ìŠ¤ ìƒì„±
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        collection.create_index("vector", index_params)
        
        print(f"  âœ… í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ ({data_size:,}ê°œ ì—”í‹°í‹°)")
        return collection
    
    def run_backup_recovery_demo(self):
        """ë°±ì—… ë° ë³µêµ¬ ì¢…í•© ë°ëª¨"""
        print("ğŸ’¾ Milvus ë°±ì—… ë° ë³µêµ¬ ì‹¤ìŠµ")
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Milvus ì—°ê²°
            self.milvus_conn.connect()
            print("âœ… Milvus ì—°ê²° ì„±ê³µ\n")
            
            print("=" * 80)
            print(" ğŸ—ï¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•")
            print("=" * 80)
            
            # í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ìƒì„±
            test_collection = self.create_test_collection("backup_test_collection", 2000)
            
            print("\n" + "=" * 80)
            print(" ğŸ’¾ ë°±ì—… ì‹œìŠ¤í…œ êµ¬ì¶•")
            print("=" * 80)
            
            # ì „ì²´ ë°±ì—… ìƒì„±
            backup_name = f"full_backup_{int(time.time())}"
            backup_manifest = self.backup_manager.create_full_backup(test_collection, backup_name)
            
            print(f"\nğŸ“‹ ë°±ì—… ìš”ì•½:")
            print(f"  ë°±ì—…ëª…: {backup_manifest['backup_name']}")
            print(f"  ë°±ì—… íƒ€ì…: {backup_manifest['backup_type']}")
            print(f"  ë°±ì—… í¬ê¸°: {backup_manifest['backup_size_mb']:.1f}MB")
            print(f"  ë°±ì—… ì‹œê°„: {backup_manifest['backup_duration']:.2f}ì´ˆ")
            
            print("\n" + "=" * 80)
            print(" ğŸ’¥ ì¬í•´ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜")
            print("=" * 80)
            
            # ë°ì´í„° ì†ìƒ ì‹œë®¬ë ˆì´ì…˜
            corruption_scenario = self.disaster_simulator.simulate_data_corruption(test_collection)
            
            # ì‹œìŠ¤í…œ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
            failure_scenario = self.disaster_simulator.simulate_system_failure()
            
            print("\n" + "=" * 80)
            print(" ğŸ”§ ë³µêµ¬ ì ˆì°¨ ì‹¤í–‰")
            print("=" * 80)
            
            # ë³µêµ¬ ì ˆì°¨ í…ŒìŠ¤íŠ¸
            recovery_results = self.disaster_simulator.test_recovery_procedures(
                test_collection, backup_name
            )
            
            print(f"\nğŸ“Š ë³µêµ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"  ì „ì²´ ì„±ê³µ: {'âœ…' if recovery_results['success'] else 'âŒ'}")
            print(f"  ì´ ì†Œìš” ì‹œê°„: {recovery_results['total_time']:.2f}ì´ˆ")
            
            for step in recovery_results["steps"]:
                status_icon = "âœ…" if step["status"] == "success" else "âŒ"
                print(f"  {step['step']}: {status_icon} ({step['duration']:.2f}ì´ˆ)")
            
            print("\n" + "=" * 80)
            print(" ğŸ“‹ ë°±ì—… ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§")
            print("=" * 80)
            
            # ë°±ì—… ëª©ë¡ ë° ìƒíƒœ
            print("ğŸ“‚ ë°±ì—… ëª©ë¡:")
            backup_list = list(self.backup_manager.backup_root.iterdir())
            for backup_path in backup_list:
                if backup_path.is_dir():
                    manifest_path = backup_path / "manifest.json"
                    if manifest_path.exists():
                        with open(manifest_path, 'r') as f:
                            manifest = json.load(f)
                        
                        print(f"  ğŸ“¦ {manifest['backup_name']}")
                        print(f"    ìƒì„±ì¼: {manifest['created_at'][:19]}")
                        print(f"    í¬ê¸°: {manifest['backup_size_mb']:.1f}MB")
                        print(f"    ì»¬ë ‰ì…˜: {manifest['collection_name']}")
            
            # ë°±ì—… ì •ì±… ê¶Œì¥ì‚¬í•­
            print(f"\nğŸ’¡ ë°±ì—… ì •ì±… ê¶Œì¥ì‚¬í•­:")
            print(f"  ğŸ“… ìŠ¤ì¼€ì¤„ë§:")
            print(f"    â€¢ ì¼ì¼ ì¦ë¶„ ë°±ì—…: ë³€ê²½ëœ ë°ì´í„°ë§Œ")
            print(f"    â€¢ ì£¼ê°„ ì „ì²´ ë°±ì—…: ì™„ì „í•œ ë³µì›ì ")
            print(f"    â€¢ ì›”ê°„ ì•„ì¹´ì´ë¸Œ: ì¥ê¸° ë³´ê´€")
            
            print(f"\n  ğŸ—„ï¸ ì €ì¥ ì „ëµ:")
            print(f"    â€¢ ë¡œì»¬ ë°±ì—…: ë¹ ë¥¸ ë³µêµ¬")
            print(f"    â€¢ ì›ê²© ë°±ì—…: ì¬í•´ ë³µêµ¬")
            print(f"    â€¢ í´ë¼ìš°ë“œ ì €ì¥ì†Œ: í™•ì¥ì„± ë° ë‚´êµ¬ì„±")
            
            print(f"\n  ğŸ” ëª¨ë‹ˆí„°ë§:")
            print(f"    â€¢ ë°±ì—… ì„±ê³µ/ì‹¤íŒ¨ ì•Œë¦¼")
            print(f"    â€¢ ë°±ì—… í¬ê¸° ì¶”ì´ ëª¨ë‹ˆí„°ë§")
            print(f"    â€¢ ë³µêµ¬ ì‹œê°„ ëª©í‘œ(RTO) ì¸¡ì •")
            print(f"    â€¢ ë³µêµ¬ ì§€ì  ëª©í‘œ(RPO) ê´€ë¦¬")
            
            print("\n" + "=" * 80)
            print(" ğŸ›¡ï¸ ê³ ê°€ìš©ì„± ë° ì¬í•´ ë³µêµ¬ ì „ëµ")
            print("=" * 80)
            
            print("ğŸ—ï¸ ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜:")
            print("  ğŸ“Š ë°ì´í„° ë³µì œ:")
            print("    â€¢ ë™ê¸° ë³µì œ: ì¼ê´€ì„± ë³´ì¥")
            print("    â€¢ ë¹„ë™ê¸° ë³µì œ: ì„±ëŠ¥ ìµœì í™”")
            print("    â€¢ ì§€ë¦¬ì  ë¶„ì‚°: ì¬í•´ ëŒ€ì‘")
            
            print("\n  âš–ï¸ ë¡œë“œ ë°¸ëŸ°ì‹±:")
            print("    â€¢ ì•¡í‹°ë¸Œ-ì•¡í‹°ë¸Œ: ìµœëŒ€ ê°€ìš©ì„±")
            print("    â€¢ ì•¡í‹°ë¸Œ-ìŠ¤íƒ ë°”ì´: ë¹ ë¥¸ ì¥ì•  ë³µêµ¬")
            print("    â€¢ ìë™ í˜ì¼ì˜¤ë²„: ë¬´ì¤‘ë‹¨ ì„œë¹„ìŠ¤")
            
            print("\n  ğŸ”„ ë°±ì—… ìë™í™”:")
            print("    â€¢ ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë°˜: cron, ìŠ¤ì¼€ì¤„ëŸ¬")
            print("    â€¢ ì´ë²¤íŠ¸ ê¸°ë°˜: ë°ì´í„° ë³€ê²½ ê°ì§€")
            print("    â€¢ í´ë¼ìš°ë“œ ë°±ì—…: AWS S3, Azure Blob")
            
            print("\n  ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼:")
            print("    â€¢ ë°±ì—… ìƒíƒœ ëŒ€ì‹œë³´ë“œ")
            print("    â€¢ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì•Œë¦¼")
            print("    â€¢ ë³µêµ¬ ì ˆì°¨ ë¬¸ì„œí™”")
            print("    â€¢ ì •ê¸°ì  ë³µêµ¬ í…ŒìŠ¤íŠ¸")
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("backup_test_collection")
            
            # ë°±ì—… íŒŒì¼ ì •ë¦¬ (ì˜µì…˜)
            print("ğŸ’¾ ë°±ì—… íŒŒì¼ ë³´ê´€ (ì •ë¦¬í•˜ì§€ ì•ŠìŒ)")
            
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            self.milvus_conn.disconnect()
            
        print(f"\nğŸ‰ ë°±ì—… ë° ë³µêµ¬ ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ì „ì²´ ë°±ì—… ë° ì¦ë¶„ ë°±ì—… ì „ëµ")
        print("  â€¢ ì¬í•´ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘ ë° ë³µêµ¬ ì ˆì°¨")
        print("  â€¢ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ë° ì„œë¹„ìŠ¤ ê¸°ëŠ¥ í™•ì¸")
        print("  â€¢ ë°±ì—… ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  python step04_advanced/06_monitoring_metrics.py")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    backup_recovery_manager = BackupRecoveryManager()
    backup_recovery_manager.run_backup_recovery_demo()

if __name__ == "__main__":
    main() 