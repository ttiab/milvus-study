#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸš€ Milvus ì„±ëŠ¥ ìµœì í™” ì‹¤ìŠµ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvusì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ë“¤ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
- ì¿¼ë¦¬ ìµœì í™” (Query Optimization)
- ìºì‹± ì „ëµ (Caching Strategy)  
- ë°°ì¹˜ ì²˜ë¦¬ (Batch Processing)
- ì—°ê²° í’€ë§ (Connection Pooling)
- ë©”ëª¨ë¦¬ ìµœì í™” (Memory Optimization)
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ (Performance Benchmarking)
"""

import os
import sys
import time
import logging
import threading
import numpy as np
from datetime import datetime
from collections import defaultdict, deque
import json
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple
import gc

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QueryCache:
    """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 300):
        self.max_size = max_size
        self.ttl = ttl  # Time to live in seconds
        self.cache = {}
        self.access_times = {}
        self.creation_times = {}
        self._lock = threading.RLock()
    
    def _generate_key(self, query_vector: np.ndarray, params: dict) -> str:
        """ì¿¼ë¦¬ í‚¤ ìƒì„±"""
        vector_str = np.array_str(query_vector)
        params_str = json.dumps(params, sort_keys=True)
        return hashlib.md5(f"{vector_str}_{params_str}".encode()).hexdigest()
    
    def _is_expired(self, key: str) -> bool:
        """ìºì‹œ ë§Œë£Œ í™•ì¸"""
        if key not in self.creation_times:
            return True
        return (time.time() - self.creation_times[key]) > self.ttl
    
    def _evict_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì œê±°"""
        current_time = time.time()
        expired_keys = [
            key for key, creation_time in self.creation_times.items()
            if (current_time - creation_time) > self.ttl
        ]
        for key in expired_keys:
            self._remove_key(key)
    
    def _evict_lru(self):
        """LRU ì •ì±…ìœ¼ë¡œ ìºì‹œ ì œê±°"""
        if len(self.cache) >= self.max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ ì ‘ê·¼ ì‹œê°„ì˜ í‚¤ ì°¾ê¸°
            lru_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            self._remove_key(lru_key)
    
    def _remove_key(self, key: str):
        """í‚¤ ì œê±°"""
        self.cache.pop(key, None)
        self.access_times.pop(key, None)
        self.creation_times.pop(key, None)
    
    def get(self, query_vector: np.ndarray, params: dict):
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        with self._lock:
            key = self._generate_key(query_vector, params)
            
            if key in self.cache and not self._is_expired(key):
                self.access_times[key] = time.time()
                return self.cache[key]
            
            return None
    
    def put(self, query_vector: np.ndarray, params: dict, result):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        with self._lock:
            self._evict_expired()
            self._evict_lru()
            
            key = self._generate_key(query_vector, params)
            current_time = time.time()
            
            self.cache[key] = result
            self.access_times[key] = current_time
            self.creation_times[key] = current_time
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        with self._lock:
            self.cache.clear()
            self.access_times.clear()
            self.creation_times.clear()
    
    def stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        with self._lock:
            return {
                "size": len(self.cache),
                "max_size": self.max_size,
                "ttl": self.ttl,
                "hit_ratio": getattr(self, '_hit_count', 0) / max(getattr(self, '_total_count', 1), 1)
            }

class ConnectionPool:
    """Milvus ì—°ê²° í’€"""
    
    def __init__(self, host: str = "localhost", port: str = "19530", 
                 pool_size: int = 10, alias_prefix: str = "pool"):
        self.host = host
        self.port = port
        self.pool_size = pool_size
        self.alias_prefix = alias_prefix
        self.available_connections = deque()
        self.in_use_connections = set()
        self._lock = threading.RLock()
        self._initialize_pool()
    
    def _initialize_pool(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        for i in range(self.pool_size):
            alias = f"{self.alias_prefix}_{i}"
            try:
                connections.connect(
                    alias=alias,
                    host=self.host,
                    port=self.port
                )
                self.available_connections.append(alias)
                logger.info(f"ì—°ê²° í’€ì— ì—°ê²° ì¶”ê°€: {alias}")
            except Exception as e:
                logger.error(f"ì—°ê²° ìƒì„± ì‹¤íŒ¨ {alias}: {e}")
    
    def get_connection(self, timeout: float = 30.0):
        """ì—°ê²° íšë“"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            with self._lock:
                if self.available_connections:
                    alias = self.available_connections.popleft()
                    self.in_use_connections.add(alias)
                    return alias
            
            time.sleep(0.1)  # ì ì‹œ ëŒ€ê¸°
        
        raise TimeoutError("ì—°ê²° í’€ì—ì„œ ì—°ê²°ì„ íšë“í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    def return_connection(self, alias: str):
        """ì—°ê²° ë°˜í™˜"""
        with self._lock:
            if alias in self.in_use_connections:
                self.in_use_connections.remove(alias)
                self.available_connections.append(alias)
    
    def close_all(self):
        """ëª¨ë“  ì—°ê²° ì¢…ë£Œ"""
        with self._lock:
            all_aliases = list(self.available_connections) + list(self.in_use_connections)
            for alias in all_aliases:
                try:
                    connections.remove_connection(alias)
                except:
                    pass
            self.available_connections.clear()
            self.in_use_connections.clear()
    
    def stats(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ í†µê³„"""
        with self._lock:
            return {
                "pool_size": self.pool_size,
                "available": len(self.available_connections),
                "in_use": len(self.in_use_connections),
                "utilization": len(self.in_use_connections) / self.pool_size
            }

class PerformanceOptimizer:
    """Milvus ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.milvus_conn = MilvusConnection()
        self.vector_utils = VectorUtils()
        self.query_cache = QueryCache(max_size=500, ttl=300)
        self.connection_pool = None
        self.performance_stats = defaultdict(list)
        
    def setup_connection_pool(self, pool_size: int = 5):
        """ì—°ê²° í’€ ì„¤ì •"""
        print("ğŸ”— ì—°ê²° í’€ ì„¤ì • ì¤‘...")
        self.connection_pool = ConnectionPool(pool_size=pool_size)
        print(f"  âœ… {pool_size}ê°œ ì—°ê²° í’€ ìƒì„± ì™„ë£Œ")
        
    def create_optimized_collection(self, collection_name: str, dim: int = 384) -> Collection:
        """ìµœì í™”ëœ ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"ğŸ“ ìµœì í™”ëœ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        
        # ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ ì •ì˜
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="score", dtype=DataType.FLOAT),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dim)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Performance optimized collection"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        
        return collection
    
    def batch_insert_optimized(self, collection: Collection, data_size: int = 10000, 
                             batch_size: int = 1000) -> float:
        """ìµœì í™”ëœ ë°°ì¹˜ ì‚½ì…"""
        print(f"ğŸ’¾ ìµœì í™”ëœ ë°°ì¹˜ ì‚½ì… ({data_size:,}ê°œ ë°ì´í„°, ë°°ì¹˜í¬ê¸°: {batch_size:,})...")
        
        start_time = time.time()
        
        # ë°°ì¹˜ë³„ë¡œ ë°ì´í„° ìƒì„± ë° ì‚½ì…
        for i in range(0, data_size, batch_size):
            batch_end = min(i + batch_size, data_size)
            actual_batch_size = batch_end - i
            
            # ë°°ì¹˜ ë°ì´í„° ìƒì„±
            texts = [f"Optimized document {j} for performance testing" for j in range(i, batch_end)]
            categories = np.random.choice(['tech', 'science', 'business', 'health'], actual_batch_size)
            scores = np.random.uniform(1.0, 5.0, actual_batch_size)
            timestamps = [int(time.time()) + j for j in range(actual_batch_size)]
            
            # ë²¡í„° ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
            vectors = self.vector_utils.texts_to_vectors(texts)
            
            # ë°ì´í„° êµ¬ì¡°í™” (List[List] í˜•ì‹)
            batch_data = [
                texts,
                categories.tolist(),
                scores.tolist(),
                timestamps,
                vectors.tolist()
            ]
            
            # ë°°ì¹˜ ì‚½ì…
            collection.insert(batch_data)
            
            if (i // batch_size + 1) % 5 == 0:
                print(f"  ì§„í–‰ë¥ : {i + actual_batch_size:,}/{data_size:,} ({(i + actual_batch_size)/data_size*100:.1f}%)")
        
        # ë°ì´í„° í”ŒëŸ¬ì‹œ
        collection.flush()
        
        insert_time = time.time() - start_time
        print(f"  âœ… ì‚½ì… ì™„ë£Œ: {insert_time:.2f}ì´ˆ")
        print(f"  ğŸ“Š ì²˜ë¦¬ëŸ‰: {data_size/insert_time:.0f} docs/sec")
        
        return insert_time
    
    def create_optimized_index(self, collection: Collection) -> float:
        """ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ” ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        start_time = time.time()
        
        # HNSW ì¸ë±ìŠ¤ (ê°€ì¥ ë¹ ë¥¸ ê²€ìƒ‰ ì„±ëŠ¥)
        index_params = {
            "metric_type": "COSINE",
            "index_type": "HNSW",
            "params": {
                "M": 16,        # ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•ë„ ì¦ê°€, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€)
                "efConstruction": 200  # êµ¬ì¶• ì‹œ íƒìƒ‰ ê¹Šì´
            }
        }
        
        collection.create_index(
            field_name="vector",
            index_params=index_params
        )
        
        index_time = time.time() - start_time
        print(f"  âœ… HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {index_time:.2f}ì´ˆ")
        
        return index_time
    
    def optimized_search(self, collection: Collection, query_text: str, 
                        use_cache: bool = True) -> Tuple[Any, float, bool]:
        """ìµœì í™”ëœ ê²€ìƒ‰ (ìºì‹± í¬í•¨)"""
        # ì¿¼ë¦¬ ë²¡í„° ìƒì„±
        query_vectors = self.vector_utils.text_to_vector(query_text)
        query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        search_params = {
            "metric_type": "COSINE",
            "params": {
                "ef": 100  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´ (ë†’ì„ìˆ˜ë¡ ì •í™•ë„ ì¦ê°€)
            }
        }
        
        # ìºì‹œ í™•ì¸
        cache_hit = False
        if use_cache:
            cached_result = self.query_cache.get(query_vector, search_params)
            if cached_result is not None:
                return cached_result, 0.0, True  # ìºì‹œ íˆíŠ¸
        
        # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
        start_time = time.time()
        
        results = collection.search(
            data=[query_vector.tolist()],
            anns_field="vector",
            param=search_params,
            limit=10,
            output_fields=["text", "category", "score", "timestamp"]
        )
        
        search_time = time.time() - start_time
        
        # ê²°ê³¼ ìºì‹±
        if use_cache:
            self.query_cache.put(query_vector, search_params, results)
        
        return results, search_time, cache_hit
    
    def benchmark_search_performance(self, collection: Collection, 
                                   num_queries: int = 100) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
        print(f"ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ({num_queries}ê°œ ì¿¼ë¦¬)...")
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±
        test_queries = [
            "machine learning artificial intelligence",
            "data science analytics",
            "cloud computing technology",
            "mobile app development",
            "cybersecurity network protection",
            "blockchain cryptocurrency",
            "artificial neural networks",
            "big data processing",
            "internet of things IoT",
            "quantum computing research"
        ]
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        total_time = 0
        cache_hits = 0
        search_times = []
        
        print("  ğŸ” ê²€ìƒ‰ ì‹œì‘...")
        
        # ì²« ë²ˆì§¸ ë¼ìš´ë“œ: ìºì‹œ ì—†ì´
        for i in range(num_queries):
            query = test_queries[i % len(test_queries)]
            
            # ìºì‹œ ë¹„í™œì„±í™” ê²€ìƒ‰
            _, search_time, _ = self.optimized_search(collection, query, use_cache=False)
            search_times.append(search_time)
            total_time += search_time
            
            if (i + 1) % 20 == 0:
                print(f"    ì§„í–‰ë¥ : {i + 1}/{num_queries}")
        
        # ìºì‹œ í†µê³„ ìˆ˜ì§‘
        print("  ğŸ—„ï¸ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        cache_test_times = []
        
        # ë‘ ë²ˆì§¸ ë¼ìš´ë“œ: ìºì‹œ í™œì„±í™” (ê°™ì€ ì¿¼ë¦¬ ë°˜ë³µ)
        for i in range(50):
            query = test_queries[i % len(test_queries)]
            _, search_time, is_cache_hit = self.optimized_search(collection, query, use_cache=True)
            cache_test_times.append(search_time)
            if is_cache_hit:
                cache_hits += 1
        
        # í†µê³„ ê³„ì‚°
        avg_search_time = np.mean(search_times)
        avg_cache_time = np.mean(cache_test_times)
        p95_time = np.percentile(search_times, 95)
        p99_time = np.percentile(search_times, 99)
        qps = num_queries / total_time
        
        benchmark_results = {
            "ì´_ì¿¼ë¦¬ìˆ˜": num_queries,
            "ì´_ì‹œê°„": f"{total_time:.3f}ì´ˆ",
            "í‰ê· _ì‘ë‹µì‹œê°„": f"{avg_search_time*1000:.2f}ms",
            "P95_ì‘ë‹µì‹œê°„": f"{p95_time*1000:.2f}ms", 
            "P99_ì‘ë‹µì‹œê°„": f"{p99_time*1000:.2f}ms",
            "QPS": f"{qps:.1f}",
            "ìºì‹œ_íˆíŠ¸ìœ¨": f"{cache_hits/50*100:.1f}%",
            "ìºì‹œ_í‰ê· ì‹œê°„": f"{avg_cache_time*1000:.2f}ms",
            "ì„±ëŠ¥_í–¥ìƒ": f"{(avg_search_time/max(avg_cache_time, 0.001)):.1f}x"
        }
        
        return benchmark_results
    
    def memory_optimization_demo(self, collection: Collection):
        """ë©”ëª¨ë¦¬ ìµœì í™” ë°ëª¨"""
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™” ë°ëª¨...")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (íŒŒì´ì¬ í”„ë¡œì„¸ìŠ¤)
        import psutil
        process = psutil.Process()
        
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"  ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}MB")
        
        # ëŒ€ëŸ‰ ë°ì´í„° ë¡œë“œ
        print("  ğŸ“Š ëŒ€ëŸ‰ ë°ì´í„° ë¡œë“œ ì¤‘...")
        collection.load()
        
        loaded_memory = process.memory_info().rss / 1024 / 1024
        print(f"  ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {loaded_memory:.1f}MB (+{loaded_memory-initial_memory:.1f}MB)")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” íŒ ì¶œë ¥
        print("\n  ğŸ’¡ ë©”ëª¨ë¦¬ ìµœì í™” íŒ:")
        print("    1. í•„ìš”í•œ í•„ë“œë§Œ output_fieldsì— ì§€ì •")
        print("    2. ì»¬ë ‰ì…˜ ì‚¬ìš© í›„ release() í˜¸ì¶œ")
        print("    3. ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„° ì¡°ì • (M, efConstruction)")
        print("    4. íŒŒí‹°ì…˜ í™œìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì‚°")
        
        # ë©”ëª¨ë¦¬ í•´ì œ
        collection.release()
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        
        released_memory = process.memory_info().rss / 1024 / 1024
        print(f"  í•´ì œ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {released_memory:.1f}MB (-{loaded_memory-released_memory:.1f}MB)")
    
    def concurrent_search_test(self, collection: Collection, num_threads: int = 5, 
                             queries_per_thread: int = 20) -> Dict[str, Any]:
        """ë™ì‹œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ”€ ë™ì‹œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ({num_threads}ê°œ ìŠ¤ë ˆë“œ, ìŠ¤ë ˆë“œë‹¹ {queries_per_thread}ê°œ ì¿¼ë¦¬)...")
        
        collection.load()
        
        test_queries = [
            "artificial intelligence machine learning",
            "data science big data analytics", 
            "cloud computing distributed systems",
            "mobile application development",
            "cybersecurity information security"
        ]
        
        def worker_function(thread_id: int) -> Dict[str, Any]:
            """ì›Œì»¤ ìŠ¤ë ˆë“œ í•¨ìˆ˜"""
            thread_times = []
            
            for i in range(queries_per_thread):
                query = test_queries[i % len(test_queries)]
                
                start_time = time.time()
                _, search_time, _ = self.optimized_search(collection, query, use_cache=False)
                total_time = time.time() - start_time
                thread_times.append(total_time)
            
            return {
                "thread_id": thread_id,
                "queries": queries_per_thread,
                "total_time": sum(thread_times),
                "avg_time": np.mean(thread_times),
                "times": thread_times
            }
        
        # ë™ì‹œ ì‹¤í–‰
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(worker_function, i) for i in range(num_threads)]
            results = [future.result() for future in as_completed(futures)]
        
        total_concurrent_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        total_queries = num_threads * queries_per_thread
        all_times = []
        for result in results:
            all_times.extend(result["times"])
        
        concurrent_stats = {
            "ìŠ¤ë ˆë“œìˆ˜": num_threads,
            "ì´_ì¿¼ë¦¬ìˆ˜": total_queries,
            "ì´_ì‹œê°„": f"{total_concurrent_time:.3f}ì´ˆ",
            "ë™ì‹œ_QPS": f"{total_queries/total_concurrent_time:.1f}",
            "í‰ê· _ì‘ë‹µì‹œê°„": f"{np.mean(all_times)*1000:.2f}ms",
            "P95_ì‘ë‹µì‹œê°„": f"{np.percentile(all_times, 95)*1000:.2f}ms"
        }
        
        return concurrent_stats
    
    def run_performance_optimization_demo(self):
        """ì„±ëŠ¥ ìµœì í™” ì¢…í•© ë°ëª¨"""
        print("ğŸš€ Milvus ì„±ëŠ¥ ìµœì í™” ì‹¤ìŠµ")
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Milvus ì—°ê²°
            self.milvus_conn.connect()
            print("âœ… Milvus ì—°ê²° ì„±ê³µ\n")
            
            # ì—°ê²° í’€ ì„¤ì •
            self.setup_connection_pool(pool_size=3)
            
            print("=" * 80)
            print(" ğŸ—ï¸ ìµœì í™”ëœ ì»¬ë ‰ì…˜ ë° ë°ì´í„° êµ¬ì¶•")
            print("=" * 80)
            
            # ìµœì í™”ëœ ì»¬ë ‰ì…˜ ìƒì„±
            collection = self.create_optimized_collection("performance_test")
            
            # ìµœì í™”ëœ ë°°ì¹˜ ì‚½ì…
            insert_time = self.batch_insert_optimized(collection, data_size=5000, batch_size=500)
            
            # ìµœì í™”ëœ ì¸ë±ìŠ¤ ìƒì„±
            index_time = self.create_optimized_index(collection)
            
            # ì»¬ë ‰ì…˜ ë¡œë“œ
            print("\nğŸ”„ ì»¬ë ‰ì…˜ ë¡œë“œ ì¤‘...")
            collection.load()
            print("  âœ… ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ")
            
            print("\n" + "=" * 80)
            print(" ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹")
            print("=" * 80)
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
            benchmark_results = self.benchmark_search_performance(collection, num_queries=50)
            
            print("\nğŸ¯ ê²€ìƒ‰ ì„±ëŠ¥ ê²°ê³¼:")
            for key, value in benchmark_results.items():
                print(f"  {key}: {value}")
            
            print("\n" + "=" * 80)
            print(" ğŸ”€ ë™ì‹œì„± í…ŒìŠ¤íŠ¸")
            print("=" * 80)
            
            # ë™ì‹œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            concurrent_results = self.concurrent_search_test(collection, num_threads=3, queries_per_thread=15)
            
            print("\nâš¡ ë™ì‹œ ê²€ìƒ‰ ì„±ëŠ¥ ê²°ê³¼:")
            for key, value in concurrent_results.items():
                print(f"  {key}: {value}")
            
            print("\n" + "=" * 80)
            print(" ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”")
            print("=" * 80)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ë°ëª¨
            self.memory_optimization_demo(collection)
            
            print("\n" + "=" * 80)
            print(" ğŸ“ˆ ìºì‹œ ë° ì—°ê²° í’€ í†µê³„")
            print("=" * 80)
            
            # ìºì‹œ í†µê³„
            cache_stats = self.query_cache.stats()
            print("\nğŸ—„ï¸ ì¿¼ë¦¬ ìºì‹œ í†µê³„:")
            for key, value in cache_stats.items():
                print(f"  {key}: {value}")
            
            # ì—°ê²° í’€ í†µê³„
            if self.connection_pool:
                pool_stats = self.connection_pool.stats()
                print("\nğŸ”— ì—°ê²° í’€ í†µê³„:")
                for key, value in pool_stats.items():
                    print(f"  {key}: {value}")
            
            print("\n" + "=" * 80)
            print(" ğŸ’¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­")
            print("=" * 80)
            
            print("\nğŸš€ ì„±ëŠ¥ ìµœì í™” íŒ:")
            print("  1. ğŸ“Š ë°°ì¹˜ ì‚½ì…: ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ë°°ì¹˜ë¡œ ì²˜ë¦¬")
            print("  2. ğŸ” ì¸ë±ìŠ¤ ì„ íƒ: HNSW (ì†ë„) vs IVF_PQ (ë©”ëª¨ë¦¬)")
            print("  3. ğŸ—„ï¸ ì¿¼ë¦¬ ìºì‹±: ë°˜ë³µ ì¿¼ë¦¬ì˜ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•")
            print("  4. ğŸ”— ì—°ê²° í’€ë§: ì—°ê²° ì˜¤ë²„í—¤ë“œ ê°ì†Œ")
            print("  5. ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬: ì‚¬ìš© í›„ release() í˜¸ì¶œ")
            print("  6. âš¡ ë™ì‹œì„±: ì ì ˆí•œ ìŠ¤ë ˆë“œ ìˆ˜ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ëŒ€")
            print("  7. ğŸ¯ í•„ë“œ ì„ íƒ: í•„ìš”í•œ output_fieldsë§Œ ì¡°íšŒ")
            print("  8. ğŸ“ˆ ëª¨ë‹ˆí„°ë§: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì§€ì† ê´€ì°°")
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("performance_test")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            # ì—°ê²° ì •ë¦¬
            if self.connection_pool:
                self.connection_pool.close_all()
            self.milvus_conn.disconnect()
            
        print(f"\nğŸ‰ ì„±ëŠ¥ ìµœì í™” ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ë°°ì¹˜ ì²˜ë¦¬ì™€ ì¸ë±ìŠ¤ ìµœì í™”ë¡œ ì„±ëŠ¥ í–¥ìƒ")
        print("  â€¢ ì¿¼ë¦¬ ìºì‹±ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•")
        print("  â€¢ ì—°ê²° í’€ë§ìœ¼ë¡œ ë™ì‹œì„± ì²˜ë¦¬ ê°œì„ ")
        print("  â€¢ ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  python step04_advanced/02_advanced_indexing.py")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    optimizer = PerformanceOptimizer()
    optimizer.run_performance_optimization_demo()

if __name__ == "__main__":
    main() 