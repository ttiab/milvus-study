#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
âš¡ Milvus ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤ìŠµ

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Milvusì˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë° ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤:
- ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
- ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
- ì‹¤ì‹œê°„ ë²¡í„°í™” ë° ì‚½ì…
- ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ ê²€ìƒ‰ ë° ì•Œë¦¼
- ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- ì‹¤ì‹œê°„ ë¶„ì„ ë° ëŒ€ì‹œë³´ë“œ
"""

import os
import sys
import time
import logging
import threading
import queue
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import List, Dict, Any, Tuple, Optional
import json
import random
from concurrent.futures import ThreadPoolExecutor
import uuid

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from common.connection import MilvusConnection
from common.vector_utils import VectorUtils
from common.data_loader import DataLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StreamingDataSource:
    """ì‹¤ì‹œê°„ ë°ì´í„° ì†ŒìŠ¤ ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self, source_type: str = "social_media"):
        self.source_type = source_type
        self.is_streaming = False
        self.stream_queue = queue.Queue(maxsize=1000)
        self.data_templates = self._get_data_templates()
        
    def _get_data_templates(self) -> Dict[str, List[str]]:
        """ë°ì´í„° í…œí”Œë¦¿ ì •ì˜"""
        templates = {
            "social_media": [
                "Breaking news: {topic} trending worldwide #news",
                "Just discovered amazing {topic} tips! Love it! #lifestyle",
                "New {topic} technology is game changing! #tech",
                "Review: {topic} product exceeded expectations #review",
                "Discussion: What do you think about {topic}? #opinion"
            ],
            "news": [
                "{topic} market shows significant growth this quarter",
                "Scientists announce breakthrough in {topic} research", 
                "Government announces new {topic} policy initiatives",
                "Industry experts predict {topic} trends for next year",
                "International summit focuses on {topic} cooperation"
            ],
            "ecommerce": [
                "Customer review: {topic} product quality excellent",
                "New {topic} product launch creates market buzz",
                "Flash sale: {topic} items up to 50% off today",
                "Customer service inquiry about {topic} features",
                "Product recommendation: {topic} perfect for summer"
            ],
            "iot_sensors": [
                "Sensor alert: {topic} temperature anomaly detected",
                "IoT device: {topic} status update normal operations",
                "Smart home: {topic} automation trigger activated",
                "Industrial sensor: {topic} pressure reading elevated",
                "Vehicle telemetry: {topic} performance metric updated"
            ]
        }
        return templates.get(self.source_type, templates["social_media"])
    
    def generate_stream_record(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¼ ë ˆì½”ë“œ ìƒì„±"""
        topics = ["AI", "blockchain", "cloud", "mobile", "security", "analytics", "automation", "sustainability"]
        topic = random.choice(topics)
        template = random.choice(self.data_templates)
        
        # í˜„ì‹¤ì ì¸ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ìƒì„±
        record = {
            "id": str(uuid.uuid4()),
            "timestamp": int(time.time() * 1000),  # ë°€ë¦¬ì´ˆ
            "source": self.source_type,
            "topic": topic,
            "content": template.format(topic=topic),
            "language": random.choice(["en", "ko", "ja", "zh"]),
            "sentiment": random.choice(["positive", "negative", "neutral"]),
            "confidence": round(random.uniform(0.6, 1.0), 3),
            "priority": random.randint(1, 5),
            "location": random.choice(["US", "EU", "ASIA", "GLOBAL"]),
            "user_id": f"user_{random.randint(1000, 9999)}",
            "metadata": {
                "device_type": random.choice(["mobile", "desktop", "tablet"]),
                "platform": random.choice(["ios", "android", "web"]),
                "version": f"{random.randint(1, 5)}.{random.randint(0, 9)}"
            }
        }
        
        return record
    
    def start_streaming(self, records_per_second: float = 2.0):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        self.is_streaming = True
        
        def stream_producer():
            while self.is_streaming:
                try:
                    record = self.generate_stream_record()
                    self.stream_queue.put(record, timeout=1)
                    time.sleep(1.0 / records_per_second)
                except queue.Full:
                    logger.warning("Stream queue is full, dropping record")
                except Exception as e:
                    logger.error(f"Stream producer error: {e}")
        
        self.producer_thread = threading.Thread(target=stream_producer)
        self.producer_thread.start()
    
    def stop_streaming(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€"""
        self.is_streaming = False
        if hasattr(self, 'producer_thread'):
            self.producer_thread.join()
    
    def get_records(self, max_records: int = 10) -> List[Dict[str, Any]]:
        """ë ˆì½”ë“œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°"""
        records = []
        for _ in range(max_records):
            try:
                record = self.stream_queue.get_nowait()
                records.append(record)
            except queue.Empty:
                break
        return records

class StreamProcessor:
    """ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, vector_utils: VectorUtils):
        self.vector_utils = vector_utils
        self.processing_stats = defaultdict(int)
        self.error_count = 0
        self.processed_count = 0
        
    def process_record(self, record: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ë ˆì½”ë“œ ì²˜ë¦¬"""
        try:
            # ë°ì´í„° ê²€ì¦
            if not self._validate_record(record):
                self.error_count += 1
                return None
            
            # í…ìŠ¤íŠ¸ ì •ê·œí™”
            content = self._normalize_text(record["content"])
            
            # ë²¡í„°í™”
            vector = self._vectorize_content(content)
            if vector is None:
                self.error_count += 1
                return None
            
            # ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìƒì„±
            processed_record = {
                "stream_id": record["id"],
                "content": content,
                "source": record["source"],
                "topic": record["topic"],
                "language": record["language"],
                "sentiment": record["sentiment"],
                "confidence": record["confidence"],
                "priority": record["priority"],
                "location": record["location"],
                "user_id": record["user_id"],
                "timestamp": record["timestamp"],
                "processed_at": int(time.time() * 1000),
                "vector": vector.tolist(),
                "metadata": json.dumps(record["metadata"])
            }
            
            self.processed_count += 1
            self.processing_stats[record["source"]] += 1
            
            return processed_record
            
        except Exception as e:
            logger.error(f"Record processing error: {e}")
            self.error_count += 1
            return None
    
    def _validate_record(self, record: Dict[str, Any]) -> bool:
        """ë ˆì½”ë“œ ìœ íš¨ì„± ê²€ì‚¬"""
        required_fields = ["id", "content", "source", "timestamp"]
        
        for field in required_fields:
            if field not in record or not record[field]:
                logger.warning(f"Missing or empty field: {field}")
                return False
        
        # ì½˜í…ì¸  ê¸¸ì´ ê²€ì‚¬
        if len(record["content"]) < 10 or len(record["content"]) > 1000:
            logger.warning(f"Content length out of range: {len(record['content'])}")
            return False
        
        return True
    
    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ë¦¬
        text = text.strip()
        text = ' '.join(text.split())  # ë‹¤ì¤‘ ê³µë°± ì œê±°
        
        # í•´ì‹œíƒœê·¸ ì •ê·œí™”
        import re
        text = re.sub(r'#\w+', lambda m: m.group().lower(), text)
        
        return text
    
    def _vectorize_content(self, content: str) -> Optional[np.ndarray]:
        """ì½˜í…ì¸  ë²¡í„°í™”"""
        try:
            vectors = self.vector_utils.text_to_vector(content)
            vector = vectors[0] if len(vectors.shape) > 1 else vectors
            return vector
        except Exception as e:
            logger.error(f"Vectorization error: {e}")
            return None
    
    def get_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        total_processed = sum(self.processing_stats.values())
        error_rate = self.error_count / max(total_processed + self.error_count, 1)
        
        return {
            "processed_count": total_processed,
            "error_count": self.error_count,
            "error_rate": error_rate,
            "source_distribution": dict(self.processing_stats)
        }

class RealTimeSearchEngine:
    """ì‹¤ì‹œê°„ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, collection: Collection, vector_utils: VectorUtils):
        self.collection = collection
        self.vector_utils = vector_utils
        self.search_history = deque(maxlen=1000)
        self.alert_rules = []
        
    def add_alert_rule(self, name: str, query: str, threshold: float = 0.8, 
                      max_results: int = 5):
        """ì•Œë¦¼ ê·œì¹™ ì¶”ê°€"""
        rule = {
            "name": name,
            "query": query,
            "threshold": threshold,
            "max_results": max_results,
            "last_triggered": 0
        }
        self.alert_rules.append(rule)
    
    def real_time_search(self, query: str, filters: Optional[str] = None, 
                        limit: int = 10) -> List[Dict[str, Any]]:
        """ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vectors = self.vector_utils.text_to_vector(query)
            query_vector = query_vectors[0] if len(query_vectors.shape) > 1 else query_vectors
            
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            search_params = {"metric_type": "COSINE", "params": {"nprobe": 16}}
            
            # ê²€ìƒ‰ ì‹¤í–‰
            start_time = time.time()
            results = self.collection.search(
                data=[query_vector.tolist()],
                anns_field="vector",
                param=search_params,
                limit=limit,
                expr=filters,
                output_fields=["content", "source", "topic", "sentiment", "timestamp", "location"]
            )
            search_time = time.time() - start_time
            
            # ê²°ê³¼ ì²˜ë¦¬
            processed_results = []
            if results and len(results[0]) > 0:
                for hit in results[0]:
                    result = {
                        "content": hit.entity.get('content'),
                        "source": hit.entity.get('source'),
                        "topic": hit.entity.get('topic'),
                        "sentiment": hit.entity.get('sentiment'),
                        "timestamp": hit.entity.get('timestamp'),
                        "location": hit.entity.get('location'),
                        "similarity": float(hit.distance),
                        "score": 1.0 - float(hit.distance)  # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜
                    }
                    processed_results.append(result)
            
            # ê²€ìƒ‰ ì´ë ¥ ì €ì¥
            search_record = {
                "query": query,
                "filters": filters,
                "timestamp": int(time.time() * 1000),
                "search_time": search_time,
                "result_count": len(processed_results),
                "top_score": processed_results[0]["score"] if processed_results else 0
            }
            self.search_history.append(search_record)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"Real-time search error: {e}")
            return []
    
    def check_alerts(self, new_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì•Œë¦¼ ê·œì¹™ í™•ì¸"""
        triggered_alerts = []
        current_time = int(time.time())
        
        for rule in self.alert_rules:
            # ìµœê·¼ íŠ¸ë¦¬ê±° í›„ ìµœì†Œ ê°„ê²© í™•ì¸ (30ì´ˆ)
            if current_time - rule["last_triggered"] < 30:
                continue
            
            try:
                # ì•Œë¦¼ ì¿¼ë¦¬ ì‹¤í–‰
                results = self.real_time_search(
                    query=rule["query"],
                    limit=rule["max_results"]
                )
                
                # ì„ê³„ê°’ í™•ì¸
                high_score_results = [r for r in results if r["score"] >= rule["threshold"]]
                
                if high_score_results:
                    alert = {
                        "rule_name": rule["name"],
                        "query": rule["query"],
                        "triggered_at": current_time,
                        "matching_records": len(high_score_results),
                        "top_matches": high_score_results[:3],
                        "max_score": max(r["score"] for r in high_score_results)
                    }
                    triggered_alerts.append(alert)
                    rule["last_triggered"] = current_time
                    
            except Exception as e:
                logger.error(f"Alert check error for rule {rule['name']}: {e}")
        
        return triggered_alerts
    
    def get_search_analytics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë¶„ì„ ì •ë³´"""
        if not self.search_history:
            return {"message": "No search history available"}
        
        recent_searches = list(self.search_history)[-100:]  # ìµœê·¼ 100ê°œ
        
        # í†µê³„ ê³„ì‚°
        avg_search_time = np.mean([s["search_time"] for s in recent_searches])
        avg_result_count = np.mean([s["result_count"] for s in recent_searches])
        avg_top_score = np.mean([s["top_score"] for s in recent_searches if s["top_score"] > 0])
        
        # ê²€ìƒ‰ ë¹ˆë„ ë¶„ì„
        search_frequency = len(recent_searches) / (time.time() - recent_searches[0]["timestamp"] / 1000) if recent_searches else 0
        
        return {
            "total_searches": len(self.search_history),
            "recent_searches": len(recent_searches),
            "avg_search_time": avg_search_time,
            "avg_result_count": avg_result_count,
            "avg_top_score": avg_top_score,
            "search_frequency": search_frequency,
            "active_alert_rules": len(self.alert_rules)
        }

class RealTimeStreamingManager:
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.milvus_conn = MilvusConnection()
        self.vector_utils = VectorUtils()
        self.stream_sources = {}
        self.stream_processor = StreamProcessor(self.vector_utils)
        self.search_engine = None
        self.is_processing = False
        self.processing_stats = defaultdict(int)
        
    def create_streaming_collection(self, collection_name: str) -> Collection:
        """ìŠ¤íŠ¸ë¦¬ë°ìš© ì»¬ë ‰ì…˜ ìƒì„±"""
        print(f"âš¡ ìŠ¤íŠ¸ë¦¬ë° ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        if utility.has_collection(collection_name):
            utility.drop_collection(collection_name)
            print(f"  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        
        # ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ìŠ¤í‚¤ë§ˆ
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="stream_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="topic", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="language", dtype=DataType.VARCHAR, max_length=10),
            FieldSchema(name="sentiment", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="confidence", dtype=DataType.FLOAT),
            FieldSchema(name="priority", dtype=DataType.INT32),
            FieldSchema(name="location", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="user_id", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="timestamp", dtype=DataType.INT64),
            FieldSchema(name="processed_at", dtype=DataType.INT64),
            FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384),
            FieldSchema(name="metadata", dtype=DataType.VARCHAR, max_length=500)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Real-time streaming collection optimized for high throughput"
        )
        
        collection = Collection(name=collection_name, schema=schema)
        
        # ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
        
        collection.create_index(
            field_name="vector",
            index_params=index_params
        )
        
        print(f"  âœ… ìŠ¤íŠ¸ë¦¬ë° ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        return collection
    
    def setup_data_sources(self):
        """ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •"""
        print("ğŸ”§ ë°ì´í„° ì†ŒìŠ¤ ì„¤ì • ì¤‘...")
        
        source_configs = [
            {"type": "social_media", "rate": 3.0},
            {"type": "news", "rate": 1.5},
            {"type": "ecommerce", "rate": 2.0},
            {"type": "iot_sensors", "rate": 5.0}
        ]
        
        for config in source_configs:
            source = StreamingDataSource(config["type"])
            self.stream_sources[config["type"]] = {
                "source": source,
                "rate": config["rate"]
            }
            print(f"  âœ… {config['type']} ì†ŒìŠ¤ ({config['rate']} rec/sec)")
        
        print(f"  âœ… {len(self.stream_sources)}ê°œ ë°ì´í„° ì†ŒìŠ¤ ì„¤ì • ì™„ë£Œ")
    
    def start_streaming_pipeline(self, collection: Collection, duration: int = 60):
        """ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        print(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹œì‘ (ì§€ì†ì‹œê°„: {duration}ì´ˆ)...")
        
        self.is_processing = True
        collection.load()
        
        # ê²€ìƒ‰ ì—”ì§„ ì„¤ì •
        self.search_engine = RealTimeSearchEngine(collection, self.vector_utils)
        
        # ì•Œë¦¼ ê·œì¹™ ì„¤ì •
        self.search_engine.add_alert_rule("High Priority AI", "artificial intelligence priority", 0.7)
        self.search_engine.add_alert_rule("Security Alert", "security threat vulnerability", 0.8)
        self.search_engine.add_alert_rule("Breaking News", "breaking news urgent", 0.75)
        
        # ë°ì´í„° ì†ŒìŠ¤ ì‹œì‘
        for source_info in self.stream_sources.values():
            source_info["source"].start_streaming(source_info["rate"])
        
        # ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘
        def processing_worker():
            batch_size = 10
            batch_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬
            
            while self.is_processing:
                try:
                    # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë ˆì½”ë“œ ìˆ˜ì§‘
                    all_records = []
                    for source_info in self.stream_sources.values():
                        records = source_info["source"].get_records(batch_size // len(self.stream_sources))
                        all_records.extend(records)
                    
                    if all_records:
                        # ë°°ì¹˜ ì²˜ë¦¬
                        processed_records = []
                        for record in all_records:
                            processed = self.stream_processor.process_record(record)
                            if processed:
                                processed_records.append(processed)
                        
                        if processed_records:
                            # Milvusì— ì‚½ì…
                            self._insert_batch(collection, processed_records)
                            
                            # ì•Œë¦¼ í™•ì¸
                            alerts = self.search_engine.check_alerts(processed_records)
                            if alerts:
                                self._handle_alerts(alerts)
                            
                            self.processing_stats["total_processed"] += len(processed_records)
                            self.processing_stats["last_batch_size"] = len(processed_records)
                            self.processing_stats["last_processed"] = int(time.time())
                    
                    time.sleep(batch_interval)
                    
                except Exception as e:
                    logger.error(f"Processing worker error: {e}")
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        self.processing_thread = threading.Thread(target=processing_worker)
        self.processing_thread.start()
        
        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        start_time = time.time()
        while time.time() - start_time < duration:
            elapsed = int(time.time() - start_time)
            remaining = duration - elapsed
            
            print(f"  â³ ì§„í–‰ ì¤‘... {elapsed}/{duration}ì´ˆ "
                  f"(ì²˜ë¦¬ë¨: {self.processing_stats.get('total_processed', 0)}ê°œ)")
            
            time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€
        self.stop_streaming_pipeline()
        
        print(f"  âœ… ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    
    def _insert_batch(self, collection: Collection, records: List[Dict[str, Any]]):
        """ë°°ì¹˜ ë°ì´í„° ì‚½ì…"""
        if not records:
            return
        
        # ë°ì´í„° êµ¬ì¡°í™”
        data = [
            [r["stream_id"] for r in records],
            [r["content"] for r in records],
            [r["source"] for r in records],
            [r["topic"] for r in records],
            [r["language"] for r in records],
            [r["sentiment"] for r in records],
            [r["confidence"] for r in records],
            [r["priority"] for r in records],
            [r["location"] for r in records],
            [r["user_id"] for r in records],
            [r["timestamp"] for r in records],
            [r["processed_at"] for r in records],
            [r["vector"] for r in records],
            [r["metadata"] for r in records]
        ]
        
        collection.insert(data)
    
    def _handle_alerts(self, alerts: List[Dict[str, Any]]):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        for alert in alerts:
            print(f"  ğŸš¨ ì•Œë¦¼: {alert['rule_name']} - {alert['matching_records']}ê°œ ë§¤ì¹­ "
                  f"(ìµœê³  ì ìˆ˜: {alert['max_score']:.3f})")
    
    def stop_streaming_pipeline(self):
        """ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        self.is_processing = False
        
        # ë°ì´í„° ì†ŒìŠ¤ ì¤‘ì§€
        for source_info in self.stream_sources.values():
            source_info["source"].stop_streaming()
        
        # ì²˜ë¦¬ ìŠ¤ë ˆë“œ ëŒ€ê¸°
        if hasattr(self, 'processing_thread'):
            self.processing_thread.join(timeout=5)
    
    def demonstrate_real_time_search(self, collection: Collection):
        """ì‹¤ì‹œê°„ ê²€ìƒ‰ ë°ëª¨"""
        print("\nğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰ ë°ëª¨...")
        
        if not self.search_engine:
            self.search_engine = RealTimeSearchEngine(collection, self.vector_utils)
        
        # ë‹¤ì–‘í•œ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤
        search_scenarios = [
            {
                "name": "AI ê´€ë ¨ ìµœì‹  ë™í–¥",
                "query": "artificial intelligence machine learning",
                "filters": "priority >= 3"
            },
            {
                "name": "ë³´ì•ˆ ì´ìŠˆ ëª¨ë‹ˆí„°ë§", 
                "query": "security vulnerability threat",
                "filters": "sentiment == 'negative'"
            },
            {
                "name": "ê¸ì •ì  ì œí’ˆ ë¦¬ë·°",
                "query": "product review excellent quality",
                "filters": "sentiment == 'positive' and source == 'ecommerce'"
            },
            {
                "name": "ì‹¤ì‹œê°„ ë‰´ìŠ¤ íŠ¸ë˜í‚¹",
                "query": "breaking news important update",
                "filters": "source == 'news'"
            }
        ]
        
        for scenario in search_scenarios:
            print(f"\n  ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            print(f"    ì¿¼ë¦¬: '{scenario['query']}'")
            print(f"    í•„í„°: {scenario.get('filters', 'None')}")
            
            start_time = time.time()
            results = self.search_engine.real_time_search(
                query=scenario["query"],
                filters=scenario.get("filters"),
                limit=5
            )
            search_time = time.time() - start_time
            
            print(f"    ê²€ìƒ‰ ì‹œê°„: {search_time*1000:.2f}ms")
            print(f"    ê²°ê³¼ ìˆ˜: {len(results)}")
            
            if results:
                print(f"    ìƒìœ„ ê²°ê³¼:")
                for i, result in enumerate(results[:3], 1):
                    print(f"      {i}. ì ìˆ˜: {result['score']:.3f}, "
                          f"ì†ŒìŠ¤: {result['source']}, ì£¼ì œ: {result['topic']}")
                    print(f"         ë‚´ìš©: {result['content'][:60]}...")
            
            time.sleep(1)  # ê²€ìƒ‰ ê°„ ê°„ê²©
    
    def streaming_analytics_dashboard(self, collection: Collection) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""
        print("\nğŸ“Š ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ...")
        
        # ì»¬ë ‰ì…˜ í†µê³„
        try:
            collection.load()
            total_entities = collection.num_entities
            collection.release()
        except:
            total_entities = 0
        
        # ì²˜ë¦¬ í†µê³„
        processor_stats = self.stream_processor.get_stats()
        
        # ê²€ìƒ‰ ë¶„ì„
        search_analytics = {}
        if self.search_engine:
            search_analytics = self.search_engine.get_search_analytics()
        
        # ëŒ€ì‹œë³´ë“œ ë°ì´í„°
        dashboard_data = {
            "streaming_metrics": {
                "total_entities": total_entities,
                "active_sources": len(self.stream_sources),
                "processing_rate": self.processing_stats.get("total_processed", 0),
                "last_processed": self.processing_stats.get("last_processed", 0)
            },
            "data_quality": {
                "processed_records": processor_stats.get("processed_count", 0),
                "error_count": processor_stats.get("error_count", 0),
                "error_rate": processor_stats.get("error_rate", 0),
                "source_distribution": processor_stats.get("source_distribution", {})
            },
            "search_performance": search_analytics,
            "system_health": {
                "pipeline_status": "running" if self.is_processing else "stopped",
                "memory_usage": "normal",  # ì‹œë®¬ë ˆì´ì…˜
                "latency_p95": "< 50ms",   # ì‹œë®¬ë ˆì´ì…˜
                "throughput": "2.5k rec/min"  # ì‹œë®¬ë ˆì´ì…˜
            }
        }
        
        # ëŒ€ì‹œë³´ë“œ ì¶œë ¥
        print(f"  ğŸ“ˆ ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­ìŠ¤:")
        metrics = dashboard_data["streaming_metrics"]
        print(f"    ì´ ì—”í‹°í‹° ìˆ˜: {metrics['total_entities']:,}")
        print(f"    í™œì„± ì†ŒìŠ¤ ìˆ˜: {metrics['active_sources']}")
        print(f"    ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {metrics['processing_rate']:,}")
        
        print(f"\n  ğŸ¯ ë°ì´í„° í’ˆì§ˆ:")
        quality = dashboard_data["data_quality"]
        print(f"    ì„±ê³µë¥ : {(1-quality['error_rate'])*100:.1f}%")
        print(f"    ì˜¤ë¥˜ ìˆ˜: {quality['error_count']}")
        print(f"    ì†ŒìŠ¤ë³„ ë¶„í¬: {quality['source_distribution']}")
        
        if search_analytics:
            print(f"\n  ğŸ” ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"    ì´ ê²€ìƒ‰ ìˆ˜: {search_analytics.get('total_searches', 0)}")
            print(f"    í‰ê·  ì‘ë‹µ ì‹œê°„: {search_analytics.get('avg_search_time', 0)*1000:.2f}ms")
            print(f"    í‰ê·  ê²°ê³¼ ìˆ˜: {search_analytics.get('avg_result_count', 0):.1f}")
        
        print(f"\n  ğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ:")
        health = dashboard_data["system_health"]
        print(f"    íŒŒì´í”„ë¼ì¸: {health['pipeline_status']}")
        print(f"    ë©”ëª¨ë¦¬ ì‚¬ìš©: {health['memory_usage']}")
        print(f"    ì§€ì—° ì‹œê°„ P95: {health['latency_p95']}")
        print(f"    ì²˜ë¦¬ëŸ‰: {health['throughput']}")
        
        return dashboard_data
    
    def run_realtime_streaming_demo(self):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¢…í•© ë°ëª¨"""
        print("âš¡ Milvus ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤ìŠµ")
        print(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Milvus ì—°ê²°
            self.milvus_conn.connect()
            print("âœ… Milvus ì—°ê²° ì„±ê³µ\n")
            
            print("=" * 80)
            print(" ğŸ—ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¸í”„ë¼ êµ¬ì¶•")
            print("=" * 80)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì»¬ë ‰ì…˜ ìƒì„±
            collection = self.create_streaming_collection("realtime_streaming")
            
            # ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •
            self.setup_data_sources()
            
            print("\n" + "=" * 80)
            print(" ğŸš€ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸")
            print("=" * 80)
            
            # ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹œì‘ (30ì´ˆ ë™ì•ˆ)
            self.start_streaming_pipeline(collection, duration=30)
            
            print("\n" + "=" * 80)
            print(" ğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰ ë° ì•Œë¦¼")
            print("=" * 80)
            
            # ì‹¤ì‹œê°„ ê²€ìƒ‰ ë°ëª¨
            self.demonstrate_real_time_search(collection)
            
            print("\n" + "=" * 80)
            print(" ğŸ“Š ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§")
            print("=" * 80)
            
            # ë¶„ì„ ëŒ€ì‹œë³´ë“œ
            dashboard_data = self.streaming_analytics_dashboard(collection)
            
            print("\n" + "=" * 80)
            print(" ğŸ’¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê¶Œì¥ì‚¬í•­")
            print("=" * 80)
            
            print("\nâš¡ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜ ì„¤ê³„:")
            print("  ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ê³„ì¸µ:")
            print("    â€¢ Kafka/Pulsar: ê³ ì„±ëŠ¥ ë©”ì‹œì§€ í")
            print("    â€¢ Schema Registry: ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬")
            print("    â€¢ Connect API: ë‹¤ì–‘í•œ ì†ŒìŠ¤ ì—°ë™")
            print("    â€¢ ë°±í”„ë ˆì…” ì œì–´: ê³¼ë¶€í•˜ ë°©ì§€")
            
            print("\n  ğŸ”„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬:")
            print("    â€¢ ë°°ì¹˜ ì²˜ë¦¬: íš¨ìœ¨ì ì¸ ë²¡í„°í™”")
            print("    â€¢ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”: ì²˜ë¦¬ëŸ‰ ì¦ëŒ€")
            print("    â€¢ ì˜¤ë¥˜ ì²˜ë¦¬: ì¬ì‹œë„ ë° DLQ")
            print("    â€¢ ë°±ì—… í: ì¥ì•  ë³µêµ¬")
            
            print("\n  ğŸ” ì‹¤ì‹œê°„ ê²€ìƒ‰:")
            print("    â€¢ ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤: ë¹ ë¥¸ ê²€ìƒ‰")
            print("    â€¢ ì¦ë¶„ ì¸ë±ì‹±: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸")
            print("    â€¢ ìºì‹œ ê³„ì¸µ: ë°˜ë³µ ì¿¼ë¦¬ ìµœì í™”")
            print("    â€¢ ì•Œë¦¼ ì‹œìŠ¤í…œ: ì´ë²¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ")
            
            print("\n  ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜:")
            print("    â€¢ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ìŠ¤: ì²˜ë¦¬ëŸ‰, ì§€ì—°ì‹œê°„, ì˜¤ë¥˜ìœ¨")
            print("    â€¢ ì•Œë¦¼ ì‹œìŠ¤í…œ: ì„ê³„ê°’ ê¸°ë°˜ ìë™ ì•Œë¦¼")
            print("    â€¢ ë¡œê·¸ ì§‘ê³„: ì¤‘ì•™í™”ëœ ë¡œê·¸ ê´€ë¦¬")
            print("    â€¢ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ: ì‹œê°í™”ëœ ëª¨ë‹ˆí„°ë§")
            
            print("\n  ğŸ› ï¸ ìµœì í™” ì „ëµ:")
            print("    â€¢ ë°°ì¹˜ í¬ê¸° ì¡°ì •: ì²˜ë¦¬ëŸ‰ vs ì§€ì—°ì‹œê°„")
            print("    â€¢ ë²¡í„° ì°¨ì› ìµœì í™”: ì •í™•ë„ vs ì„±ëŠ¥")
            print("    â€¢ íŒŒí‹°ì…”ë‹: ë³‘ë ¬ ì²˜ë¦¬ ì¦ëŒ€")
            print("    â€¢ ì••ì¶•: ë„¤íŠ¸ì›Œí¬ ë° ì €ì¥ì†Œ íš¨ìœ¨")
            
            # ì •ë¦¬
            print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘...")
            utility.drop_collection("realtime_streaming")
            print("âœ… ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        finally:
            self.stop_streaming_pipeline()
            self.milvus_conn.disconnect()
            
        print(f"\nğŸ‰ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹¤ìŠµ ì™„ë£Œ!")
        print("\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:")
        print("  â€¢ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê´€ë¦¬")
        print("  â€¢ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„°í™”")
        print("  â€¢ ì‹¤ì‹œê°„ ê²€ìƒ‰ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ")
        print("  â€¢ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("  python step04_advanced/05_backup_recovery.py")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    streaming_manager = RealTimeStreamingManager()
    streaming_manager.run_realtime_streaming_demo()

if __name__ == "__main__":
    main() 