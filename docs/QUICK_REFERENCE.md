# ğŸš€ Milvus ë¹ ë¥¸ ì°¸ì¡° ê°€ì´ë“œ (Quick Reference)

## ğŸ“‹ ëª©ì°¨

1. [í•„ìˆ˜ ëª…ë ¹ì–´](#í•„ìˆ˜-ëª…ë ¹ì–´)
2. [ì—°ê²° ë° ê¸°ë³¸ ì„¤ì •](#ì—°ê²°-ë°-ê¸°ë³¸-ì„¤ì •)
3. [ìŠ¤í‚¤ë§ˆ ë° ì»¬ë ‰ì…˜](#ìŠ¤í‚¤ë§ˆ-ë°-ì»¬ë ‰ì…˜)
4. [ë°ì´í„° ì¡°ì‘](#ë°ì´í„°-ì¡°ì‘)
5. [ì¸ë±ìŠ¤ ê´€ë¦¬](#ì¸ë±ìŠ¤-ê´€ë¦¬)
6. [ê²€ìƒ‰ ë° ì¿¼ë¦¬](#ê²€ìƒ‰-ë°-ì¿¼ë¦¬)
7. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
8. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## âš¡ í•„ìˆ˜ ëª…ë ¹ì–´

### í”„ë¡œì íŠ¸ ì‹¤í–‰
```bash
# í™˜ê²½ í™œì„±í™”
source venv/bin/activate

# Milvus ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose ps

# ë‹¨ê³„ë³„ ì‹¤ìŠµ ì‹¤í–‰
python step01_basics/01_environment_setup.py
python step02_core_features/01_index_management.py
python step03_use_cases/01_text_similarity_search.py
python step04_advanced/01_performance_optimization.py
python step05_production/01_kubernetes_deployment.py
```

### í™˜ê²½ ê´€ë¦¬
```bash
# ì„œë¹„ìŠ¤ ì¤‘ì§€
docker-compose down

# ë°ì´í„° ì´ˆê¸°í™”
docker-compose down -v

# ë¡œê·¸ í™•ì¸
docker-compose logs milvus

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps | grep milvus
```

---

## ğŸ”— ì—°ê²° ë° ê¸°ë³¸ ì„¤ì •

### ê¸°ë³¸ ì—°ê²°
```python
from pymilvus import connections, Collection

# ì—°ê²° ì„¤ì •
connections.connect(
    alias="default",
    host="localhost",
    port="19530",
    timeout=60
)

# ì—°ê²° ìƒíƒœ í™•ì¸
print(connections.list_connections())
```

### ì—°ê²° í’€ë§ (í”„ë¡œë•ì…˜ìš©)
```python
connections.connect(
    alias="default",
    host="localhost",
    port="19530",
    pool_size=10,
    timeout=60
)
```

### ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
```python
from common.connection import MilvusConnection

with MilvusConnection() as conn:
    # ì—¬ê¸°ì„œ Milvus ì‘ì—… ìˆ˜í–‰
    collection = conn.get_collection("my_collection")
```

---

## ğŸ“„ ìŠ¤í‚¤ë§ˆ ë° ì»¬ë ‰ì…˜

### ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
```python
from pymilvus import FieldSchema, CollectionSchema, DataType

# í•„ë“œ ì •ì˜
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=64),
    FieldSchema(name="score", dtype=DataType.FLOAT),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
]

# ìŠ¤í‚¤ë§ˆ ìƒì„±
schema = CollectionSchema(
    fields=fields,
    description="Text similarity search collection"
)

# ì»¬ë ‰ì…˜ ìƒì„±
collection = Collection(name="text_search", schema=schema)
```

### ê³ ê¸‰ ìŠ¤í‚¤ë§ˆ (ë™ì  í•„ë“œ)
```python
schema = CollectionSchema(
    fields=fields,
    description="Advanced collection with dynamic fields",
    enable_dynamic_field=True  # ë™ì  í•„ë“œ í™œì„±í™”
)
```

### ì»¬ë ‰ì…˜ ê´€ë¦¬
```python
# ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
from pymilvus import utility
if utility.has_collection("text_search"):
    print("ì»¬ë ‰ì…˜ ì¡´ì¬")

# ì»¬ë ‰ì…˜ ì‚­ì œ
utility.drop_collection("text_search")

# ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡
collections = utility.list_collections()
print(f"ì»¬ë ‰ì…˜: {collections}")

# ì»¬ë ‰ì…˜ ì •ë³´
collection_info = collection.describe()
print(collection_info)
```

---

## ğŸ’¾ ë°ì´í„° ì¡°ì‘

### ë°ì´í„° ì‚½ì… (List[List] ë°©ì‹ - ê¶Œì¥)
```python
# 5,000ê°œ ë°ì´í„° ì‚½ì… ì˜ˆì‹œ
texts = ["document text 1", "document text 2", ...]
categories = ["tech", "business", ...]
scores = [0.95, 0.87, ...]
embeddings = [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]

# List[List] í˜•íƒœë¡œ êµ¬ì„± (í•„ë“œ ìˆœì„œëŒ€ë¡œ)
data = [
    texts,      # text í•„ë“œ
    categories, # category í•„ë“œ  
    scores,     # score í•„ë“œ
    embeddings  # embedding í•„ë“œ
]

# ì‚½ì… ì‹¤í–‰
result = collection.insert(data)
collection.flush()  # ë©”ëª¨ë¦¬ â†’ ë””ìŠ¤í¬
print(f"ì‚½ì…ëœ ID: {result.primary_keys}")
```

### ë°°ì¹˜ ì‚½ì…
```python
batch_size = 1000
for i in range(0, len(all_data), batch_size):
    batch_data = all_data[i:i+batch_size]
    collection.insert(batch_data)
    
    if i % (batch_size * 10) == 0:  # 10ë°°ì¹˜ë§ˆë‹¤ í”ŒëŸ¬ì‹œ
        collection.flush()

# ìµœì¢… í”ŒëŸ¬ì‹œ
collection.flush()
```

### íŒŒí‹°ì…˜ ì‚¬ìš©
```python
# íŒŒí‹°ì…˜ ìƒì„±
collection.create_partition("2024_Q1")
collection.create_partition("technology")

# íŒŒí‹°ì…˜ì— ë°ì´í„° ì‚½ì…
collection.insert(data, partition_name="technology")

# íŒŒí‹°ì…˜ ëª©ë¡ í™•ì¸
partitions = collection.partitions
for p in partitions:
    print(f"íŒŒí‹°ì…˜: {p.name}")
```

---

## ğŸ”§ ì¸ë±ìŠ¤ ê´€ë¦¬

### HNSW ì¸ë±ìŠ¤ (ê³ ì„±ëŠ¥)
```python
hnsw_index = {
    "metric_type": "L2",
    "index_type": "HNSW",
    "params": {
        "M": 16,               # ì—°ê²° ìˆ˜ (8-64)
        "efConstruction": 256  # êµ¬ì¶• ì‹œ íƒìƒ‰ (64-512)
    }
}

collection.create_index(
    field_name="embedding",
    index_params=hnsw_index
)
```

### IVF ê³„ì—´ ì¸ë±ìŠ¤
```python
# IVF_FLAT (ê· í˜•ì )
ivf_flat_index = {
    "metric_type": "L2",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}  # í´ëŸ¬ìŠ¤í„° ìˆ˜
}

# IVF_SQ8 (ë©”ëª¨ë¦¬ ì ˆì•½)
ivf_sq8_index = {
    "metric_type": "L2", 
    "index_type": "IVF_SQ8",
    "params": {"nlist": 1024}
}

# IVF_PQ (ëŒ€ìš©ëŸ‰)
ivf_pq_index = {
    "metric_type": "L2",
    "index_type": "IVF_PQ", 
    "params": {
        "nlist": 1024,
        "m": 8,      # ë¶€ë²¡í„° ìˆ˜
        "nbits": 8   # ì–‘ìí™” ë¹„íŠ¸
    }
}
```

### ì¸ë±ìŠ¤ ê´€ë¦¬
```python
# ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
collection.drop_index(field_name="embedding")

# ì¸ë±ìŠ¤ ì •ë³´ í™•ì¸
indexes = collection.indexes
for idx in indexes:
    print(f"ì¸ë±ìŠ¤: {idx.field_name}, íƒ€ì…: {idx.index_type}")

# ì»¬ë ‰ì…˜ ë¡œë“œ (ê²€ìƒ‰ ì „ í•„ìˆ˜)
collection.load()

# ì»¬ë ‰ì…˜ í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
collection.release()
```

---

## ğŸ” ê²€ìƒ‰ ë° ì¿¼ë¦¬

### ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
```python
# ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
search_params = {
    "metric_type": "L2",
    "params": {
        "ef": 128      # HNSWìš© (32-512)
        # "nprobe": 64 # IVFìš© (1-nlist)
    }
}

# ë‹¨ì¼ ê²€ìƒ‰
query_vector = [[0.1, 0.2, 0.3, ...]]  # 2D ë°°ì—´
results = collection.search(
    data=query_vector,
    anns_field="embedding",
    param=search_params,
    limit=10,
    output_fields=["text", "category", "score"]
)

# ê²°ê³¼ ì¶œë ¥
for hits in results:
    for hit in hits:
        print(f"ID: {hit.id}, ê±°ë¦¬: {hit.distance:.4f}")
        print(f"í…ìŠ¤íŠ¸: {hit.entity.get('text')}")
```

### í•„í„°ë§ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ)
```python
# ìŠ¤ì¹¼ë¼ í•„í„° + ë²¡í„° ê²€ìƒ‰
results = collection.search(
    data=query_vector,
    anns_field="embedding", 
    param=search_params,
    limit=10,
    expr="category == 'technology' and score >= 0.8",  # í•„í„° ì¡°ê±´
    output_fields=["text", "category", "score"]
)
```

### ë³µì¡í•œ í•„í„° ì¡°ê±´
```python
# ì—¬ëŸ¬ ì¡°ê±´ ì¡°í•©
filter_expressions = [
    "category in ['tech', 'ai', 'ml']",
    "score >= 0.7 and score <= 1.0", 
    "text like '%artificial%'",
    "id in [1, 2, 3, 4, 5]",
    "category == 'tech' and (score > 0.8 or id < 1000)"
]

for expr in filter_expressions:
    results = collection.search(
        data=query_vector,
        anns_field="embedding",
        param=search_params,
        expr=expr,
        limit=5
    )
```

### íŒŒí‹°ì…˜ë³„ ê²€ìƒ‰
```python
# íŠ¹ì • íŒŒí‹°ì…˜ì—ì„œë§Œ ê²€ìƒ‰
results = collection.search(
    data=query_vector,
    anns_field="embedding",
    param=search_params,
    partition_names=["technology", "ai"],  # íŠ¹ì • íŒŒí‹°ì…˜
    limit=10
)
```

### ë°°ì¹˜ ê²€ìƒ‰
```python
# ì—¬ëŸ¬ ì¿¼ë¦¬ ë™ì‹œ ê²€ìƒ‰
query_vectors = [
    [0.1, 0.2, ...],
    [0.3, 0.4, ...],
    [0.5, 0.6, ...]
]

results = collection.search(
    data=query_vectors,
    anns_field="embedding",
    param=search_params,
    limit=10
)

# ê° ì¿¼ë¦¬ë³„ ê²°ê³¼ ì²˜ë¦¬
for i, hits in enumerate(results):
    print(f"Query {i+1} results:")
    for hit in hits:
        print(f"  ID: {hit.id}, Distance: {hit.distance:.4f}")
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# ì»¬ë ‰ì…˜ ë¡œë“œ (ê²€ìƒ‰ ì „)
collection.load()

# íŠ¹ì • íŒŒí‹°ì…˜ë§Œ ë¡œë“œ
collection.load(partition_names=["2024_Q1"])

# ë©”ëª¨ë¦¬ í•´ì œ
collection.release()

# íŠ¹ì • íŒŒí‹°ì…˜ í•´ì œ
collection.release(partition_names=["old_partition"])
```

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
# ìµœì  ë°°ì¹˜ í¬ê¸°
OPTIMAL_BATCH_SIZE = 1000

def optimized_insert(collection, all_data):
    """ìµœì í™”ëœ ë°°ì¹˜ ì‚½ì…"""
    total_inserted = 0
    
    for i in range(0, len(all_data), OPTIMAL_BATCH_SIZE):
        batch = all_data[i:i+OPTIMAL_BATCH_SIZE]
        result = collection.insert(batch)
        total_inserted += len(result.primary_keys)
        
        # ì£¼ê¸°ì  í”ŒëŸ¬ì‹œ
        if i % (OPTIMAL_BATCH_SIZE * 10) == 0:
            collection.flush()
            print(f"Progress: {total_inserted} inserted")
    
    # ìµœì¢… í”ŒëŸ¬ì‹œ
    collection.flush()
    return total_inserted
```

### ê²€ìƒ‰ ì„±ëŠ¥ íŠœë‹
```python
# ì¸ë±ìŠ¤ë³„ ìµœì  íŒŒë¼ë¯¸í„°
optimal_params = {
    "HNSW": {
        "build": {"M": 16, "efConstruction": 256},
        "search": {"ef": 128}
    },
    "IVF_FLAT": {
        "build": {"nlist": 1024},
        "search": {"nprobe": 64}
    },
    "IVF_SQ8": {
        "build": {"nlist": 4096},
        "search": {"nprobe": 128}
    }
}

# ë™ì  íŒŒë¼ë¯¸í„° ì¡°ì •
def auto_tune_search_params(data_size):
    if data_size < 100_000:
        return {"ef": 64}   # ì†Œê·œëª¨
    elif data_size < 1_000_000:
        return {"ef": 128}  # ì¤‘ê·œëª¨
    else:
        return {"ef": 256}  # ëŒ€ê·œëª¨
```

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì—°ê²° ë¬¸ì œ
```python
def check_milvus_connection():
    """Milvus ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        from pymilvus import connections
        connections.connect("default", host="localhost", port="19530")
        print("âœ… Milvus ì—°ê²° ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âŒ Milvus ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def diagnose_connection():
    """ì—°ê²° ë¬¸ì œ ì§„ë‹¨"""
    import subprocess
    
    # Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
    result = subprocess.run(['docker', 'ps'], capture_output=True, text=True)
    if 'milvus' in result.stdout:
        print("âœ… Milvus ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì¤‘")
    else:
        print("âŒ Milvus ì»¨í…Œì´ë„ˆ ì¤‘ì§€ë¨")
        print("í•´ê²°ì±…: docker-compose up -d")
```

### ë°ì´í„° ì‚½ì… ë¬¸ì œ
```python
def safe_insert_with_validation(collection, data):
    """ê²€ì¦ì„ í¬í•¨í•œ ì•ˆì „í•œ ë°ì´í„° ì‚½ì…"""
    try:
        # 1. ë°ì´í„° í˜•ì‹ ê²€ì¦
        if not isinstance(data, list):
            raise ValueError("ë°ì´í„°ëŠ” List í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        # 2. ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸
        schema_fields = [f.name for f in collection.schema.fields if not f.is_primary or not f.auto_id]
        if len(data) != len(schema_fields):
            raise ValueError(f"í•„ë“œ ìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {len(schema_fields)}, ì‹¤ì œ {len(data)}")
        
        # 3. ë°ì´í„° ê¸¸ì´ í™•ì¸
        first_field_len = len(data[0])
        for i, field_data in enumerate(data):
            if len(field_data) != first_field_len:
                raise ValueError(f"í•„ë“œ {i}ì˜ ë°ì´í„° ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤")
        
        # 4. ì‚½ì… ì‹¤í–‰
        result = collection.insert(data)
        collection.flush()
        
        print(f"âœ… {len(result.primary_keys)}ê°œ ë°ì´í„° ì‚½ì… ì™„ë£Œ")
        return result
        
    except Exception as e:
        print(f"âŒ ì‚½ì… ì‹¤íŒ¨: {e}")
        print(f"ë°ì´í„° í˜•íƒœ: {type(data)}")
        if isinstance(data, list) and data:
            print(f"ì²« ë²ˆì§¸ í•„ë“œ ê¸¸ì´: {len(data[0])}")
            print(f"ì´ í•„ë“œ ìˆ˜: {len(data)}")
        raise
```

### ì¸ë±ìŠ¤ ë¬¸ì œ
```python
def fix_index_issues(collection):
    """ì¸ë±ìŠ¤ ê´€ë ¨ ë¬¸ì œ í•´ê²°"""
    try:
        # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        indexes = collection.indexes
        if indexes:
            print("ê¸°ì¡´ ì¸ë±ìŠ¤ ë°œê²¬, ì‚­ì œ ì¤‘...")
            for idx in indexes:
                collection.drop_index(field_name=idx.field_name)
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
        index_params = {
            "metric_type": "L2",
            "index_type": "HNSW", 
            "params": {"M": 16, "efConstruction": 256}
        }
        
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        
        print("âœ… ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨: {e}")
```

### ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨
```python
def diagnose_performance(collection):
    """ì„±ëŠ¥ ë¬¸ì œ ì§„ë‹¨"""
    import time
    
    report = {"timestamp": time.strftime("%Y-%m-%d %H:%M:%S")}
    
    # 1. ì»¬ë ‰ì…˜ í†µê³„
    try:
        stats = collection.get_stats()
        report["stats"] = stats
    except:
        report["stats"] = "ì¡°íšŒ ì‹¤íŒ¨"
    
    # 2. ì¸ë±ìŠ¤ ìƒíƒœ
    try:
        indexes = collection.indexes
        report["indexes"] = [{"field": idx.field_name, "type": idx.index_type} for idx in indexes]
    except:
        report["indexes"] = "ì¡°íšŒ ì‹¤íŒ¨"
    
    # 3. ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    try:
        dummy_vector = [[0.1] * 384]
        start_time = time.time()
        
        collection.search(
            data=dummy_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"ef": 64}},
            limit=1
        )
        
        latency = (time.time() - start_time) * 1000
        report["search_latency_ms"] = round(latency, 2)
        report["performance_status"] = "ì •ìƒ" if latency < 100 else "ëŠë¦¼"
        
    except Exception as e:
        report["search_latency_ms"] = "ì¸¡ì • ì‹¤íŒ¨"
        report["error"] = str(e)
    
    return report
```

### ë¡œê·¸ ë¶„ì„
```bash
# Milvus ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
docker logs milvus-standalone

# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
docker logs -f milvus-standalone

# ìµœê·¼ 100ì¤„ë§Œ í™•ì¸
docker logs --tail 100 milvus-standalone

# íŠ¹ì • ì‹œê°„ ì´í›„ ë¡œê·¸
docker logs --since "2024-01-01T10:00:00" milvus-standalone
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

### ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
```python
def get_collection_info(collection):
    """ì»¬ë ‰ì…˜ ì¢…í•© ì •ë³´"""
    info = {
        "name": collection.name,
        "description": collection.description,
        "schema": collection.schema,
        "stats": collection.get_stats(),
        "indexes": [(idx.field_name, idx.index_type) for idx in collection.indexes],
        "partitions": [p.name for p in collection.partitions]
    }
    return info

# ì‚¬ìš© ì˜ˆì‹œ
info = get_collection_info(collection)
print(f"ì»¬ë ‰ì…˜ ì´ë¦„: {info['name']}")
print(f"ë°ì´í„° ìˆ˜: {info['stats']}")
```

### ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
```python
def system_health_check():
    """ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸"""
    from pymilvus import utility
    
    health = {
        "connections": len(connections.list_connections()),
        "collections": len(utility.list_collections()),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # ê° ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸
    collection_status = {}
    for coll_name in utility.list_collections():
        try:
            coll = Collection(coll_name)
            collection_status[coll_name] = {
                "loaded": "ë¡œë“œë¨" if coll.has_index() else "ì–¸ë¡œë“œë¨",
                "indexes": len(coll.indexes),
                "partitions": len(coll.partitions)
            }
        except:
            collection_status[coll_name] = "ì˜¤ë¥˜"
    
    health["collections_detail"] = collection_status
    return health
```

---

## ğŸš€ ì‹¤ìš©ì ì¸ ì½”ë“œ ìŠ¤ë‹ˆí«

### ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
class VectorSearchSystem:
    def __init__(self, collection_name):
        self.collection = Collection(collection_name)
        self.vector_utils = VectorUtils()
    
    def add_documents(self, texts, categories=None):
        """ë¬¸ì„œ ì¶”ê°€"""
        vectors = self.vector_utils.text_to_vector(texts)
        
        if categories is None:
            categories = ["general"] * len(texts)
        
        data = [texts, categories, vectors.tolist()]
        result = self.collection.insert(data)
        self.collection.flush()
        return result.primary_keys
    
    def search(self, query_text, top_k=10, category_filter=None):
        """í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        query_vector = self.vector_utils.text_to_vector([query_text])
        
        expr = None
        if category_filter:
            expr = f"category == '{category_filter}'"
        
        results = self.collection.search(
            data=query_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"ef": 128}},
            limit=top_k,
            expr=expr,
            output_fields=["text", "category"]
        )
        
        return [(hit.entity.get('text'), hit.distance) for hit in results[0]]

# ì‚¬ìš© ì˜ˆì‹œ
search_system = VectorSearchSystem("my_search")
doc_ids = search_system.add_documents(
    ["AI is amazing", "Machine learning rocks"],
    ["tech", "tech"]
)
results = search_system.search("artificial intelligence", top_k=5)
```

### ë°°ì¹˜ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
```python
def batch_process(collection, data, batch_size=1000, show_progress=True):
    """íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬"""
    total_items = len(data[0])  # ì²« ë²ˆì§¸ í•„ë“œì˜ ê¸¸ì´
    total_inserted = 0
    
    for i in range(0, total_items, batch_size):
        # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
        batch_data = []
        for field_data in data:
            batch_data.append(field_data[i:i+batch_size])
        
        # ì‚½ì…
        result = collection.insert(batch_data)
        total_inserted += len(result.primary_keys)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if show_progress and i % (batch_size * 5) == 0:
            progress = (total_inserted / total_items) * 100
            print(f"ì§„í–‰ë¥ : {progress:.1f}% ({total_inserted}/{total_items})")
        
        # ì£¼ê¸°ì  í”ŒëŸ¬ì‹œ
        if i % (batch_size * 10) == 0:
            collection.flush()
    
    # ìµœì¢… í”ŒëŸ¬ì‹œ
    collection.flush()
    print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {total_inserted}ê°œ í•­ëª©")
    return total_inserted
```

---

## ğŸ”§ ìœ ìš©í•œ í•¨ìˆ˜ë“¤

### ìë™ ìŠ¤í‚¤ë§ˆ ìƒì„±
```python
def create_schema_from_sample(sample_data, collection_name):
    """ìƒ˜í”Œ ë°ì´í„°ë¡œë¶€í„° ìë™ ìŠ¤í‚¤ë§ˆ ìƒì„±"""
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True)
    ]
    
    for field_name, sample_value in sample_data.items():
        if isinstance(sample_value, str):
            fields.append(FieldSchema(name=field_name, dtype=DataType.VARCHAR, max_length=512))
        elif isinstance(sample_value, int):
            fields.append(FieldSchema(name=field_name, dtype=DataType.INT64))
        elif isinstance(sample_value, float):
            fields.append(FieldSchema(name=field_name, dtype=DataType.FLOAT))
        elif isinstance(sample_value, list) and len(sample_value) > 0:
            if isinstance(sample_value[0], float):
                fields.append(FieldSchema(name=field_name, dtype=DataType.FLOAT_VECTOR, dim=len(sample_value)))
    
    schema = CollectionSchema(fields=fields, description=f"Auto-generated schema for {collection_name}")
    return schema
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```python
def benchmark_search(collection, query_vectors, num_runs=10):
    """ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    
    latencies = []
    
    for i in range(num_runs):
        start_time = time.time()
        
        results = collection.search(
            data=query_vectors,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"ef": 128}},
            limit=10
        )
        
        latency = (time.time() - start_time) * 1000  # ms
        latencies.append(latency)
    
    avg_latency = sum(latencies) / len(latencies)
    min_latency = min(latencies)
    max_latency = max(latencies)
    
    print(f"í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.2f}ms")
    print(f"ìµœì†Œ ì§€ì—°ì‹œê°„: {min_latency:.2f}ms")  
    print(f"ìµœëŒ€ ì§€ì—°ì‹œê°„: {max_latency:.2f}ms")
    print(f"QPS (ì¶”ì •): {1000/avg_latency:.1f}")
    
    return avg_latency
```

---

ì´ ë¹ ë¥¸ ì°¸ì¡° ê°€ì´ë“œë¥¼ í†µí•´ **Milvus ê°œë°œ ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë“¤ì„ ë¹ ë¥´ê²Œ ì°¾ì•„ í™œìš©**í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ“šâœ¨ 